{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this article we will be looking at how to handle the missing values using PySpark, as we all know that handling the missing value is one of the most critical part of any data exploration and analysis pipeline and when we have a large dataset so data engineers should have enough skills to handle the NA/missing values in the dataset.\n",
    "\n",
    "This is the second article in the PySpark's series if you don't understand the basics of dataframe in Pyspark then I'll suggest you to go through my previous article on: Data Preprocessing using PySpark - Pyspark's DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WaxwbmWjTdGg"
   },
   "source": [
    "## Table of content\n",
    "Handling NULL values and missing values using **`PySpark`**\n",
    "* **Spark Session:** Starting the spark session - Mandatory.\n",
    "* **Reading the dataset:** Reading the **Dummy** dataset.\n",
    "* **Dropping columns:** Droping the columns which have null values and know when to drop the complete columns.\n",
    "* **Dropping rows:** Dropping particular rows based on the null values encountered.\n",
    "* **Parameter in Dropping functionalities:** Know about various parameter in the dropping function of PySpark.\n",
    "* **Missing values by Mean, Median and Mode:** Handing missing values by imputing Mean, Median or Mode depending on the requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVdnFt4hWugJ"
   },
   "source": [
    "Before moving on with the main topics of this article let's first do the mandatory thing : **`Starting the PySpark Session`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X-t51eNNW4-v",
    "outputId": "8ffc8779-8b89-432f-c3e1-6d32732483b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 281.4 MB 34 kB/s \n",
      "\u001b[?25hCollecting py4j==0.10.9.3\n",
      "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
      "\u001b[K     |████████████████████████████████| 198 kB 54.6 MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=2b898ef4c839ce8cfa4b440d6bb32c40d4b459b5281213629d710658a8bdf9fa\n",
      "  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lb9j9KbtZATv"
   },
   "source": [
    "## Starting the PySpark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "id": "el7QBWY1VVLs",
    "outputId": "90fee564-0d3b-47e1-8458-6838953fb83c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://275aaddadede:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Handling Missing values using PySpark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f87ce6ae550>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "null_spark = SparkSession.builder.appName('Handling Missing values using PySpark').getOrCreate()\n",
    "null_spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJQDd2biYSPq"
   },
   "source": [
    "**Note:** This segment I have already covered in detail in my first blog of the PySpark Series - **Getting started with PySpark** so please visit this article before moving forward with this one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SBRgDpleZHhW"
   },
   "source": [
    "## Reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dd5SlwuHXq26",
    "outputId": "87ab358b-0387-43e2-d57a-c0fc0fd49590"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Employee Name: string, Age of Employee: int, Experience (in years): int, Salary (per month - $): int]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_null_pyspark = null_spark.read.csv('/content/part2.csv', header = True, inferSchema = True)\n",
    "df_null_pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k5TTLcRDZkkO"
   },
   "source": [
    "Breaking down the **read.csv()** function:\n",
    "This function is basically is sole responsible for reading the CSV formatted data in PySpark.\n",
    "* 1st parameter: Complete path of the dataset.\n",
    "* 2nd parameter: **Header-** This will be responsible to make the column name as the column header **when the flag is True**.\n",
    "* 3rd parameter: **inferScehma-** This will make us show the **orginal data type** of each column **when the flag is True**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HcmdcktIbZVM"
   },
   "source": [
    "## Displaying the dataset using show() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DfpOg2kkZXU1",
    "outputId": "fef6c9d2-834f-472c-ce61-045af72896a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+---------------------+----------------------+\n",
      "|Employee Name|Age of Employee|Experience (in years)|Salary (per month - $)|\n",
      "+-------------+---------------+---------------------+----------------------+\n",
      "|       Oliver|             31|                   10|                 30000|\n",
      "|        Harry|             30|                    8|                 25000|\n",
      "|       George|             29|                    4|                 20000|\n",
      "|         Jack|             24|                    3|                 20000|\n",
      "|        Jacob|             21|                    1|                 15000|\n",
      "|          Leo|             23|                    2|                 18000|\n",
      "|        Oscar|           null|                 null|                 40000|\n",
      "|         null|             34|                   10|                 38000|\n",
      "|         null|             36|                 null|                  null|\n",
      "+-------------+---------------+---------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_null_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_1OJ_EW_b8Qw"
   },
   "source": [
    "As mentioned in the Table of the content so we will be working with the **Dummy dataset** to deal with the missing values in this article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ck6up37CcVli"
   },
   "source": [
    "## Dropping the NULL values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZGbbg9YX8vx"
   },
   "source": [
    "Before start dropping the columns with null values let me introduced you with a function that can let us know about which column has null values and which don't\n",
    "**bold text**\n",
    "So the function is **`printSchema()`** which works in the same way as **describe()** function of pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XzjQRRVkeA6y",
    "outputId": "840d4b49-8509-49fd-da11-e9081eb8ddea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Employee Name: string (nullable = true)\n",
      " |-- Age of Employee: integer (nullable = true)\n",
      " |-- Experience (in years): integer (nullable = true)\n",
      " |-- Salary (per month - $): integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_null_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Am741053eNHp"
   },
   "source": [
    "**Inference:** Here one can see that just after the name of the column of our dataset we can see **nullable = True** which means there are some null values in that column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5UcKEnjqczNw"
   },
   "source": [
    "For dropping the **Null (na) values** from the dataset we simply use the **na.drop() function** and it will drop all the rows which have even one null value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GOsIrZfHbqcF",
    "outputId": "2cc5fb14-9a34-4304-ef79-91467934cb1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+---------------------+----------------------+\n",
      "|Employee Name|Age of Employee|Experience (in years)|Salary (per month - $)|\n",
      "+-------------+---------------+---------------------+----------------------+\n",
      "|       Oliver|             31|                   10|                 30000|\n",
      "|        Harry|             30|                    8|                 25000|\n",
      "|       George|             29|                    4|                 20000|\n",
      "|         Jack|             24|                    3|                 20000|\n",
      "|        Jacob|             21|                    1|                 15000|\n",
      "|          Leo|             23|                    2|                 18000|\n",
      "+-------------+---------------+---------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_null_pyspark.na.drop().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kll0-d_wa2yl"
   },
   "source": [
    "**Inference:** In the above output we can see that rows that contain the NULL values are dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "COgOnJj_ipY8"
   },
   "source": [
    "Previously we saw how to remove the NULL values from rows but we also saw that it removed a complete row even if we have only one NULL value \n",
    "\n",
    "So can we control it for some extent that based on some condition only it will remove the null values?\n",
    "\n",
    "Answer is YES! we can so let's discuss how we can do that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Omqq1fXjeLs"
   },
   "source": [
    "## \"HOW\" parameter in na.drop() function\n",
    "\n",
    "So this paramter is one way where we can decide that in which condition we can skip the NULL values or remove them and while using this parameter we have two options with us let's keep a note of it:\n",
    "\n",
    "* **HOW = \"ANY\":** The kind of keywords given to these functionalities is itself a straightforward explanation yet when we will select **ANY** that signifies if **atleast one non-null value** is there then the **no row will be dropped.**\n",
    "\n",
    "* **HOW = \"ALL\":** When we will select ALL option that signifies if the row have **all the null values** in its record then only it will drop that row otherwise there would be no effect (column will not drop)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n7gR6lbLps8q"
   },
   "source": [
    "### HOW = \"ANY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w82gIYlCeZNP",
    "outputId": "487a147d-65fe-41a0-ca10-9c2277a68e5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+---------------------+----------------------+\n",
      "|Employee Name|Age of Employee|Experience (in years)|Salary (per month - $)|\n",
      "+-------------+---------------+---------------------+----------------------+\n",
      "|       Oliver|             31|                   10|                 30000|\n",
      "|        Harry|             30|                    8|                 25000|\n",
      "|       George|             29|                    4|                 20000|\n",
      "|         Jack|             24|                    3|                 20000|\n",
      "|        Jacob|             21|                    1|                 15000|\n",
      "|          Leo|             23|                    2|                 18000|\n",
      "+-------------+---------------+---------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_null_pyspark.na.drop(how=\"any\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2x1LHqX_qPDG"
   },
   "source": [
    "**Inference:** As discussed in \"any\" option it will drop the complete row when there are **more than one NULL value** otherwise row will remain unaffected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tNFF-suFs6Gx"
   },
   "source": [
    "### HOW = \"ALL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "21eSP34_qDgB",
    "outputId": "1a4e85bf-c9aa-4856-c7f2-99226037b379"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+---------------------+----------------------+\n",
      "|Employee Name|Age of Employee|Experience (in years)|Salary (per month - $)|\n",
      "+-------------+---------------+---------------------+----------------------+\n",
      "|       Oliver|             31|                   10|                 30000|\n",
      "|        Harry|             30|                    8|                 25000|\n",
      "|       George|             29|                    4|                 20000|\n",
      "|         Jack|             24|                    3|                 20000|\n",
      "|        Jacob|             21|                    1|                 15000|\n",
      "|          Leo|             23|                    2|                 18000|\n",
      "|        Oscar|           null|                 null|                 40000|\n",
      "|         null|             34|                   10|                 38000|\n",
      "|         null|             36|                 null|                  null|\n",
      "+-------------+---------------+---------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_null_pyspark.na.drop(how=\"all\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2DesMUz4hZI0"
   },
   "source": [
    "**Inference:** As discussed in \"all\" option that it will drop the NULL values only if all the values in one tuple of record is NULL otherwise there will be no change i.e. no row will be dropped and based on that only we can see there is no change in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RU2TagfZmtc-"
   },
   "source": [
    "## \"THRESH\" parameter in na.drop() function\n",
    "\n",
    "In this parameter we set the threshold value of the **minimum NON NULL values** in a particular row i.e. Suppose if we set the threshold value to **2** then that means the row will be dropped only if the total number of null values exceed **2 otherwise that row will not get dropped.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4vi1E1m4t-IM",
    "outputId": "130b5edc-a6d9-4a9d-a520-4ad8f713010c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+---------------------+----------------------+\n",
      "|Employee Name|Age of Employee|Experience (in years)|Salary (per month - $)|\n",
      "+-------------+---------------+---------------------+----------------------+\n",
      "|       Oliver|             31|                   10|                 30000|\n",
      "|        Harry|             30|                    8|                 25000|\n",
      "|       George|             29|                    4|                 20000|\n",
      "|         Jack|             24|                    3|                 20000|\n",
      "|        Jacob|             21|                    1|                 15000|\n",
      "|          Leo|             23|                    2|                 18000|\n",
      "|        Oscar|           null|                 null|                 40000|\n",
      "|         null|             34|                   10|                 38000|\n",
      "+-------------+---------------+---------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_null_pyspark.na.drop(thresh=2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W-SZpzraqQFb"
   },
   "source": [
    "**Inference:** Here in this output we can see that our last row has been dropped because it has total 3 null values which exceeded our threshold value and for other rows which have null values either equal to or less than 2 so it won't get dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nZ44PqCSsjMI"
   },
   "source": [
    "## \"SUBSET\" parameter in na.drop() function\n",
    "\n",
    "This parameter will remind us of the **pandas** as the functionality of this parameter is same as we used to pluck out specific columns from the dataset so here also we will get to know that **how we can draw a subset of specific columns from a complete dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KB5_ppdnqKai",
    "outputId": "a3eae384-563f-4cbf-d1d4-4cfc818505a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+---------------------+----------------------+\n",
      "|Employee Name|Age of Employee|Experience (in years)|Salary (per month - $)|\n",
      "+-------------+---------------+---------------------+----------------------+\n",
      "|       Oliver|             31|                   10|                 30000|\n",
      "|        Harry|             30|                    8|                 25000|\n",
      "|       George|             29|                    4|                 20000|\n",
      "|         Jack|             24|                    3|                 20000|\n",
      "|        Jacob|             21|                    1|                 15000|\n",
      "|          Leo|             23|                    2|                 18000|\n",
      "|         null|             34|                   10|                 38000|\n",
      "+-------------+---------------+---------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_null_pyspark.na.drop(how='any', subset=['Experience (in years)']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FOmpyCwLuEu3"
   },
   "source": [
    "**Inference:** In the above output we can compare that the null value which was there in the **\"Experience (in years)\"** columns is sucessfully removed and other than that column no other null value has been dropped as we used the **subset parameter**.\n",
    "\n",
    "Similarly, if we want the same thing with multiple column we can simply add more of the columns seperated by commas and inside the inverted commas and then we are good to go with multiple columns as well.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9b3Wv2_cvHHE"
   },
   "source": [
    "## Filling missing values\n",
    "\n",
    "This parameter will be responsibile to fill the **missing (NULL) values** in the dataset which is present in **na.fill()** function.\n",
    "\n",
    "* The first parameter of this function will be the **value** that needs to be **imputed** in the place of missing/ null value.\n",
    "* Second parameter is where we will mention the **name of the column/columns** on which we want to perform this imputation, this is completely **optional** as if we don't consider it then the imputation will be performed on **whole dataset**.\n",
    "\n",
    "Let's see the live example of the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G-whgC23tZlO",
    "outputId": "e8e3e896-4b51-4b32-e9f8-3d4a5f0757da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+---------------------+----------------------+\n",
      "|Employee Name|Age of Employee|Experience (in years)|Salary (per month - $)|\n",
      "+-------------+---------------+---------------------+----------------------+\n",
      "|       Oliver|             31|                   10|                 30000|\n",
      "|        Harry|             30|                    8|                 25000|\n",
      "|       George|             29|                    4|                 20000|\n",
      "|         Jack|             24|                    3|                 20000|\n",
      "|        Jacob|             21|                    1|                 15000|\n",
      "|          Leo|             23|                    2|                 18000|\n",
      "|        Oscar|           null|                 null|                 40000|\n",
      "|    NA values|             34|                   10|                 38000|\n",
      "|    NA values|             36|                 null|                  null|\n",
      "+-------------+---------------+---------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_null_pyspark.na.fill('NA values', 'Employee Name').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fxwL70syey-r"
   },
   "source": [
    "**Inference:** In the above output one can clearly see that I have utilised **both the options** i.e. imputing values as well as on specific column and got the expected results as well.\n",
    "\n",
    "Note: If we want to perform the above operation of **multiple columns** then we just need to pass the name of those columns in the **list data type.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-s8oa11afeMB"
   },
   "source": [
    "### Imputing NA values with central tendency measured\n",
    "\n",
    "This is basically something of a more professional way to handle the missing values i.e imputing the null values with mean/median/mode depending on the domain of dataset. Here we will be using the Imputer function from PySpark library to use the mean/median/mode functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "rJhXtEZ1coxj"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols = ['Age of Employee', 'Experience (in years)', 'Salary (per month - $)'],\n",
    "    outputCols = [\"{}_imputed\".format(a) for a in ['Age of Employee', 'Experience (in years)', 'Salary (per month - $)']]\n",
    ").setStrategy(\"mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q5cFRiG8iEqK"
   },
   "source": [
    "**Code breakdown:** There is lot of things going on here so let's break it down.\n",
    "\n",
    "* First we have called the **Imputer function** from **PySpark's ml.feature** library.\n",
    "* Then using that Imputer object we have defined our **input columns** as well as **output columns** in input columns we gave the name of the column which needs to be imputed and output column is the imputed one.\n",
    "* Then at the last we **set the strategy** of imputing values (here it's **mean**) but we can either use **median or mode** depending on the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whbU0MUNjFDi"
   },
   "source": [
    "### Fit and transform\n",
    "\n",
    "Now so we have used the Imputer object to impute the mean values in the place of null values but to see the changes we need to use the **fit-transform method** simulatenously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EPpeXs3zhbcO",
    "outputId": "b2a4c3bd-c981-4ab5-eb09-9fe15c4c0007",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+---------------------+----------------------+-----------------------+-----------------------------+------------------------------+\n",
      "|Employee Name|Age of Employee|Experience (in years)|Salary (per month - $)|Age of Employee_imputed|Experience (in years)_imputed|Salary (per month - $)_imputed|\n",
      "+-------------+---------------+---------------------+----------------------+-----------------------+-----------------------------+------------------------------+\n",
      "|       Oliver|             31|                   10|                 30000|                     31|                           10|                         30000|\n",
      "|        Harry|             30|                    8|                 25000|                     30|                            8|                         25000|\n",
      "|       George|             29|                    4|                 20000|                     29|                            4|                         20000|\n",
      "|         Jack|             24|                    3|                 20000|                     24|                            3|                         20000|\n",
      "|        Jacob|             21|                    1|                 15000|                     21|                            1|                         15000|\n",
      "|          Leo|             23|                    2|                 18000|                     23|                            2|                         18000|\n",
      "|        Oscar|           null|                 null|                 40000|                     28|                            5|                         40000|\n",
      "|         null|             34|                   10|                 38000|                     34|                           10|                         38000|\n",
      "|         null|             36|                 null|                  null|                     36|                            5|                         25750|\n",
      "+-------------+---------------+---------------------+----------------------+-----------------------+-----------------------------+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imputer.fit(df_null_pyspark).transform(df_null_pyspark).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QFsYW3ZgjKm-"
   },
   "source": [
    "**Inference:** Here we can see that three more columns got added at the last with postfix as **\"imputed\"** and the Null values are also replaced in those columns with **mean values** for that we have to use the **fit and transform function simultaneously** which will deliberately add the imputed columns in our DataFrame.\n",
    "\n",
    "\n",
    "**Note:** It's always a good practice to **drop the previous columns** that are still holding the NULL values as it will **hamper the data analysis and machine learning phase**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaways from the article\n",
    "\n",
    "1. First of all we did the mandatory steps which are required whenever we have to work with PySpark i.e. to start the Pyspark session and reading the dataset on which we will be performing the operations.\n",
    "\n",
    "2. Then we learned how and when to drop the complete columns from the dataset and which functions are required to do so.\n",
    "\n",
    "3. After knowing how to drop the columns we also came across how to drop the rows from the dataset depending on the business requirements.\n",
    "\n",
    "4. Then we deep dived into the different paramters of the dropping functions which let us knew that what each parameter was contributing in the function.\n",
    "\n",
    "5. At the last we learned how to impute the values using either mean, mode or median which is ome of the standard way to deal with missing values."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Data preprocessing using PySpark - Handling missing values",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
