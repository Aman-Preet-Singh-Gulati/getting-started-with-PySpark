{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xolC6wcfLUdQ"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this article we will be predicting the famous machine learning problem statement i.e. **Titanic Survival Prediction using PySpark's MLIB** this is one of the best dataset to getting started with new concepts as we being a machine learning enthusiasts already are well aware of this particular dataset and we are gonna do everything from scratch i.e. from **data preprocessing steps ,dealing with categorical variables (converting them) and building and evaluating the model using MLIB**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ut3_ADiBLaC6",
    "outputId": "fadb9fd5-2076-4fc3-e7ec-1f83300525d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting pyspark\n",
      "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 281.4 MB 34 kB/s \n",
      "\u001b[?25hCollecting py4j==0.10.9.3\n",
      "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
      "\u001b[K     |████████████████████████████████| 198 kB 50.0 MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=e46d3ebdc239acc3e532977348abdaced9e8419659979f09dce0e8a2e8854dba\n",
      "  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CM3a98pYPlJY"
   },
   "source": [
    "## Mandatory process to follow\n",
    "\n",
    "As discussed in the introduction section that we will be predicting about** which passenger survived the Titanic ship crash and for that we will be using PySpark's Mlib library**, for doing soo we need to first create and **setup an environment** to start the **Spark Session** and this will enable us to use all the required libraries which we need for the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "id": "1aNJP6-TLUdX"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "id": "52PCpkF-LUdc",
    "outputId": "8a160374-93a1-43f6-d71b-00c24d2bbf00"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://c7bfe6b2805f:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Titanic_project</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f5a11d1e910>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('Titanic_project').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HYkI63fWULk"
   },
   "source": [
    "**Code breakdown:**\n",
    "\n",
    "1. The very first step has to be to import the **SparkSession** object and for that we are importing it from **pyspark.sql** library.\n",
    "\n",
    "2. Then comes the part of building and creating the Spark Session and for that **builder** function is used to build it then for creating the same we have **getOrCreate()** method.\n",
    "\n",
    "3. To view the kind of GUI version of the session we can simply use the object name and it will show all the relevant information about the same like **version**, **app name** and **Master location**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJg7QTcXaz7f"
   },
   "source": [
    "## Reading the dataset\n",
    "\n",
    "So by far we have setup our Spark Session now it's time to read the legendary Titanic dataset and for that we will be using the **read.csv** method of PySpark, but before heading towards the coding part let's first look at the **features** that this dataset holds.\n",
    "\n",
    "1. **PassengerId:** This is just the unique ID which was assigned to each passenger.\n",
    "2. **Survived:** This is the target column which our model will predict.\n",
    "3. **Pclass:** This column holds the different class of passengers who were travelling.\n",
    "4. **Name:** Name of the passenger.\n",
    "5. **Sex:** Gender of the passenger.\n",
    "6. **Age:** Age of the passenger.\n",
    "7. **SibSp:** No. of Sibblings and Spouse of the passenger.\n",
    "8. **Parch:** Parents and no. of childern of the passenger.\n",
    "9. **Ticket:** The unique number assigned to the ticket.\n",
    "10. **Fare:** Fare of the titanic ticket based on the different criteria like which class and facilites they will get.\n",
    "11. **Cabin:** Cabin number assigned to each passenger.\n",
    "12. **Embarked:** Which port the passenger will be embarked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "id": "fiC0Z7z2LUdh"
   },
   "outputs": [],
   "source": [
    "data = spark.read.csv('titanic.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PJqmB-KGiDeZ"
   },
   "source": [
    "**Inference:** By far we are well aware of the fact that read.csv will read the dataset but here we will discuss the parameters of this method\n",
    "\n",
    "1. **inferSchema:** Notice that this param is set to **True** that means it will **return the real data type of each column** that our original data have hence keeping it True is a good practice to see the real face of the dataset.\n",
    " \n",
    "2. **header:** Keeping this parameter **True** will let the first row of the dataset as the **header of the DataFrame** otherwise the original heading will also be treated as the records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-T8eJx_RLUdj",
    "outputId": "899605f1-b701-449f-a990-cdac4ca3de87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_Ie6qJWljJZ"
   },
   "source": [
    "**Inference:** Okay! So now from the above output we got the original data type of each column and also the information that the particular column will be able to hold the **NULL** value or not. Apart from these inferences, we should notice that features like **Sex, Embarked** is in the string format so we need to change them in **categorical features**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N0tOQURdLUdk",
    "outputId": "047d7d7c-b6f1-4667-a9b9-35989617d205"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PassengerId',\n",
       " 'Survived',\n",
       " 'Pclass',\n",
       " 'Name',\n",
       " 'Sex',\n",
       " 'Age',\n",
       " 'SibSp',\n",
       " 'Parch',\n",
       " 'Ticket',\n",
       " 'Fare',\n",
       " 'Cabin',\n",
       " 'Embarked']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Z-zC3mzFpVh"
   },
   "source": [
    "**Inference:** In case one needs to find out what columns are present in the dataset then he/she can use the columns object corresponding to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "id": "HdhdtbojLUdm"
   },
   "outputs": [],
   "source": [
    "my_cols = data.select(['Survived',\n",
    " 'Pclass',\n",
    " 'Sex',\n",
    " 'Age',\n",
    " 'SibSp',\n",
    " 'Parch',\n",
    " 'Fare',\n",
    " 'Embarked'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-z9M8IzF-KN"
   },
   "source": [
    "**Inference:** In the previous DataFrame we got everything (the **target column** as well which is not required) hence here we are filtering the columns to only have **features (dependent variables)** using the **select statement.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovGk8_eMG4FZ"
   },
   "source": [
    "**Dropping NULL values**\n",
    "\n",
    "There are various method to deal with null values we can either **impute it with central tendency methods like mean/media/mode** depending on the nature of the data or we can simply **drop all the null values** here we are dropping all of them as we don't really have many of them hence it is the better option to get rid of all at once.\n",
    "\n",
    "**Note:** na.drop() method is used to drop all the NA values from the features DataFrame and then we are assigning it to new variable which would be the final data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "G8GoFCFoLUdo"
   },
   "outputs": [],
   "source": [
    "my_final_data = my_cols.na.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_rxNHCHWLUdr"
   },
   "source": [
    "### Dealing with Categorical Columns\n",
    "\n",
    "As we discussed about dealing with the categorical columns which are now in the String state but as we know String type is not accepted by any ML algorithm so we need to deal with it and for that we have to go through set of operations/steps in PySpark.\n",
    "\n",
    "So, let's break down each step and convert the necessary features columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSmv8mtoJUru"
   },
   "source": [
    "## Vector Assembler and OneHotEncoder\n",
    "\n",
    "**Vector Assembler**: From the name itself it is indicating that it kind of put together columns in a collective vectorised format i.e. **all the features get stacked up as a single unit in the form of vector** and this is one of the rule as well that MLIB library takes all the features as **single unit only**.\n",
    "\n",
    "\n",
    "**One Hot Encoder:** There are multiple ways of dealing with categorical variables this time going with One hot encoder where each **categorical value is seperated to independent column** and it get the binary value i.e. **either 0 or 1**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "id": "W9QeXcuyLUdu"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import (VectorAssembler,VectorIndexer,\n",
    "                                OneHotEncoder,StringIndexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "id": "a70jEyslLUdu"
   },
   "outputs": [],
   "source": [
    "gender_indexer = StringIndexer(inputCol='Sex',outputCol='SexIndex')\n",
    "gender_encoder = OneHotEncoder(inputCol='SexIndex',outputCol='SexVec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "id": "H-dT2X9CLUdw"
   },
   "outputs": [],
   "source": [
    "embark_indexer = StringIndexer(inputCol='Embarked',outputCol='EmbarkIndex')\n",
    "embark_encoder = OneHotEncoder(inputCol='EmbarkIndex',outputCol='EmbarkVec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "id": "GR3ThsRxLUdx"
   },
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=['Pclass',\n",
    " 'SexVec',\n",
    " 'Age',\n",
    " 'SibSp',\n",
    " 'Parch',\n",
    " 'Fare',\n",
    " 'EmbarkVec'],outputCol='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sfc01mB_T6W4"
   },
   "source": [
    "**Code breakdown:**\n",
    "\n",
    "1. As discussed that Vector assembler and One Hot encoding technique is required for conversion hence we imported both of them from **ml.feature library of PySpark**.\n",
    "\n",
    "2. While importing other important methods note that **StringIndexer** was also there which would be responsible for converting the String type to categorical type.\n",
    "\n",
    "3. Then One Hot Encoder will convert each **categorical value to its binary value i.e. 0 or 1** by it's predefined object. Repeating the same process for **\"Embarked\"** column as we did for **\"Gender\"** column.\n",
    "\n",
    "4. At the last, Vector Assembler will put together all the **preprocessed feature column together** and removing the unwanted one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "id": "XV5NubGDLUdy"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wIFNZ2KLnmG2"
   },
   "source": [
    "So we are good to go with **model development phase** and for that first thing that we need to import is ML algorithm, for this particular problem statement we have to **predict the categorical data hence classification machine learning** algorithm should be accessed i.e. **Logisitic Regression**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_v-9l_d8LUdz"
   },
   "source": [
    "## Pipelines \n",
    "\n",
    "Sometimes to cope up with the whole process of **model development is complex** and we get stuck to choose the right flow if the execution in this type of problems **Pipelines from PySpark** comes in to rescue us as it helps to maintain the **proper flow of the execution cycle** so that each step should be performed at its given stage neither before nor soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "id": "sUvkztakLUd0"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "id": "KKSspkwjLUd0"
   },
   "outputs": [],
   "source": [
    "log_reg_titanic = LogisticRegression(featuresCol='features',labelCol='Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "id": "zzAQN9gSLUd2"
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[gender_indexer,embark_indexer,\n",
    "                           gender_encoder,embark_encoder,\n",
    "                           assembler,log_reg_titanic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "id": "3TIZ3lpULUd3"
   },
   "outputs": [],
   "source": [
    "train_titanic_data, test_titanic_data = my_final_data.randomSplit([0.7,.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "id": "qSW9iZ7ALUd4"
   },
   "outputs": [],
   "source": [
    "fit_model = pipeline.fit(train_titanic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "id": "r4DGvKp1LUd6"
   },
   "outputs": [],
   "source": [
    "results = fit_model.transform(test_titanic_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZSJbfDFrpTfS"
   },
   "source": [
    "**Code breakdown:** \n",
    "\n",
    "1. First and foremost **Pipeline** module is being accessed and imported by **pyspark.ml** library.\n",
    "\n",
    "2. Then for developing the model, **Logistic Regression** method is used and in the parameters passing in the features columns and label (independent) column.\n",
    "\n",
    "3. Now comes the **Pipeline** method where one can look in the **stages** section that all the preprocessed steps are lined up one after the other.\n",
    "\n",
    "4. Then using the **randomSplit**() method the final dataset is being break down into training set of **70**% and testing set of **30**%.\n",
    "\n",
    "5. At the last it's important to have all the changes committed for that we are first fitting the pipeline with **training** data and **transforming** the **testing** data with **pipeline model**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-nobBMYcurME"
   },
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Okay! So we are now in the Model Evaluation phase that means development is already done and now we should evaluate it and from evaluating we mean that it should be working as per our requirement with good results i.e. **good accuracy over testing data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "id": "LgUmworhLUd7"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "id": "3N4KMrZMLUd8"
   },
   "outputs": [],
   "source": [
    "my_eval = BinaryClassificationEvaluator(rawPredictionCol='prediction',\n",
    "                                       labelCol='Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5okTml9pLUd9",
    "outputId": "e9cdc15f-eb6b-4dbf-8053-e4540d10c67b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|Survived|prediction|\n",
      "+--------+----------+\n",
      "|       0|       1.0|\n",
      "|       0|       1.0|\n",
      "|       0|       1.0|\n",
      "|       0|       1.0|\n",
      "|       0|       1.0|\n",
      "|       0|       1.0|\n",
      "|       0|       1.0|\n",
      "|       0|       1.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       1.0|\n",
      "|       0|       1.0|\n",
      "|       0|       0.0|\n",
      "+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.select('Survived','prediction').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7JnAv7a4GWQ"
   },
   "source": [
    "**Code breakdown:**\n",
    "\n",
    "1. Imported the **Binary Classifier** as this problem statement has the binary type of target column.\n",
    "\n",
    "2. Then by applying the **Binary Classification evaluator** object we are passing in the values to **raw prediction** column and the **label column**\n",
    "\n",
    "3. At the end when the DataFrame is having both **Survived** and **Prediction** column after the evaluation then it is shown using select statement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Here we come in the final section of this article where we will allow ourself to go along whatever we did so far in this article i.e. from starting the spark session to building and evaluating the model we will discuss each step in brief.\n",
    "\n",
    "1. Firstly we started the spark session and read the famous titanic survivals dataset using PySpark's data preprocessing techniques.\n",
    "\n",
    "2. Then, we dealt with NULL values by dropping all of them along with that we also handled the categorical features and converted them to relevant type using Vector Assembler and One Hot Encoder.\n",
    "\n",
    "3. During the next phase we came across the concept of Pipelines which helped us to build a end to end pipeline of all the stages.\n",
    "\n",
    "4. At the last we build the Logistic regresson model using PySpark's MLIB and later evaluate it too so that we should see how well our model performed based on the testing data."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "Titanic Survival Prediction using Mlib",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
