{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This is the third article in the PySpark series and in this article we will be looking at PySpark's GroupBy and Aggregate functions that could be very handy and useful when it comes to segmenting out the data according to the requirements so that it would become a bit easier task to analyse the chunks of data seperately based on the groups.\n",
    "\n",
    "If you are already following my PySpark series then it's well and good, if not then please refer to the links which I'm providing-\n",
    "\n",
    "1. Getting started with PySpark using Python\n",
    "2. Data Preprocessing using PySpark - PySpark's DataFrame\n",
    "3. Data preprocessing using PySpark - Handling missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bg1K8r3hQHV4"
   },
   "source": [
    "## Table of content \n",
    "\n",
    "1. Why Aggregate and GroupBy functions are needed? \n",
    "2. Mandatory steps\n",
    "3. PySpkark Aggregate function\n",
    "4. PySpark's Group By function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do we need GroupBy and aggregate functions\n",
    "\n",
    "Grouping out the data is one of the most essential and good to have skill whenever we are working with Big data because especially when we are dealing with huge amount of the data during that time if we are not able to segment that data then it will be way more hard to analyse it and use it further for drawing the business related insights. \n",
    "\n",
    "And when it comes to aggregate function then it is the golden rule to remember that GroupBy and Agrregate functions go hand in hand i.e. we can't use the groupBy without aggregate function like SUM, COUNT, AVG, MAX, MIN and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tCPfxtTTRIFF"
   },
   "source": [
    "Before moving to main topic of this particular article **let's do following mandatory steps.**\n",
    "\n",
    "1. Starting the Spark Session\n",
    "2. Reading the dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZoPCSXORoWY"
   },
   "source": [
    "## Mandatory Steps\n",
    "\n",
    "In this section we will get our **PySpark connection** with **Apache Spark** distribution and then we will **read our dataset** on which we will be applying the aggregate and groupby operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TL7OVoMQSGMo"
   },
   "source": [
    "### Starting the Spark Session\n",
    "\n",
    "By far if you are following my **PySpark series** then it would be easier for you to undertand that this is the **starter template** everytime we want to get started with PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ASJPfGtKUFQ7",
    "outputId": "b78d3fcb-e84c-4f39-a824-1f0d045265ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.2.1)\n",
      "Requirement already satisfied: py4j==0.10.9.3 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "id": "qhXidKF9SNtR",
    "outputId": "a560f0b5-058a-411e-d50f-cd9c5687bf66"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://0283f67c0800:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Aggreagte and GroupBy</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f7d692b53d0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark_aggregate = SparkSession.builder.appName('Aggreagte and GroupBy').getOrCreate()\n",
    "spark_aggregate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yUIBMtQ2SvS-"
   },
   "source": [
    "In a nutshell what we have done is simply imported the **SparkSession** from **`pyspark.sql`** package and created the SparkSession with **`getOrCreate()`** function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzTmkSbSTTxp"
   },
   "source": [
    "### Reading the dataset\n",
    "\n",
    "Here we will be **reading our dummy dataset** on which we will be performing the **GroupBy** and **Aggregate functions**. The reason I have choosen the dummy dataset is to provide the **simplicity in understanding the concepts**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oNnb4MkXSliz",
    "outputId": "ae135e97-5556-4b1e-e1c4-6c308ded597f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+------+\n",
      "|  Name| Departments|salary|\n",
      "+------+------------+------+\n",
      "|Oliver|Data Science| 10000|\n",
      "|Oliver|         IOT|  5000|\n",
      "| Johny|    Big Data|  4000|\n",
      "|Oliver|    Big Data|  4000|\n",
      "| Johny|Data Science|  3000|\n",
      "|Mathew|Data Science| 20000|\n",
      "|Mathew|         IOT| 10000|\n",
      "|Mathew|    Big Data|  5000|\n",
      "| Jacob|Data Science| 10000|\n",
      "| Jacob|    Big Data|  2000|\n",
      "+------+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_aggregate_data = spark_aggregate.read.csv('/content/part4.csv', header = True, inferSchema = True)\n",
    "spark_aggregate_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uXkvUhFKldGu"
   },
   "source": [
    "Here we have sucessfully **read our dummy dataset** and with the help of **show function** we can see the DataFrame too.\n",
    "\n",
    "**Note:** **Starting a spark session** and **reading the dataset** part I've already covered in my first article to this series which is, **`Getting started with PySpark using python`** so if one is not able to grab each function related to above mentioned stuff then please visit that blog too where I've seggragated each function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QdOaJHiXnGDw"
   },
   "source": [
    "Let's check the **`scehma`** of our table/dataset to see what kind of data each column holds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "615v_RtmTniX",
    "outputId": "9b35905d-1f3b-479c-dd19-8103394f9a94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Departments: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_aggregate_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZFIHHcM_ng1J"
   },
   "source": [
    "**Inference:** In the above output we can see that after using **`printScehma()`** function we have the type of each column of our dataset.\n",
    "\n",
    "1. **Name** column holds the **string** data.\n",
    "2. **Departments** column holds the **string** data.\n",
    "3. **Salary** column holds the **integer** data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njKKcLLapAVW"
   },
   "source": [
    "## GroupBy operations\n",
    "\n",
    "Now let's dive into the main topic of the blog where we will start by performing few GroupBy operations and see how PySpark can do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nx4Te9p0nbj-",
    "outputId": "4ee4aae0-2907-45c6-fc51-9205c04d84d7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.group.GroupedData at 0x7f7d6927a9d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_aggregate_data.groupBy('Name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nkxSDwLrphmg"
   },
   "source": [
    "**Inference:** In the above code for grouping by the dataset we have used the **`grouoBy() function`** and here specifically we are using the **Name** column to groupBy data and when we will see the output of the same so one can easily see that it returns the **`GroupedData`** format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iouJ6CKFqCBL"
   },
   "source": [
    "**Note:** This is pretty common thing and the one who is familiar with aggregate function of SQL they know that using the **GroupBy function without aggregate function is not possible or we can say it doesn't give the relavant output** so along with SQL this same strategy involves here as we will be using the groupBy function along with aggregate function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5FsYdoGrkC8"
   },
   "source": [
    "Before using those aggregate function with our dataset corresponding to groupBy function we will first see **some common aggregate function and what operation it performs**:\n",
    "\n",
    "1. **`AVG`:** This is the average aggregate function which returns the result set by grouping the column based on the average of set of values.\n",
    "2. **`COUNT`:** This is the count aggregate function which will return the total number of set of values in a particular column corresponding to groupBy function.\n",
    "3. **`MIN`:** This is the minimum aggregate function which returns the minimum or the smallest value among all the set of values in the whole row.\n",
    "4. **`MAX`** The working and approach of using the MAX aggregate function is same as MIN aggregate function only the main differnce is that it will return the maximum value among the set of value in the row.\n",
    "5. **`SUM`:** Now comes the SUM aggregate function which will return the sum of all the numeric values cooresponding to the grouped column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gWTyE0ILtJ3L"
   },
   "source": [
    "So now as we have discussed few most commonly used aggregate function hence we will now implement some of them and see what kind of results it will return."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OGLBJoB31czK"
   },
   "source": [
    "### GroupBy \"Name\" column\n",
    "\n",
    "In this sub-section we will discuss about the **`\"Name\"`** parameter of the GroupBy function and see how useful it could be in dealing with **summation method**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BGcyW3hupdE2",
    "outputId": "f960550b-b670-4738-8cb0-e8dc75b6ac8f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, sum(salary): bigint]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_aggregate_data.groupBy('Name').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hFOJcwlfvOo6"
   },
   "source": [
    "**Inference:** In the above code along with groupBy function we have used the sum aggregate function and it has returned as the **DataFrame** which hold two columns.\n",
    "\n",
    "* **Name:** Which holds the string data as we already know that sum cannot be applied on the string hence it will remain same.\n",
    "* **Sum:** If we look closely so we can find out that salary is **grouped with sum aggregate function** and things will get more clear when we will see the DataFrame that it had returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "afBTk6vFrJHc",
    "outputId": "8dc965d1-664f-4e72-875c-2155a627d503"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n",
      "|  Name|sum(salary)|\n",
      "+------+-----------+\n",
      "| Jacob|      12000|\n",
      "|Mathew|      35000|\n",
      "|Oliver|      19000|\n",
      "| Johny|       7000|\n",
      "+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_aggregate_data.groupBy('Name').sum().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ohkd0IRSxn3Q"
   },
   "source": [
    "**Inference:** In the above output it is clear that **Name column** has been grouped together along with the **sum of the salary column**.\n",
    "\n",
    "Note: In short we have answered one question: **Who is earning the highest salary?**\n",
    "**Answer**: It's **Jacob with 12000$** earning highest among all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jthfVRVr1ToM"
   },
   "source": [
    "Now let's find out which department is giving the maximum salary by using groupBy function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3uAFclgh1lqr"
   },
   "source": [
    "### GroupBy \"Department\" column\n",
    "\n",
    "By grouping the **Department column** and using the **sum aggregate** function along with it we can find which department gives the maximum salary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VKBtK2XwxlW7",
    "outputId": "efb029c6-e579-4628-ebbc-adf3c8880841"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "| Departments|sum(salary)|\n",
      "+------------+-----------+\n",
      "|         IOT|      15000|\n",
      "|    Big Data|      15000|\n",
      "|Data Science|      43000|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_aggregate_data.groupBy('Departments').sum().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nrHmwaZJ2BpH"
   },
   "source": [
    "**Inference:** So from the above output it is clearly visible that **Data Science department** gives the **maximum salary** while IOT and Data science gives equal salary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vMM7kS3P3Hc3"
   },
   "source": [
    "Now at the same time if we want to see the mean of the salary, department wise so we will be grouping the **department column** but this time will use the **mean aggregate function**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D5VhsKBn17pb",
    "outputId": "838a4fd1-db1a-4329-f1df-a814b928ab7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "| Departments|avg(salary)|\n",
      "+------------+-----------+\n",
      "|         IOT|     7500.0|\n",
      "|    Big Data|     3750.0|\n",
      "|Data Science|    10750.0|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_aggregate_data.groupBy('Departments').mean().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wa3SCbA83cYq"
   },
   "source": [
    "**Inference:** from the above outout we can see the mean salary from each department that employee get."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FkZttExt3pGR"
   },
   "source": [
    "Let's find out another insight by using groupBy function along with another aggregate function.\n",
    "\n",
    "**This time we will find out total number of employees in each department** and for that we will be using the **count function** along with grouping the department column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UvP1RN473YuI",
    "outputId": "6eceb0d0-eca8-46b7-f751-13592ad48b9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "| Departments|count|\n",
      "+------------+-----+\n",
      "|         IOT|    2|\n",
      "|    Big Data|    4|\n",
      "|Data Science|    4|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_aggregate_data.groupBy('Departments').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SqXD0imJ4d8E"
   },
   "source": [
    "**Inference:** Here we can see that highest number of employee is working in **Data science and Big Data department i.e. 4** while in IOT department the total count is 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCIJ07MH5G2I"
   },
   "source": [
    "Similarly we can use variety of aggregate function depending on our requirements. Suppose we need to find out that **who is getting the maximum salary ?** so for that we will groupBy **`\"Name\"`** column and use the **`\"max\"`** aggregate function and after that we will get the desired result and if the question is opposite so we will use the **`\"min\"`** function to find the **least salary of the employee**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key takeaways from this article\n",
    "\n",
    "1. First we performed the key tasks which were, setting up the spark session and reading the data on which we will be performing the operations.\n",
    "\n",
    "2. Then we head towards the main thing i.e GroupBy operations and learned about how PySpark has implement the same along with that we deep dived into the parameter part where we disucussed,\n",
    "    * Name column\n",
    "    * Department column\n",
    "    \n",
    "3. Along with groupBy operations we also discussed aggregate function simultenously because now we already know that both of them go hand in hand, some of the functions that we go through were,\n",
    "    * SUM()\n",
    "    * MEAN()\n",
    "    * COUNT()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Data Preprocessing using PySpark - Aggregate and GroupBy functions",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
