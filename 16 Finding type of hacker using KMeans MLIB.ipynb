{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gohQDBpd6v3w"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This article will be a cakewalkthrough a **consulting project** where we will be working a large technology firm to predict that **certain type of hackers were involved in hacking their servers or not!** For solving this real world problem we will take help from **PySpark's KMeans algorithm** and then based on the features that the **forensic engineers** have extracted will pave out the way to find out whether **3rd type of hacker was involved in this malicious act or not!**\n",
    "\n",
    "\n",
    "## About the dataset\n",
    "\n",
    "The dataset was actually generated after the hackers have hacked the servers in order to save the company data from such activities in future by the **forensic engineers and they grabbed some features which will give us some relevant meta data about the type of hackers.**\n",
    "\n",
    "**Here is the brief description of each features:**\n",
    "\n",
    "1. **Session connection time:** This indicates the total time **session existed in minutes**.\n",
    "2. **Bytes transferred:** This will let us know how many **mega bytes were transferred during the session**.\n",
    "3. **Kali trace used:** This is kind of flag variable which indcates that whether hacker used the **Kali linux operator**.\n",
    "4. **Servers corrupted:** How many **servers got corrupted** during the attack.\n",
    "5. **Pages corrupted:** How many pages were accessed by them illegally.\n",
    "6. **Location:** Though this meta information is also available with us but this one is of no use as **hackers use VPNs**\n",
    "7. **WPM typing speed:** **Typing speed** of those criminals based on the logs available.\n",
    "\n",
    "\n",
    "## What approach we have to follow?\n",
    "\n",
    "First let's understand what company already know, So they are aware of the fact that **there are 3 types of hackers** who might penerated the attack. They are quite sure about the **2 of them** but they want us to know whether the third type of attacker was involved in this act of **criminal or not**.\n",
    "\n",
    "One key thing we should know before moving forward i.e. forensic engineers knew that **hackers trade off**, which means the number of attacks were same from each hacker. So if there will be **3 type of hackers** then three of them might have equally distributed the attacks otherwise third suspect would have not involved this time.\n",
    "\n",
    "**Example:** If all three type of attackers were the suspect then for **100 attacks** each one will be responsible for **33**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CVMgJMSYlpNm",
    "outputId": "427ac4f2-a9d3-403b-fa5c-c818de68e04b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting pyspark\n",
      "  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 281.3 MB 44 kB/s \n",
      "\u001b[?25hCollecting py4j==0.10.9.5\n",
      "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
      "\u001b[K     |████████████████████████████████| 199 kB 48.1 MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764026 sha256=2e59b4e3fc6e0e5e5361288f36e53f701d917650c536b2477c049d035c2109e5\n",
      "  Stored in directory: /root/.cache/pip/wheels/7a/8e/1b/f73a52650d2e5f337708d9f6a1750d451a7349a867f928b885\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.5 pyspark-3.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8B9NvtSxexAj"
   },
   "source": [
    "## Mandatory steps to follow\n",
    "\n",
    "Before analysing the dataset at high level and implementing the KMeans clustering algorithm on top of it we have to follow some steps that are mentioned below:\n",
    "\n",
    "1. **Initializing the Spark object:** In this step we are gonna setup an environment for the Apache Spark so that the Spark session would be created and one can access all the libraries supported by Spark.\n",
    "\n",
    "2. **Reading the dataset:** If one have to do cooking then fire is necessary similarly before model prepration and data analysis reading the dataset is equally important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fmcbMWURqbBe"
   },
   "source": [
    "**Starting the PySpark session**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "id": "uxLOVKAr6v4L",
    "outputId": "569d004c-1c2e-4ae2-fd80-1401508645a6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://a5b86ae8280c:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>find_hackers</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f9440d367d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('find_hackers').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5JeNDd-pnWjX"
   },
   "source": [
    "**Inference:** We created the **SparkSession** named as \"**find_hackers**\" by using **getOrCreate**() function. Note that before \"**create**\" method builder method is there which sets the name of the **session/app**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Im5v6ziy6v4R",
    "outputId": "36adca01-f376-403e-ceda-5a343e7456f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-----------------+---------------+-----------------+---------------+--------------------+----------------+\n",
      "|Session_Connection_Time|Bytes Transferred|Kali_Trace_Used|Servers_Corrupted|Pages_Corrupted|            Location|WPM_Typing_Speed|\n",
      "+-----------------------+-----------------+---------------+-----------------+---------------+--------------------+----------------+\n",
      "|                    8.0|           391.09|              1|             2.96|            7.0|            Slovenia|           72.37|\n",
      "|                   20.0|           720.99|              0|             3.04|            9.0|British Virgin Is...|           69.08|\n",
      "|                   31.0|           356.32|              1|             3.71|            8.0|             Tokelau|           70.58|\n",
      "|                    2.0|           228.08|              1|             2.48|            8.0|             Bolivia|            70.8|\n",
      "|                   20.0|            408.5|              0|             3.57|            8.0|                Iraq|           71.28|\n",
      "|                    1.0|           390.69|              1|             2.79|            9.0|    Marshall Islands|           71.57|\n",
      "|                   18.0|           342.97|              1|              5.1|            7.0|             Georgia|           72.32|\n",
      "|                   22.0|           101.61|              1|             3.03|            7.0|         Timor-Leste|           72.03|\n",
      "|                   15.0|           275.53|              1|             3.53|            8.0|Palestinian Terri...|           70.17|\n",
      "|                   12.0|           424.83|              1|             2.53|            8.0|          Bangladesh|           69.99|\n",
      "|                   15.0|           249.09|              1|             3.39|            9.0|Northern Mariana ...|           70.77|\n",
      "|                   32.0|           242.48|              0|             4.24|            8.0|            Zimbabwe|           67.93|\n",
      "|                   23.0|           514.54|              0|             3.18|            8.0|         Isle of Man|           68.56|\n",
      "|                    9.0|           284.77|              0|             3.12|            9.0|Sao Tome and Prin...|           70.82|\n",
      "|                   27.0|           779.25|              1|             2.37|            8.0|              Greece|           72.73|\n",
      "|                   12.0|           307.31|              1|             3.22|            7.0|     Solomon Islands|           67.95|\n",
      "|                   21.0|           355.94|              1|              2.0|            7.0|       Guinea-Bissau|            72.0|\n",
      "|                   10.0|           372.65|              0|             3.33|            7.0|        Burkina Faso|           69.19|\n",
      "|                   20.0|           347.23|              1|             2.33|            7.0|            Mongolia|           70.41|\n",
      "|                   22.0|           456.57|              0|             1.52|            8.0|             Nigeria|           69.35|\n",
      "+-----------------------+-----------------+---------------+-----------------+---------------+--------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = spark.read.csv(\"hack_data.csv\",header=True,inferSchema=True)\n",
    "dataset.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v84VOtRMqVPl"
   },
   "source": [
    "**Inference:** Keeping the **header** and **inferSchema** as **True** so that the first tuple of record should be treated as the heading of the features column and returning the original type of data of each column as well. **In the output the code returned a DataFrame with too 20 rows of it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R5ovFLEi6v4U",
    "outputId": "5e75037f-7076-438b-b51d-46dd3105a02b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Session_Connection_Time=8.0, Bytes Transferred=391.09, Kali_Trace_Used=1, Servers_Corrupted=2.96, Pages_Corrupted=7.0, Location='Slovenia', WPM_Typing_Speed=72.37)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yRUhCGdUu7zS"
   },
   "source": [
    "**Inference:** Head is one of those method supported by PySpark which will not only return the name of the columns but also the **values associated with it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ySSh0gUJ6v4W",
    "outputId": "c1703d07-c266-49bd-91d3-a68bf8fea482"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+------------------+------------------+-----------------+------------------+-----------+------------------+\n",
      "|summary|Session_Connection_Time| Bytes Transferred|   Kali_Trace_Used|Servers_Corrupted|   Pages_Corrupted|   Location|  WPM_Typing_Speed|\n",
      "+-------+-----------------------+------------------+------------------+-----------------+------------------+-----------+------------------+\n",
      "|  count|                    334|               334|               334|              334|               334|        334|               334|\n",
      "|   mean|     30.008982035928145| 607.2452694610777|0.5119760479041916|5.258502994011977|10.838323353293413|       null|57.342395209580864|\n",
      "| stddev|     14.088200614636158|286.33593163576757|0.5006065264451406| 2.30190693339697|  3.06352633036022|       null| 13.41106336843464|\n",
      "|    min|                    1.0|              10.0|                 0|              1.0|               6.0|Afghanistan|              40.0|\n",
      "|    max|                   60.0|            1330.5|                 1|             10.0|              15.0|   Zimbabwe|              75.0|\n",
      "+-------+-----------------------+------------------+------------------+-----------------+------------------+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5EuBswvrxB06"
   },
   "source": [
    "**Inference:** Getting the statistical information is one of the key things to do while analyzing the dataset as it will tell us about the **minimum**, **maximum** value, **standard deviation** and what not!.\n",
    "\n",
    "Similarly here one inference is clearly visible that **there are no null values in the dataset (from the count row)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSaF6srw0xbd"
   },
   "source": [
    "## Vector Assembler\n",
    "\n",
    "Machine learning algorithm always accepts the **rightly formatted data** no matter what libraries we are using whether it is **scikit-learn** or as in our case **PySpark**, formatting the data is always necessary so that we only fed a right type of data to our **KMeans model**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "id": "cQibkO-16v4a"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "feat_cols = ['Session_Connection_Time', 'Bytes Transferred', 'Kali_Trace_Used',\n",
    "             'Servers_Corrupted', 'Pages_Corrupted','WPM_Typing_Speed']\n",
    "\n",
    "vec_assembler = VectorAssembler(inputCols = feat_cols, outputCol='features')\n",
    "\n",
    "final_data = vec_assembler.transform(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcnY7u7S2Ztu"
   },
   "source": [
    "**Code breakdown:**  \n",
    "\n",
    "1.  Importing the **VectorAssembler** object from the **feature** module of **ml** library.\n",
    "2. Then making a **new variable** where we will store all the **features** in the form of **list**.\n",
    "3. Then at the last calling that **assembler object** and passing the **input columns as features** along with that transforming it too so that changes should be there in **original data** as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ljXOP_Q5zwW5"
   },
   "source": [
    "## Scaling the features\n",
    "\n",
    "Scaling the features turns out to be an important steps when we have **diversity in the range of values** in our dataset i.e. **the range is quite variable** that it might leads to the condition of **curse of dimensionality** which will yeilds results but not as we expect hence now we will scale down our **feature** columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "id": "WpgwiVZb6v4i"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=False)\n",
    "\n",
    "scalerModel = scaler.fit(final_data)\n",
    "\n",
    "cluster_final_data = scalerModel.transform(final_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MkNYPbCm6ffP"
   },
   "source": [
    "**Code breakdown:** \n",
    "\n",
    "1. Importing the **StandardScaler** object and then calling the same so that we can give the input column as our features keeping the **withStd parameter as True** because here we want to scale the data in terms of **standard deviation** not with **mean**.\n",
    "\n",
    "2. Plucking out the **summary of the statistics** which is obtained by **fitting the standard scaler from fit** method.\n",
    "\n",
    "3. After **fitting the object** **transforming** is the next step where each feature will be **normalized** for unit **standard deviation** metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iXH_tqxM6v4q"
   },
   "source": [
    "**Let's find out whether k=2 is required or 3!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uHpUph2Cz_jM"
   },
   "source": [
    "## Model training \n",
    "\n",
    "Here comes the model training phase where we are gonna create a **KMeans model** to help us **create a cluster** of all three or two types of attacker involved in hacking the server. \n",
    "\n",
    "Note that we will build two model here one will be when number of **clusters is 2 and one when it's 3.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "VQzIu0wOmWWA"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "kmeans3 = KMeans(featuresCol='scaledFeatures',k=3)\n",
    "kmeans2 = KMeans(featuresCol='scaledFeatures',k=2)\n",
    "\n",
    "model_k3 = kmeans3.fit(cluster_final_data)\n",
    "model_k2 = kmeans2.fit(cluster_final_data)\n",
    "\n",
    "model_k3 = model_k3.transform(cluster_final_data)\n",
    "model_k2 = model_k2.transform(cluster_final_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UtA7aWli9dSk"
   },
   "source": [
    "**Inference:** In the above set of code we are simply creating two KMeans models one where the **k=3** and the other when **k=2** so that we can compare both the scenarios and solve the problem where we need to find out the **third hacker was involved in this malicious act or not**.\n",
    "\n",
    "**Note:** **fit** and **transform** method can be used simulataneously as they both contribute to the model building phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GcRn6OEG0HPy"
   },
   "source": [
    "## Model Evaluation\n",
    "\n",
    "After building the model evaluating it is equally important because, \"**there could be millions of model but only 1 will be actually useful**\" and to get the most optimal model we need to evaluate it based on some metrics. In the case of **KMeans model** we use the Clustering Evaluator object for evaluation purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "VJ6DfHY96v4t",
    "outputId": "9fc7c3ee-d5c3-49f1-e545-68246d28b4ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When K=3\n",
      "Error results = 0.3068084951287429\n",
      "-----------------------------------------------------\n",
      "When K=2\n",
      "Error results = 0.6683623593283755\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "k3_evaluator = evaluator.evaluate(model_k3)\n",
    "k2_evaluator = evaluator.evaluate(model_k2)\n",
    "\n",
    "print(\"When K=3\")\n",
    "print(\"Error results = \" + str(k3_evaluator))\n",
    "print('-'*53)\n",
    "print(\"When K=2\")\n",
    "print(\"Error results = \" + str(k2_evaluator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jn4h4tT-B6ac"
   },
   "source": [
    "**Inference:** After evaluation we printed the results for the cases, We can note from here that when the k value was 2 the **error results are relatively more** as compare to when k=3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1KX-JjCGDKxc"
   },
   "source": [
    "Though we should not be satisified with just checking two k values instead we have to try with multiple K values for that I have created one **for loop setup that will check further for more number of clusters at one go.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zyM6MNOq6v4x",
    "outputId": "c3f59b54-0e28-436a-e7a7-5a495d2b7a27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With K=2\n",
      "Error results  = 0.6683623593283755\n",
      "-----------------------------------------------------\n",
      "With K=3\n",
      "Error results  = 0.3068084951287429\n",
      "-----------------------------------------------------\n",
      "With K=4\n",
      "Error results  = -0.04792891045570489\n",
      "-----------------------------------------------------\n",
      "With K=5\n",
      "Error results  = -0.1047113268903205\n",
      "-----------------------------------------------------\n",
      "With K=6\n",
      "Error results  = -0.10603693913180695\n",
      "-----------------------------------------------------\n",
      "With K=7\n",
      "Error results  = -0.13283304792499523\n",
      "-----------------------------------------------------\n",
      "With K=8\n",
      "Error results  = -0.1645464293373172\n",
      "-----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for k in range(2,9):\n",
    "    kmeans = KMeans(featuresCol='scaledFeatures',k=k)\n",
    "    model = kmeans.fit(cluster_final_data)\n",
    "    model = model.transform(cluster_final_data)\n",
    "    k_evaluator = evaluator.evaluate(model)\n",
    "    print(\"With K={}\".format(k))\n",
    "    print(\"Error results  = \" + str(k_evaluator))\n",
    "    print('-'*53)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05J83EMKD8S-"
   },
   "source": [
    "**Inference:** From the above output we can see the pattern as the **number of k values increases the results are getting worse** and worse. K=2 seems to be the most optimal value of K as the error results of evaluation metrics are on the positive side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BYD3MnfWPuuo"
   },
   "source": [
    "The final evaluation will be done based on the point that we discuss earlier i.e. **hacker's trade off** - equal number of attacks be each attacker was there. To confirm this we will **groupBy prediction column to get the count of each**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0kpW4pqS6v4z",
    "outputId": "e1fa3fba-e6f9-45cd-c73a-15c280834a8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|         1|  167|\n",
      "|         2|   84|\n",
      "|         0|   83|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_k3.groupBy('prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c1nhoqb96v41",
    "outputId": "18df1b22-fab1-4151-a5db-ad6e4499cd99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|         1|  167|\n",
      "|         0|  167|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_k2.groupBy('prediction').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uh1y2RjkQuRL"
   },
   "source": [
    "**Inference:** Here we can see that equal number of predictions are there in the case where number of **clusters are only 2** hence we can **conclude that third type of attacker was not involved in hacking the servers of the company.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kLzEylSwRmvL"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "The final part of the article cum **end to end solution of a consulting project is the conclusion** where we will brief out each step so that the **pipeline of the project** is understandable and crystal clear so the one can use it as a template for other such problems statement.\n",
    "\n",
    "1. Firstly we throughly investigated what is the problem statement and clarified the approach then we move forward and completed some mandatory steps **such as reading dataset and setting up spark session.**\n",
    "\n",
    "2. After reading we did some analysis on the dataset and **formatted** it further to make it ready for **model development phase (Vector assembler and Standard Scaling).**\n",
    "\n",
    "3. At the last we **build the model and evaluated it** and came to the conclusion that third type of attacker has nothing to do with this session of hacking along with other 2 type. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "Finding type of hacker using KMeans - MLIB",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
