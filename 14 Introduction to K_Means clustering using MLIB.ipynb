{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SIQu3wIpW8xY"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "**K-means** **clustering** is one of the algorithm which **unsupervised** machine learning supports hence before moving forward with K-means let's have background knowledge of unsupervised learning method. In this method we don't really have the **predefined lables** unlike the **supervised** method hence we **don't draw predictions** but make **clusters/group** out of them so that the data could get **segmented** according to the features that are fed to model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-dL5tC2e0zmf"
   },
   "source": [
    "## What Approach does K-Means clustering follows?\n",
    "\n",
    "As discussed that this algorithm is a part of unsupervised learning so instead of making predictions we will segment that data based on the different number of clusters. K-means follow few mathematical steps which are important to discuss:\n",
    "\n",
    "**Step 1:** Selecting the number of clusters, and there are few ways to select the **appropriate number of clusters** like **elbow method** and **domain** knowledge.\n",
    "\n",
    "**Step 2:** Assigning the K points or we can also say **randomly assigning the centroids** from the dataset.\n",
    "\n",
    "**Step 3:** In the last step each K point will be adjusted closely towards their **closest centroid** that will eventually form the **clusters/group**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xQOX3FC7D154"
   },
   "source": [
    "Okay! Enough of the theories now let's get our hands dirty on implementing the K-means clustering on Spark's official clustering [dataset](https://https://github.com/apache/spark/blob/master/data/mllib/sample_kmeans_data.txt). **Though this dataset is very small and pecuilar still enough that we can explain each concept of K-means clustering using PySpark's MLIB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ow0_D3XWmV00",
    "outputId": "32bc605e-f2f0-4f1c-9676-2936ee30c03d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.3.0)\n",
      "Requirement already satisfied: py4j==0.10.9.5 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ccnALfiQIgLW"
   },
   "source": [
    "## Initiating the Spark Object\n",
    "\n",
    "This step in any spark related project is the first thing we have to do as by **creating and instantiating the PySpark environment** so that we can access all of it's utilities and functions that are required to implement the **K-means clustering algorithm.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "id": "duWPHWrYW8xy",
    "outputId": "adcf66a1-2bfd-4e83-f339-2fd08f511eaa"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://166e7f746f6d:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>intro_cluster</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fc369f052d0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('intro_cluster').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "syCKwGFiNIBD"
   },
   "source": [
    "**Inference:** First and foremost SparkSession object is called from pyspark's sql module then we created the **intro_cluster** session using the **getOrCreate() function** incorporated by **builder() function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "BwpwsmgbW8x5"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FXwTGWb5P7RS"
   },
   "source": [
    "**Inference:** In PySpark we import all the classification algorithms like **decision tree classifier, GBT classifier, Random forest classifier and logistic regression** from the **classification** class same way we are importing the **KMeans** object from **clustering** class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OIXapKPBR0N0"
   },
   "source": [
    "## Reading the dataset\n",
    "\n",
    "As mentioned earlier as well that this is the dataset which is provided by the **Spark in their official github repository**, this dataset is quite **small** so one should not expect the real world results but surely it can help to **undertand the concept of practically implementing the k-means clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ebhBoET9aWkx",
    "outputId": "b23f6779-3998-4f0d-8149-a2fba5b6ecba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|           (3,[],[])|\n",
      "|  1.0|(3,[0,1,2],[0.1,0...|\n",
      "|  2.0|(3,[0,1,2],[0.2,0...|\n",
      "|  3.0|(3,[0,1,2],[9.0,9...|\n",
      "|  4.0|(3,[0,1,2],[9.1,9...|\n",
      "|  5.0|(3,[0,1,2],[9.2,9...|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_kmeans = spark.read.format(\"libsvm\").load(\"sample_kmeans_data.txt\")\n",
    "dataset_kmeans.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uwzH4dUrUUq9"
   },
   "source": [
    "**Inference:** Here notice that we are reading the dataset after converting it in the **format of libsvm** as the dataset is in the form of libsvm and then it become easy to load. Later we are looking at dataset using **show function**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2FYjWkpbm8Bt",
    "outputId": "4fc6dce9-4f2e-491b-b258-0b4cbbab787b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=0.0, features=SparseVector(3, {})),\n",
       " Row(label=1.0, features=SparseVector(3, {0: 0.1, 1: 0.1, 2: 0.1})),\n",
       " Row(label=2.0, features=SparseVector(3, {0: 0.2, 1: 0.2, 2: 0.2})),\n",
       " Row(label=3.0, features=SparseVector(3, {0: 9.0, 1: 9.0, 2: 9.0})),\n",
       " Row(label=4.0, features=SparseVector(3, {0: 9.1, 1: 9.1, 2: 9.1}))]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_kmeans.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cv-sMxXiVgRz"
   },
   "source": [
    "**Inference:** There is one other method to have a sneak peek of the dataset i.e. the **head function** which returns all the column name with their corresponding values. Notice that features are being returned as **SparseVector**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ls0104c9nBM_",
    "outputId": "5f7c70a7-b466-454f-b559-b217f1236d75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_kmeans.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aRora8JNV85t"
   },
   "source": [
    "**Inference:** If one want to know that how the Schema of the dataset looks like then they can use the **printSchema**() function. Here it have returned two column one is the **label** and other is **features** column in the **vector format**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yzlY3lD1LHW2"
   },
   "source": [
    "## Implementation\n",
    "\n",
    "As we know that we don't predict any labels in K-means instead we used to make a clusters to segment the data in best possible way, but here comes the complex part as **there are not 100% optimal way to find best number of clusters** either one should have the amazing domain knowledge or **elbow method**, sometimes for easier problem we can give a shot to **hit and trial method**, that's what we are gonna do here:\n",
    "\n",
    "1. **When clusters = 2** i.e. when the dataset will be seperated into two groups.\n",
    "2. **When clusters = 3** i.e. when the dataset will be seperated into three groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKSyNo9lloeA"
   },
   "source": [
    "**When clusters = 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "uIKs1DyOaWhZ"
   },
   "outputs": [],
   "source": [
    "kmeans_2_cluster = KMeans().setK(2).setSeed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "klRnU_jeOLgl"
   },
   "source": [
    "**Inference:** Above code is the way how we set the number of clusters from **setK()** method. Note that we are using the **setSeed()** function as well as KMeans choose the **random number** of data for each groups so if we set an approporiate number then after each time it will execute it will take the **same random distribution**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "fUQmnSRxaWev"
   },
   "outputs": [],
   "source": [
    "first_model = kmeans_2_cluster.fit(dataset_kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ItSqh4ivO0Xq"
   },
   "source": [
    "**Inference:** Now we are **fitting the KMeans model** i.e. training the model based on the available dataset. Note that we are feeding the **complete data for training** as we don't have the labels hence there is **no point of splitting the dataset**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YWAVzG27kQJS",
    "outputId": "370ae51e-ea7f-431f-d2c0-2f908a7c8b01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+----------+\n",
      "|label|            features|prediction|\n",
      "+-----+--------------------+----------+\n",
      "|  0.0|           (3,[],[])|         1|\n",
      "|  1.0|(3,[0,1,2],[0.1,0...|         1|\n",
      "|  2.0|(3,[0,1,2],[0.2,0...|         1|\n",
      "|  3.0|(3,[0,1,2],[9.0,9...|         0|\n",
      "|  4.0|(3,[0,1,2],[9.1,9...|         0|\n",
      "|  5.0|(3,[0,1,2],[9.2,9...|         0|\n",
      "+-----+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_first_model = first_model.transform(dataset_kmeans)\n",
    "predictions_first_model.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qWCH_ny5P8Vc"
   },
   "source": [
    "**Inference:** Now it's time to make some predictions (here let's make a relevant clusters based on the input data) and for that we are using **transform** method on the whole dataset. When we saw the output we can see that **there are two groups one is tagged as 1 and other one as 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "NzrKmxVOaWcX"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "evaluator = ClusteringEvaluator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zXp74hewRXEI"
   },
   "source": [
    "**Inference:** Building the model is one thing and testing and evaluating it is more important as it will allow us to consider that this model is good to go or need any changes. Hence, **we are using the clustering evaluator object to get some Kmeans evaluation method.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hX1OflYtaWaG",
    "outputId": "4279e29d-9f4f-4a27-c91e-350fbab55114"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette evaluation results = 0.9997530305375207\n"
     ]
    }
   ],
   "source": [
    "silhouette_2_clusters = evaluator.evaluate(predictions_first_model)\n",
    "print(\"Silhouette evaluation results = \" + str(silhouette_2_clusters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vIiOWbciSIFv"
   },
   "source": [
    "**Inference:** sillhoutte is a statistical method which checks upon the **consistency within the clusters of data**. It's coefficient value ranges between **[-1,1]**  the more positive is the coefficient the better is the more the data point is within that particular clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Pq-9P0JaWVv",
    "outputId": "b96651d8-eba4-4938-ee5b-261ff18e4dcd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of clusters: \n",
      "[9.1 9.1 9.1]\n",
      "[0.1 0.1 0.1]\n"
     ]
    }
   ],
   "source": [
    "two_centroid = first_model.clusterCenters()\n",
    "print(\"Center of clusters: \")\n",
    "for c in two_centroid:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VbVQXJjQTHXe"
   },
   "source": [
    "**Inference:** To get to know about the **center/centroid of each cluster is very important as they help us to know how much seperable they are with each other**. In the output one can see that there are two clusters as we choose the number of K (clusters) as **2**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "djHhd7uVlthC"
   },
   "source": [
    "**When clusters = 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "2c5utUy_aWP7"
   },
   "outputs": [],
   "source": [
    "kmeans_3_clusters = KMeans().setK(3).setSeed(1)\n",
    "\n",
    "second_model = kmeans_3_clusters.fit(dataset_kmeans)\n",
    "\n",
    "predictions_second_model = second_model.transform(dataset_kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_icqnXVTxyJ"
   },
   "source": [
    "**Inference:** Now it's time to check how our model will perform when we choose to have three clusters and for that the process is almost the same like here we will **set the K value as 3** i.e. **3 groups** then we will **fit/train the complete data** (reason I have already discussed) at the last for drawing predictions **transform method** comes to rescue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BjauwYlXXTjl",
    "outputId": "e779d944-a1d9-4a2e-fa02-7918832d177e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette evaluation results = 0.6248737134600261\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "silhouette_3_clusters = evaluator.evaluate(predictions_second_model)\n",
    "print(\"Silhouette evaluation results = \" + str(silhouette_3_clusters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DcwCtHoFUeSU"
   },
   "source": [
    "**Inference:** Here if we will compare the Silhouette distance when clusters were 2 then **one can conclude that we should go with 2 clusters only as it is giving us better results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8BzmRt-PlHp4",
    "outputId": "62d6a68a-b4f3-47c6-da59-1fbe35be9e1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of clusters: \n",
      "[9.1 9.1 9.1]\n",
      "[0.05 0.05 0.05]\n",
      "[0.2 0.2 0.2]\n"
     ]
    }
   ],
   "source": [
    "three_centroid = second_model.clusterCenters()\n",
    "print(\"Center of clusters: \")\n",
    "for c in three_centroid:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IEW8Bhz0VKDG"
   },
   "source": [
    "**Inference:** The sole purpose of building the model with **3-K value** is to compare both of them and **choose the best possible K value** here we can see three centroid value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qMfgxvJFVqJj"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "We are in the endgame guys :) In this section we will go through everything we did soo far in this article in terms of practical implementation. **From introduction with K-means algorithm and it's way of operating we went through comparing two different cases and chose the best one.**\n",
    "\n",
    "1. Firstly we discuss how the **K-Means algorithm** works and then setup an PySpark platform so that we could implement it and get an hands on experience.\n",
    "\n",
    "2. Then we read the **official dataset** from PySpark documentation example and also **analyze the Schema** and got the basic understanding on the same.\n",
    "\n",
    "\n",
    "3. Then at the last we build the K-means model on two cases (when **clusters are 2 and 3**) and after seeing the evaluation results we concluded that for this particular data when **cluster were 2 it performed better**. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "Introduction to K-Means clustering using MLIB",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
