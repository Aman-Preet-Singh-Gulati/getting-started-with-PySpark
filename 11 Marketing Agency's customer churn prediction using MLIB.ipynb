{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ExHyqUq5Gik"
   },
   "source": [
    "https://www.kaggle.com/datasets/hassanamin/customer-churn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xtyenVG4pOOF"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Customer Churn Prediction is one of the most enlightened problem statement nowadays as possibly everything is done for the purpose of **making profit from business** and that profit comes from customers that company holds from it's products and services so the goal of the organisation is to **hold up their permanent customers and analyse the potential one who may choose other alternatives this condition is known as the customer churn**.\n",
    "\n",
    "\n",
    "In this blog we will build the potential model to predict the **customer churn** with the help of **PySpark's MLIB** library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XwhvM92Q63uF"
   },
   "source": [
    "## Problem Statement\n",
    "\n",
    "Let's not take it as just an article from now on let's suppose that we are working for a marketing agency who have hired us to draw a prediction about the potential customers who might stop buying their marketing services i.e which customer are more likely to be churned.\n",
    "\n",
    "\n",
    "## Approach\n",
    "\n",
    "As we are working on a real-world project so let's understand the flow of it. Firstly one important thing to mention is that we have the **\"new_customer\"** independent data which will eventually used as the testing data after the **model development phase**. We also need to create a **classification algorithm** which would help to classify based on the features we fed to the model that **customer will churn or not**.\n",
    "\n",
    "\n",
    "\n",
    "## About the dataset\n",
    "\n",
    "This is the data of the marketing agency which have altogether 8 features and 1 target variable. If you want to know more about this dataset then go through this [link](https://www.kaggle.com/datasets/hassanamin/customer-churn\n",
    ").\n",
    "\n",
    "1. **Name:** Name of the company whom customer is tagged to\n",
    "2. **Age:** Age of the Customer\n",
    "3. **Total_Purchase:** Total Ads Purchased \n",
    "4. **Account_Manager:** Binary 0=No manager, 1= Account manager assigned\n",
    "5. **Years:** Total Years of customer using the company service\n",
    "6. **Num_sites:** Total number of websites who are using this service.\n",
    "7. **Onboard_date:** Onboarding date of the latest contacted person.\n",
    "8. **Location:** Head Quarter address of the client\n",
    "9. **Company:** Name of Client's Company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZBLELJe8riWq",
    "outputId": "4758f29a-e51c-4fdf-e0d9-fc7b0e5eb041"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting pyspark\n",
      "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 281.4 MB 35 kB/s \n",
      "\u001b[?25hCollecting py4j==0.10.9.3\n",
      "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
      "\u001b[K     |████████████████████████████████| 198 kB 13.6 MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=d2a0febb8a9f5811b50bd9075043ff4a13a02067863ac4dbf01ed8c9473e2fc9\n",
      "  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bEynwEdFB491"
   },
   "source": [
    "## Importing libraries and starting the Spark Session\n",
    "\n",
    "Here we are starting the first phase where the required libraries is imported for **setting up the Spark environment** and starting the **Spark Session** which is always the mandatory step to get started with **PySpark**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "id": "l9YXeSSdpOOV"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "id": "cJB1VhVRpOOc",
    "outputId": "7a1f2e2e-f78a-4802-acab-a47ec0e7db98"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://d6b8f5169fc8:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>customer_churn_prediction</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f72d726d6d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('customer_churn_prediction').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FlJonen3XzaE"
   },
   "source": [
    "**Inference:** In the first step Spark Session module is imported with **pyspark.sql** library and then for building and creating the SparkSession **builder** and **getOrCreate()** methods are used respectively.\n",
    "\n",
    "Note that when we are looking the GUI version of the session then we can see the **App name**, **Version** of the Spark and the **location** where the session is created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7_oLKvCWgB0b"
   },
   "source": [
    "## Reading the dataset\n",
    "\n",
    "In this section we will be reading our dataset which includes all the **features that are required to predict which customer is most likely to be churned** and think of other alternatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "id": "FgJp_pX2pOOh"
   },
   "outputs": [],
   "source": [
    "data = spark.read.csv('customer_churn.csv',inferSchema=True,\n",
    "                     header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pPRd9vOwRN7E"
   },
   "source": [
    "**Inference:** In the above line of code we have read the **CSV** formatted data using **read.csv** function and put the **inferSchema** and **header** parameter as True so that we can see the real essence of dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_EV2rzZupOOj",
    "outputId": "2ce01ded-8b19-413a-f156-6f43238c8c78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Names: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Total_Purchase: double (nullable = true)\n",
      " |-- Account_Manager: integer (nullable = true)\n",
      " |-- Years: double (nullable = true)\n",
      " |-- Num_Sites: double (nullable = true)\n",
      " |-- Onboard_date: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Churn: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ilUQFHrcR23m"
   },
   "source": [
    "**Inference:** Printing the Schema of the data is one of the best practice to know about the **type of each column** like what **kind of data it can hold**. From the above output it is shown that **Onboard_date** is of String type so in following code if this feature is required then we should convert it to proper date format (if needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvB1jOJfpOOm"
   },
   "source": [
    "**Let's do some statistical analysis of our dataset** where describe method alone can provide lots of insights about the statistics about the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vF_tE6zGpOOn",
    "outputId": "7f8b4055-a305-4e7d-b4bf-7199fa3c3d1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+-----------------+-----------------+------------------+-----------------+------------------+-------------------+--------------------+--------------------+-------------------+\n",
      "|summary|        Names|              Age|   Total_Purchase|   Account_Manager|            Years|         Num_Sites|       Onboard_date|            Location|             Company|              Churn|\n",
      "+-------+-------------+-----------------+-----------------+------------------+-----------------+------------------+-------------------+--------------------+--------------------+-------------------+\n",
      "|  count|          900|              900|              900|               900|              900|               900|                900|                 900|                 900|                900|\n",
      "|   mean|         null|41.81666666666667|10062.82403333334|0.4811111111111111| 5.27315555555555| 8.587777777777777|               null|                null|                null|0.16666666666666666|\n",
      "| stddev|         null|6.127560416916251|2408.644531858096|0.4999208935073339|1.274449013194616|1.7648355920350969|               null|                null|                null| 0.3728852122772358|\n",
      "|    min|   Aaron King|             22.0|            100.0|                 0|              1.0|               3.0|2006-01-02 04:16:13|00103 Jeffrey Cre...|     Abbott-Thompson|                  0|\n",
      "|    max|Zachary Walsh|             65.0|         18026.01|                 1|             9.15|              14.0|2016-12-28 04:07:38|Unit 9800 Box 287...|Zuniga, Clark and...|                  1|\n",
      "+-------+-------------+-----------------+-----------------+------------------+-----------------+------------------+-------------------+--------------------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eX7l9JSiS8eq"
   },
   "source": [
    "**Inference:** The very first inference that we can draw is there are **no NULL values in the dataset** as the count is **900** for all the features hence we got rid of dealing with missing values. Then after looking at the **mean and standard deviation** of **Names** column we can conclude **string type** don't contribute anything to statistical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2fyJLYadpOOo",
    "outputId": "ab3573a1-65e7-4b88-c54f-566596479a29"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Names',\n",
       " 'Age',\n",
       " 'Total_Purchase',\n",
       " 'Account_Manager',\n",
       " 'Years',\n",
       " 'Num_Sites',\n",
       " 'Onboard_date',\n",
       " 'Location',\n",
       " 'Company',\n",
       " 'Churn']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yhdHfS4STnzR"
   },
   "source": [
    "**Inference:** The column object is used only to **get the names of all the columns that the current instance of dataset's varible** holds and in the above output one can see the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1cVPZuUTpOOq"
   },
   "source": [
    "## Feature Selection\n",
    "\n",
    "As we all are well aware the **feature selection** is one of the most important step in the data preprocessing where we select all the features that based on our knowledge would be best fit for the **model development phase**. Hence here all the valid numerical columns will be taken into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "id": "MP476faepOOu"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "id": "WPldsUkMpOOw"
   },
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=['Age',\n",
    " 'Total_Purchase',\n",
    " 'Account_Manager',\n",
    " 'Years',\n",
    " 'Num_Sites'],outputCol='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Bbmp2SoUsCl"
   },
   "source": [
    "**Inference:** While working with MLIB we should know the **format of data** which MLIB as library accepts hence we use **VectorAssembler** module which **clubs all the selected features togethere** in one column and that are treated as the feature column (summation of all the features), same thing we can see in the parameter section of the **assembler object**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "id": "y_gt8sGApOOy"
   },
   "outputs": [],
   "source": [
    "output = assembler.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uzg4sWt_VXtX"
   },
   "source": [
    "**Inference:** Transforming the data is very much necessary as it works as the **commit** statement i.e. **all the transaction (changes) which are processed should be seen in the real dataset** if we see it hence we used **transform** method for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "mF3oFtg9pOOz",
    "outputId": "26ef2f81-9338-4032-b1a3-f43fc90d7aa4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|churn|\n",
      "+--------------------+-----+\n",
      "|[42.0,11066.8,0.0...|    1|\n",
      "|[41.0,11916.22,0....|    1|\n",
      "|[38.0,12884.75,0....|    1|\n",
      "|[42.0,8010.76,0.0...|    1|\n",
      "|[37.0,9191.58,0.0...|    1|\n",
      "|[48.0,10356.02,0....|    1|\n",
      "|[44.0,11331.58,1....|    1|\n",
      "|[32.0,9885.12,1.0...|    1|\n",
      "|[43.0,14062.6,1.0...|    1|\n",
      "|[40.0,8066.94,1.0...|    1|\n",
      "|[30.0,11575.37,1....|    1|\n",
      "|[45.0,8771.02,1.0...|    1|\n",
      "|[45.0,8988.67,1.0...|    1|\n",
      "|[40.0,8283.32,1.0...|    1|\n",
      "|[41.0,6569.87,1.0...|    1|\n",
      "|[38.0,10494.82,1....|    1|\n",
      "|[45.0,8213.41,1.0...|    1|\n",
      "|[43.0,11226.88,0....|    1|\n",
      "|[53.0,5515.09,0.0...|    1|\n",
      "|[46.0,8046.4,1.0,...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_data = output.select('features','churn').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Whcg542Wkhy"
   },
   "source": [
    "**Inference:** So while looking at the above output things will get clear that what we were aiming to do as the first column is **features** which have all the **selected columns** and then the label column i.e. **churn**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xnU6DRFWpOO0"
   },
   "source": [
    "## Test Train Split\n",
    "\n",
    "Now if you are following me from the very beginning of the article might have a question that **if we already have the seperate testing data then why are spliting this dataset? right?**\n",
    "\n",
    "So the answer is keep this phase of **splitting as the validation of the model** and we do not have to perform this routine again when we would be dealing with new data as it is **already splitted** into different CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "id": "lsSNXxTJpOO0"
   },
   "outputs": [],
   "source": [
    "train_churn,test_churn = final_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dY2co78BXd6X"
   },
   "source": [
    "**Inference:** With the help of tuple unpacking we have stored the **70%** of the data in **train_churn** and **30%** of it in **test_churn** by using PySpark's **randomSplit**() method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dtcrZSzapOO1"
   },
   "source": [
    "## Model development\n",
    "\n",
    "We reaching this phase of the article is the proof that we have already **cleaned our data** completely that it is ready to be fed to the classification algorithm model (more specifically the **Logistic Regression**)\n",
    "\n",
    "**Note that we have to do this model building again when we have to deal with new customers data.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "id": "Zv2ShedJpOO2"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "id": "dZDkaEj9pOO3"
   },
   "outputs": [],
   "source": [
    "lr_churn = LogisticRegression(labelCol='churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "id": "UoQFYo0gpOO4"
   },
   "outputs": [],
   "source": [
    "fitted_churn_model = lr_churn.fit(train_churn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "id": "K2wg72UopOO4"
   },
   "outputs": [],
   "source": [
    "training_sum = fitted_churn_model.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RqS9zYK1ZP-0"
   },
   "source": [
    "**Code breakdown:** This would be a complete explanation of the steps that are required in the model building phase using MLIB\n",
    "\n",
    "1. Importing the **LogisticRegression** module from the **ml.classification** lirary of the Pyspark.\n",
    "\n",
    "2. Creating a Logistic Regression object and passing the **label column (churn).**\n",
    "\n",
    "3. **Fitting the model** i.e. starting the training of the model on the training dataset.\n",
    "\n",
    "4. Getting the **summary of the training** using the summary object which was attained over trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "61mwyIhnpOO5",
    "outputId": "f4259aa6-36a3-4532-acfa-d30ca8bf8b98"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/pyspark/sql/context.py:127: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+\n",
      "|summary|              churn|         prediction|\n",
      "+-------+-------------------+-------------------+\n",
      "|  count|                633|                633|\n",
      "|   mean| 0.1579778830963665|0.11848341232227488|\n",
      "| stddev|0.36500869525442065|0.32343524011826996|\n",
      "|    min|                0.0|                0.0|\n",
      "|    max|                1.0|                1.0|\n",
      "+-------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_sum.predictions.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RXQN6XDfaMaW"
   },
   "source": [
    "**Inference:** So the summary object of the MLIB library returned a lot insights about the **trained logistic regresion model** and with the statistical information available we can conclude that model has performed well as the **mean, standard deviation** of the **churn** (actual values) and **prediction** (predicted values) is very close."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qVRETDzBpOO5"
   },
   "source": [
    "## Model Evaluation\n",
    "\n",
    "In this stage of the churn prediction we should **analyze our model** which was trained on the **70%** of the dataset and by evaluating it we can decide that we should **go with model or some twitches are required**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "id": "khriP0WxpOO6"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "id": "D-d7EUv4pOO7"
   },
   "outputs": [],
   "source": [
    "pred_and_labels = fitted_churn_model.evaluate(test_churn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m-cN5qzgbll6"
   },
   "source": [
    "**Inference:** One can notice that in the first step we imported the **BinaryClassificationEvaluator** which is quite logical as well because we are dealing with the label column that has binary values only.\n",
    "\n",
    "Then **evaluate**() method comes into existence where it take the **testing data** (**30**% of the total dataset) as the parameter and returns the multiple fields from where we can evaluate the model (manually)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O_KRyu0gpOO7",
    "outputId": "0c2734d3-59d0-4876-a8f9-d7cf09d2c1ae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/pyspark/sql/context.py:127: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|churn|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|[26.0,8787.39,1.0...|    1|[0.26741618669989...|[0.56645847293786...|       0.0|\n",
      "|[26.0,8939.61,0.0...|    0|[5.91744417715237...|[0.99731515594254...|       0.0|\n",
      "|[27.0,8628.8,1.0,...|    0|[4.81589064770682...|[0.99196507828919...|       0.0|\n",
      "|[28.0,11204.23,0....|    0|[1.80953499926464...|[0.85930566533014...|       0.0|\n",
      "|[29.0,9378.24,0.0...|    0|[4.47613350870519...|[0.98875066870221...|       0.0|\n",
      "|[29.0,9617.59,0.0...|    0|[4.20685116490178...|[0.98532536140902...|       0.0|\n",
      "|[30.0,8403.78,1.0...|    0|[5.25931135316257...|[0.99482800491977...|       0.0|\n",
      "|[30.0,10183.98,1....|    0|[2.55476995464296...|[0.92789331160039...|       0.0|\n",
      "|[31.0,7073.61,0.0...|    0|[2.77243369914190...|[0.94116788740757...|       0.0|\n",
      "|[31.0,8829.83,1.0...|    0|[3.90380036409701...|[0.98023346398239...|       0.0|\n",
      "|[32.0,6367.22,1.0...|    0|[2.33479077516347...|[0.91171770056305...|       0.0|\n",
      "|[32.0,8617.98,1.0...|    1|[0.74934658751144...|[0.67903630719943...|       0.0|\n",
      "|[32.0,9036.27,0.0...|    0|[-0.1671587722847...|[0.45830734329068...|       1.0|\n",
      "|[32.0,11540.86,0....|    0|[6.52020026815388...|[0.99852879363429...|       0.0|\n",
      "|[32.0,11715.72,0....|    0|[3.39840257364388...|[0.96765457446972...|       0.0|\n",
      "|[32.0,12254.75,1....|    0|[2.45598304218354...|[0.92099787965996...|       0.0|\n",
      "|[33.0,7720.61,1.0...|    0|[1.28387237151430...|[0.78310821873203...|       0.0|\n",
      "|[33.0,8556.73,0.0...|    0|[3.49627045611347...|[0.97058146596041...|       0.0|\n",
      "|[33.0,10709.39,1....|    0|[5.80855726248955...|[0.99700722588455...|       0.0|\n",
      "|[33.0,12638.51,1....|    0|[3.71555846591320...|[0.97623660168760...|       0.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_and_labels.predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tve8ZhSUcT_i"
   },
   "source": [
    "**Inference:** In the above output one can see 4 columns that were returned by the evaluate method they are:\n",
    "\n",
    "1. **Features:** All the features values that were clubbed together by VectorAssembler during **feature selection phase**.\n",
    "\n",
    "2. **Churn:** The Actual values i.e. the actual **label** column\n",
    "\n",
    "3. **Probability:** This column have the **probability of the predictions** that were made by the model.\n",
    "\n",
    "4. **Predictions:** The predicted values (**here 0 or 1**) by the model on the testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o-y3xaK5pOPC"
   },
   "source": [
    "## Predicting the new data (new customers)\n",
    "\n",
    "Finally comes the last stage of the article where till now we have already **build and evaluated our model** and now here the predictions will be made on the **completely new data** i.e. the new customers dataset and see how well the model performed.\n",
    "\n",
    "Note that in this stage the steps will be same but the dataset will be different according to the situation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "id": "fpRxr3w3pOPD"
   },
   "outputs": [],
   "source": [
    "final_lr_model = lr_churn.fit(final_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qTYirRdZe2OF"
   },
   "source": [
    "**Inference:** Yes! Yes! nothing extra to discuss here as we have already gone through this step but the main thing to notice is that we are **performing the training on the complete dataset (final_data)** as we know we already have the testing data in the CSV file hence **no splitting of the dataset is required**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "id": "qJV36czQpOPE"
   },
   "outputs": [],
   "source": [
    "new_customers = spark.read.csv('new_customers.csv',inferSchema=True,\n",
    "                              header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aplVe7DgpOPF",
    "outputId": "b036b210-14cd-4bb7-a42f-950c7fde6edc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Names: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Total_Purchase: double (nullable = true)\n",
      " |-- Account_Manager: integer (nullable = true)\n",
      " |-- Years: double (nullable = true)\n",
      " |-- Num_Sites: double (nullable = true)\n",
      " |-- Onboard_date: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_customers.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1cgjB6NDfrEC"
   },
   "source": [
    "**Inference:** As the testing data is in the different file then it becomes neccesary to **read** it in the same way we did it before in the case of **customer_churn** dataset.\n",
    "\n",
    "Then we saw the Schema of this new dataset and comes to conclusion that it has the **exactly same Schema**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "id": "wCR2jQ-rpOPF"
   },
   "outputs": [],
   "source": [
    "test_new_customers = assembler.transform(new_customers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YcwOxlp1gdNR"
   },
   "source": [
    "**Inference:** Assembler object was already created while the main features were selected so now same **assembler object is being used to transform this new testing data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "id": "sOfG0qpCpOPH"
   },
   "outputs": [],
   "source": [
    "final_results = final_lr_model.transform(test_new_customers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UrZTpipWigkX"
   },
   "source": [
    "**Inference:** As we did the transformation of the features using assembler object similarly we also need to do the **transformation of the final model** on top of **new customers**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ImzdrOiPpOPH",
    "outputId": "d8630fe7-dd63-41e2-bc1c-ae8a748d0fa5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------+\n",
      "|         Company|prediction|\n",
      "+----------------+----------+\n",
      "|        King Ltd|       0.0|\n",
      "|   Cannon-Benson|       1.0|\n",
      "|Barron-Robertson|       1.0|\n",
      "|   Sexton-Golden|       1.0|\n",
      "|        Wood LLC|       0.0|\n",
      "|   Parks-Robbins|       1.0|\n",
      "+----------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_results.select('Company','prediction').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E2HS1QAEi3_k"
   },
   "source": [
    "**Inference:** Here comes the data which we were aiming to achieve where we could know that the companies like **Cannon-Benson,Barron-Robertson,Sexton-GOlden, and Parks-Robbins** need to assign an Account Manager to decrease the churn of the customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKEQ0-sukDZx"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "This is an important aspect of the article where I'll try to give brief about everything we did in this article like **how we are able to assign the Account Managers to the customers** to decrease the rate of churn in those particular companies and discuss each step in brief.\n",
    "\n",
    "1. First we read the **customer_churn** data and analyzed it both statistically and logically.\n",
    "\n",
    "2. Then we **selected the main features** that could be best fit for the model development phase after splitting the dataset (for this instance it was required)\n",
    "\n",
    "3. Then after building the model we evaluated it too using the **BinaryClassificationEvaluator** which helped us to know how well our model performed on **testing data.**\n",
    "\n",
    "\n",
    "4. Then we did the same process on top of the new dataset (**new testing data**) i.e. feature selection, model building and at the last making predictions **that at the end helped in knowing which company requires the Account Manager.**"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "Marketing Agency's customer churn prediction using MLIB",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
