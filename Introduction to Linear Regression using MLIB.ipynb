{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OeR0qpTyPki"
      },
      "source": [
        "# Introduction to Linear Regression using MLIB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVlSj-X6yPkq"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In this article we will be learning about the **Linear Regression using Mlib** and everything will be hands on i.e. we will be building an end to end Linear regression model which will predict the **customer's yearly spend on the company's product** if we talk about the dataset so it is completely a dummy dataset which is generated in purpose to undertand the concepts of **model building for continous data** using **\"MLIB\"**."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LDsi3Ys3Sc0",
        "outputId": "9dcd71fd-d5c7-4286-c592-8b677d7cb170"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 34 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.3\n",
            "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 59.6 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=b1421377ef53b43d724cfef41ed551f2f530eccd4473ea920ddc7a9d4ea468bc\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mandatory Steps:\n",
        "\n",
        "Before getting into the machine learning process and following the steps to predict the customer's yearly spend we must need to intialize the Spark Session and read our dummy dataset of Ecommerce website which have all the relevant features.\n",
        "\n",
        "1. Initializing the Spark Session\n",
        "2. Reading the dataset "
      ],
      "metadata": {
        "id": "ytrqPZ_sHUOy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setting up the spark session**\n",
        "\n",
        "In this particular section we will setup up the Spark object so that we will be able to create an environment to perfor the operations which are supported and managed by it."
      ],
      "metadata": {
        "id": "U6A-DcpRIEyg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "YY_j0YdwyPk1"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('E-commerce').getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference:** So from the above two code of lines we have sucessfully imported the **SparkSession object** from pyspark's sql package and then we have created the environment using **getOrCreate**() function and one thing to note is that before creating it we have build it using **builder** function and given it the name as **\"E-commerce\"**"
      ],
      "metadata": {
        "id": "U6QZVNvVJHny"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reading the dataset**\n",
        "\n",
        "In this section we will be **reading the dummy dataset** which I've created to perform the **ML operations** along with **Data Preprocessing using PySpark**."
      ],
      "metadata": {
        "id": "OV0_Yqf0QVJ1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "_3reGNSryPk8"
      },
      "outputs": [],
      "source": [
        "data = spark.read.csv(\"Ecommerce_Customers.csv\",inferSchema=True,header=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference:** So in the above line of code we have read the Ecommerce data and kept the **inferSchema** parameter as **True** so that it will return the real data type which dataset possess and **header** as **True** so that first tuple of record will be stated as header."
      ],
      "metadata": {
        "id": "cSz_35BaQwhs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Showing the Schema of our dataset**\n",
        "\n",
        "Here the **Schema** of the dataset will be shown so that one could get the inference of what kind of data each column holds and then the analysis could be done with more precision."
      ],
      "metadata": {
        "id": "XBWrfFdDR8ib"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QX5t9xlzyPk-",
        "outputId": "1c8ba4d1-7242-4b37-90da-0b9101d1b7c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Email: string (nullable = true)\n",
            " |-- Address: string (nullable = true)\n",
            " |-- Avatar: string (nullable = true)\n",
            " |-- Avg Session Length: double (nullable = true)\n",
            " |-- Time on App: double (nullable = true)\n",
            " |-- Time on Website: double (nullable = true)\n",
            " |-- Length of Membership: double (nullable = true)\n",
            " |-- Yearly Amount Spent: double (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference:** So we have used the **printSchema**() function to show the information about each column that our dataset holds and while looking at the output one can clearly see what kind of data type is there."
      ],
      "metadata": {
        "id": "3LTTQwhdTY3z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will go through the dataset using three different ways so that one could also know all the methods to investigate it.\n",
        "\n",
        "1. show() function\n",
        "2. head() function\n",
        "3. Iterating through each item"
      ],
      "metadata": {
        "id": "P4-TybqwV06b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the data using the **show() function** where it will return the **top 20 rows** from the complete data."
      ],
      "metadata": {
        "id": "Y5SwGRNUWb2D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JmemxB5yPlE",
        "outputId": "1fb1d48d-34a3-4507-af0a-abb239c5bd6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+----------------+------------------+------------------+------------------+--------------------+-------------------+\n",
            "|               Email|             Address|          Avatar|Avg Session Length|       Time on App|   Time on Website|Length of Membership|Yearly Amount Spent|\n",
            "+--------------------+--------------------+----------------+------------------+------------------+------------------+--------------------+-------------------+\n",
            "|mstephenson@ferna...|835 Frank TunnelW...|          Violet| 34.49726772511229| 12.65565114916675| 39.57766801952616|  4.0826206329529615|  587.9510539684005|\n",
            "|   hduke@hotmail.com|4547 Archer Commo...|       DarkGreen| 31.92627202636016|11.109460728682564|37.268958868297744|    2.66403418213262|  392.2049334443264|\n",
            "|    pallen@yahoo.com|24645 Valerie Uni...|          Bisque|33.000914755642675|11.330278057777512|37.110597442120856|   4.104543202376424| 487.54750486747207|\n",
            "|riverarebecca@gma...|1414 David Throug...|     SaddleBrown| 34.30555662975554|13.717513665142507| 36.72128267790313|   3.120178782748092|  581.8523440352177|\n",
            "|mstephens@davidso...|14023 Rodriguez P...|MediumAquaMarine| 33.33067252364639|12.795188551078114| 37.53665330059473|   4.446308318351434|  599.4060920457634|\n",
            "|alvareznancy@luca...|645 Martha Park A...|     FloralWhite|33.871037879341976|12.026925339755056| 34.47687762925054|   5.493507201364199|   637.102447915074|\n",
            "|katherine20@yahoo...|68388 Reyes Light...|   DarkSlateBlue| 32.02159550138701|11.366348309710526| 36.68377615286961|   4.685017246570912|  521.5721747578274|\n",
            "|  awatkins@yahoo.com|Unit 6538 Box 898...|            Aqua|32.739142938380326| 12.35195897300293| 37.37335885854755|  4.4342734348999375|  549.9041461052942|\n",
            "|vchurch@walter-ma...|860 Lee KeyWest D...|          Salmon| 33.98777289568564|13.386235275676436|37.534497341555735|  3.2734335777477144|  570.2004089636196|\n",
            "|    bonnie69@lin.biz|PSC 2734, Box 525...|           Brown|31.936548618448917|11.814128294972196| 37.14516822352819|   3.202806071553459|  427.1993848953282|\n",
            "|andrew06@peterson...|26104 Alexander G...|          Tomato|33.992572774953736|13.338975447662113| 37.22580613162114|   2.482607770510596|  492.6060127179966|\n",
            "|ryanwerner@freema...|Unit 2413 Box 034...|          Tomato| 33.87936082480498|11.584782999535266| 37.08792607098381|    3.71320920294043|  522.3374046069357|\n",
            "|   knelson@gmail.com|6705 Miller Orcha...|       RoyalBlue|29.532428967057943|10.961298400154098| 37.42021557502538|   4.046423164299585|  408.6403510726275|\n",
            "|wrightpeter@yahoo...|05302 Dunlap Ferr...|          Bisque| 33.19033404372265|12.959226091609382|36.144666700041924|   3.918541839158999|  573.4158673313865|\n",
            "|taylormason@gmail...|7773 Powell Sprin...|        DarkBlue|32.387975853153876|13.148725692056516| 36.61995708279922|   2.494543646659249|  470.4527333009554|\n",
            "| jstark@anderson.com|49558 Ramirez Roa...|            Peru|30.737720372628182|12.636606052000127|36.213763093698624|  3.3578468423262944|  461.7807421962299|\n",
            "| wjennings@gmail.com|6362 Wilson Mount...|      PowderBlue| 32.12538689728784|11.733861690857394|  34.8940927514398|  3.1361327164897803| 457.84769594494855|\n",
            "|rebecca45@hale-ba...|8982 Burton RowWi...|       OliveDrab|32.338899323067196|12.013194694014402| 38.38513659413844|   2.420806160901484| 407.70454754954415|\n",
            "|alejandro75@hotma...|64475 Andre Club ...|            Cyan|32.187812045932155|  14.7153875441565| 38.24411459434352|   1.516575580831944|  452.3156754800354|\n",
            "|samuel46@love-wes...|544 Alexander Hei...|   LightSeaGreen| 32.61785606282345|13.989592555825254|37.190503800397956|   4.064548550437977|   605.061038804892|\n",
            "+--------------------+--------------------+----------------+------------------+------------------+------------------+--------------------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the head function needs to be introduced which is quite similar to **head function used in pandas** in the below code's output we can see that head function returned the **Row** object which holds a one complete **record/tuple**."
      ],
      "metadata": {
        "id": "UatRZwCpXCnf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkYJ79yiyPlK",
        "outputId": "598e53aa-cf93-4227-cd8c-883f77e1df10"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Row(Email='mstephenson@fernandez.com', Address='835 Frank TunnelWrightmouth, MI 82180-9605', Avatar='Violet', Avg Session Length=34.49726772511229, Time on App=12.65565114916675, Time on Website=39.57766801952616, Length of Membership=4.0826206329529615, Yearly Amount Spent=587.9510539684005)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's see the more clear version of getting into the data where each item will be **iterable** through the combination of for loop and head function and the output shown is the more clear version of the **Row object** output."
      ],
      "metadata": {
        "id": "uVWG_LH4XgpL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrUBCeVEyPlN",
        "outputId": "a454842f-676f-4cc5-df25-540ed7c858c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mstephenson@fernandez.com\n",
            "835 Frank TunnelWrightmouth, MI 82180-9605\n",
            "Violet\n",
            "34.49726772511229\n",
            "12.65565114916675\n",
            "39.57766801952616\n",
            "4.0826206329529615\n",
            "587.9510539684005\n"
          ]
        }
      ],
      "source": [
        "for item in data.head():\n",
        "    print(item)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Linear Regression library\n",
        "\n",
        "As mentioned earlier that we will gonna predict the customer's yearly expenditure on products so based on that we already know, we have to deal with **continous data** and when we are working with such type of data we have to use the **linear regression** model.\n",
        "\n",
        "For that reason we will be importing the **Linear Regression** package from the **ML** library of **PySpark**."
      ],
      "metadata": {
        "id": "7uX8DllHKf2O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "lyLL5at8yPk5"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.regression import LinearRegression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJgMKRjCyPlR"
      },
      "source": [
        "## Data preprocessing for Machine Learning\n",
        "\n",
        "In this section all the data preprocessing techniques will be performed which are required to make the dataset ready to be sent it across the ML pipeline where the model could easily adapt it and build an efficient model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing **Vector** and **VectorAssembler** libraries so that we could easily seperate the **features** columns and the **Label** column i.e. all the dependent columns will be stacked together as the feature column and the independent column will be as a label column."
      ],
      "metadata": {
        "id": "Wuw7a8f1d678"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "BtNE5_9VyPlT"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.feature import VectorAssembler"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's have a look at which columns are present in our dataset"
      ],
      "metadata": {
        "id": "9HnG6tedgjYb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvgPUGz3yPlU",
        "outputId": "80773c11-3703-4f11-f75a-0f6075b77421"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Email',\n",
              " 'Address',\n",
              " 'Avatar',\n",
              " 'Avg Session Length',\n",
              " 'Time on App',\n",
              " 'Time on Website',\n",
              " 'Length of Membership',\n",
              " 'Yearly Amount Spent']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "data.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference:** So from the above output all the columns are listed down in the form of list type only but this will not give us the enough information about which column to select hence for that reason we will use the **describe** method."
      ],
      "metadata": {
        "id": "WvbCvhkRhGvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPx4GCyKiKNq",
        "outputId": "d915dd51-eb33-489f-8480-a055714806d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[summary: string, Email: string, Address: string, Avatar: string, Avg Session Length: string, Time on App: string, Time on Website: string, Length of Membership: string, Yearly Amount Spent: string]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference:** If you will go through the output closely you will find that, columns which have a **string as the data type will have no role in model development** phase as machine learning is the involvment of mathematical calculation where only number game is allowed hence integer and double data type columns are accepted."
      ],
      "metadata": {
        "id": "rJsX6y2zimJJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the above discussion the columns which are selected to be part of machine learning pipeline are as follows:\n",
        "\n",
        "1. Average Session Length\n",
        "2. Time on App\n",
        "3. Time on Website\n",
        "4. Length of Membership"
      ],
      "metadata": {
        "id": "lERanZK5j22M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "6U7EceZByPlW"
      },
      "outputs": [],
      "source": [
        "assembler = VectorAssembler(\n",
        "    inputCols=[\"Avg Session Length\", \"Time on App\", \n",
        "               \"Time on Website\",'Length of Membership'],\n",
        "    outputCol=\"features\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference:** In the above code we chose the VectorAssembler method to stack all our features column togethere and return them as \"features\" column by the **outputCol** parameter."
      ],
      "metadata": {
        "id": "pYlaMl3TkLpO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xrduLY9yPlX"
      },
      "outputs": [],
      "source": [
        "output = assembler.transform(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, Transform function is used to **fit the real data** with the changes that we have done in assembler variable using the VectorAssembler function so that the changes should reflect in real dataset."
      ],
      "metadata": {
        "id": "9SelgWCulXG9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L807JzXlyPlY",
        "outputId": "ae8a903b-6b29-4554-999f-42e69642aeb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|            features|\n",
            "+--------------------+\n",
            "|[34.4972677251122...|\n",
            "|[31.9262720263601...|\n",
            "|[33.0009147556426...|\n",
            "|[34.3055566297555...|\n",
            "|[33.3306725236463...|\n",
            "|[33.8710378793419...|\n",
            "|[32.0215955013870...|\n",
            "|[32.7391429383803...|\n",
            "|[33.9877728956856...|\n",
            "|[31.9365486184489...|\n",
            "|[33.9925727749537...|\n",
            "|[33.8793608248049...|\n",
            "|[29.5324289670579...|\n",
            "|[33.1903340437226...|\n",
            "|[32.3879758531538...|\n",
            "|[30.7377203726281...|\n",
            "|[32.1253868972878...|\n",
            "|[32.3388993230671...|\n",
            "|[32.1878120459321...|\n",
            "|[32.6178560628234...|\n",
            "+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "output.select(\"features\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now with **select** function we have selected only the **features** column from the dataset and showed it in the form of DataFrame using **show**() function"
      ],
      "metadata": {
        "id": "apcmfFpdyXyu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "umPFu8qLyPlc"
      },
      "outputs": [],
      "source": [
        "final_data = output.select(\"features\",'Yearly Amount Spent')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above code we are concatenating the stack of dependent features (named as features) and **independent** features together and naming it as **final_data** and this frame will be analyse further in the process."
      ],
      "metadata": {
        "id": "LLCrn7Lgy3K8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Test Split\n",
        "\n",
        "In this step of the Model building we will be dividing our data into **training set and the testing set**, where training data will be the one on top of which our model will be build and on the other hand testing data is the one on which we will test our model that how well it performed."
      ],
      "metadata": {
        "id": "NvvhLz2E0VZ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Mlib, for dividing the data into testing and training set we have to use **randomSplit**() function which take an input in the form of the **list type**."
      ],
      "metadata": {
        "id": "_MWkGfGG3Q4Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "tDhF6pFqyPld"
      },
      "outputs": [],
      "source": [
        "train_data,test_data = final_data.randomSplit([0.7,0.3])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference:** From the help of tuple unpacking concept we have stored the training set (**70**%) into train_data and similarly **30**% of the dataset into test_data. Note that in the **randomsplit**() method the list is passed."
      ],
      "metadata": {
        "id": "nLxlWScn4Gmv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzNspalyyPle",
        "outputId": "d2016818-f2e3-4bb2-82db-01377d13338d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+\n",
            "|summary|Yearly Amount Spent|\n",
            "+-------+-------------------+\n",
            "|  count|                349|\n",
            "|   mean| 501.61565245873084|\n",
            "| stddev|  79.05343373222998|\n",
            "|    min|   266.086340948469|\n",
            "|    max|  765.5184619388373|\n",
            "+-------+-------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "train_data.describe().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pua2hw_yPlf",
        "outputId": "16b4e301-35b4-40b7-9c7d-f88ffbc13215"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+\n",
            "|summary|Yearly Amount Spent|\n",
            "+-------+-------------------+\n",
            "|  count|                151|\n",
            "|   mean|  493.9944133854194|\n",
            "| stddev|  79.92486453175475|\n",
            "|    min| 256.67058229005585|\n",
            "|    max|  744.2218671047146|\n",
            "+-------+-------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_data.describe().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference:** **Describe** method seems to be an accurate way to analyse and draw the difference between training and testing data where we can see that in training set there is **349** records while **151** on the other hand."
      ],
      "metadata": {
        "id": "Z19AhGWM6O1n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Development\n",
        "\n",
        "Finally we have come across the step where we will be building our Linear Regression Model and for that **LinearRegression** object is used which if you remember we have imported in the starting and then passed the **\"Yearly Amount Spent\"** column in the **labelCol** parameter which is our **independent** column."
      ],
      "metadata": {
        "id": "YRhtsP1m64c8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5HB6rmVyPlg"
      },
      "outputs": [],
      "source": [
        "lr = LinearRegression(labelCol='Yearly Amount Spent')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, as we have created our **Linear Regression** object so now we can easily fit our data i.e. we can do the **model training** by passing the training data in the **fit** method."
      ],
      "metadata": {
        "id": "O4xQPDyJCtoJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuEXIREzyPlg"
      },
      "outputs": [],
      "source": [
        "lrModel = lr.fit(train_data,)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's print the **Coefficients** of each feature and **intercepts** of the model which is being trained on training dataset and this is also one of the information which will let you know that how well your model is involving with each independent variable seperately."
      ],
      "metadata": {
        "id": "FKS5e2_xDk_j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4hjQu3kyPli",
        "outputId": "02a78d80-6eaf-4bf9-eaf9-10706e09bb15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficients: [25.324513354618116,38.880247333555445,0.20347373150823037,61.82593066961652] Intercept: -1031.8607952442187\n"
          ]
        }
      ],
      "source": [
        "print(\"Coefficients: {} Intercept: {}\".format(lrModel.coefficients,lrModel.intercept))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Evaluation\n",
        "\n",
        "So in this step we will be **evaluating our model** i.e. We will analyze that how well our model performed and in this stage of the model building we decide whether to go with existing one or not in the **model deployment stage**."
      ],
      "metadata": {
        "id": "qNQyvAXLEeAd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So for evaluating we have come across **\"evaluate\"** function and store it in the **test_results** variable as we will use it for further analysis."
      ],
      "metadata": {
        "id": "hI4Yy1QJFfBI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPDvwr-SyPlj"
      },
      "outputs": [],
      "source": [
        "test_results = lrModel.evaluate(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The one who knows the mathematical intuition behind Linear Regression they must be aware of the fact that **residual = Original result -  Predicted result** i.e. difference between the predicted result by the model and the original result of the label column. "
      ],
      "metadata": {
        "id": "f9vXEqlaGHbv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1tKq94EyPlj",
        "outputId": "4dc47b41-b506-4933-ad17-8a1d9a8dcf34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pyspark/sql/context.py:127: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+\n",
            "|          residuals|\n",
            "+-------------------+\n",
            "|   8.93880303335402|\n",
            "| -6.031583754482824|\n",
            "| -7.850241028538278|\n",
            "| -9.098072078293853|\n",
            "| -5.381642839990491|\n",
            "| -0.193321226719263|\n",
            "| -4.383507943842062|\n",
            "| -6.621851079839303|\n",
            "| -9.652981447308719|\n",
            "|-19.370052426012762|\n",
            "|  16.93656694674837|\n",
            "| -5.377740911170633|\n",
            "|  5.678080576919854|\n",
            "| -2.432170378950275|\n",
            "| -7.623945670593628|\n",
            "|-12.779065649375752|\n",
            "|  -4.78248762240969|\n",
            "|-19.039173653509636|\n",
            "|-10.188722409023455|\n",
            "| -3.275297621379309|\n",
            "+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_results.residuals.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now its time to make predictions from our model for that we will first store the unlablled data i.e the feature data and transform it too so that changes will take place."
      ],
      "metadata": {
        "id": "UOUo6mZ2I64f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "8BAz7ezWyPlj"
      },
      "outputs": [],
      "source": [
        "unlabeled_data = test_data.select('features')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "3e_QmXPlyPlk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da4d40f3-9e63-427c-e3c1-f210c790cdc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+------------------+\n",
            "|            features|        prediction|\n",
            "+--------------------+------------------+\n",
            "|[30.7377203726281...|452.84193916287586|\n",
            "|[30.8364326747734...|473.53348418147243|\n",
            "|[31.0613251567161...| 495.4056990864399|\n",
            "|[31.1280900496166...| 566.3507588253485|\n",
            "|[31.2681042107507...| 428.8521760138144|\n",
            "|[31.3895854806643...|410.26293228670215|\n",
            "|[31.5171218025062...| 280.3019285942278|\n",
            "|[31.5257524169682...| 450.5874778897212|\n",
            "|[31.5261978982398...| 418.7475076396465|\n",
            "|[31.5702008293202...| 565.3155445674176|\n",
            "|[31.6005122003032...|462.23628454434856|\n",
            "|[31.6253601348306...| 381.7146416680948|\n",
            "|[31.6548096756927...|469.58534315062866|\n",
            "|[31.7216523605090...| 350.2090970108229|\n",
            "|[31.7242025238451...| 511.0118329585541|\n",
            "|[31.8093003166791...| 549.5509650122169|\n",
            "|[31.8124825597242...| 397.5928326062069|\n",
            "|[31.8164283341993...|  520.161665157166|\n",
            "|[31.8279790554652...|  450.191469955965|\n",
            "|[31.8627411090001...|  559.573438795426|\n",
            "+--------------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "predictions = lrModel.transform(unlabeled_data)\n",
        "predictions.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference:** So from the above output we can see that it returned a DataFrame which practically have two columns one is the complete stack of features column and the other one is **prediction** column. "
      ],
      "metadata": {
        "id": "1ptOs2HKJkPg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "So, in this section we will see by far what we have learnt in this article if I have to mention it in the nutshell then we have gone through a complete machine learning pipeline for the linear regression algorithm.\n",
        "\n",
        "1. We started the spark session and read the dataset on top of which everything was performed.\n",
        "2. Then we performed each data preprocessing step which was required to make the data ready for a ML algorithm to accept.\n",
        "3. After Data cleaning we moved towards dividing the data and later towards model building where we built Linear regression model.\n",
        "4. At the end we evaluated the model using relevant functions and predicted the results."
      ],
      "metadata": {
        "id": "7LX18ULSM4lr"
      }
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "Introduction to Linear Regression using MLIB",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}