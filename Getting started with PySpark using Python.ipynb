{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this article we will be getting our hands dirty with **`PySpark using Python`** and understand how to get started with **data preprocessing using PySpark** this particular article's whole attention is to get to know how PySpark can help in **data cleaning** process for **data engineers** and even for **data analyst** along with some basic operation of PySpark (which is the Spark distribution for Python) we will also discuss some functional thing about **`Apache Spark`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of content\n",
    "\n",
    "1. What is Apache spark?\n",
    "2. Benefits of using Apache Spark\n",
    "3. Installing and importing PySpark library\n",
    "4. Looking at the dataset\n",
    "5. Reading data using PySpark\n",
    "6. Difference between PySparks's type and Pandas's type\n",
    "7. PySpark's head() function\n",
    "8. PySpark's printScehma() function\n",
    "9. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Apache Spark?\n",
    "\n",
    "Apache Spark is a sort of engine which helps in operating and executing the **data analysis, data engineering** and **machine learning** tasks both in cloud as well as on a local machine and for that it can either use single machine or the **clusters** i.e **distributed system**.\n",
    "\n",
    "\n",
    "## Benefits of using Apache Spark\n",
    "\n",
    "We already have some relevant tools available in the market which can perform the data engineering tasks so in this section we will be discuss that why we should choose Apache Spark over its other alternatives.\n",
    "\n",
    "1. **Streaming data**: When we say streaming the data it is in the form of **`batch streaming`** and in this key feature Apache Spark will be able to stream our data in **real-time** by using our preferred programming language.\n",
    "\n",
    "\n",
    "2. **Increasing Data science scalability**: Apache Spark is one of the widely used engine for scalable computing and to perform Data science task which requires high computational power than Apache Spark should be a first choice.\n",
    "\n",
    "\n",
    "3. **Handling Big Data projects**: As previously mentioned that it have high comptational power so for that reason it can handle Big data projects in **cloud computing** as well using the **distributed systems/clusters** when working with cloud not in local machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GfHLXGL2USUj"
   },
   "source": [
    "## Installing Pyspark library using \"pip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mLX3_gaRUTTW",
    "outputId": "0b51f73a-a183-4988-f96a-254ff2823dee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 281.4 MB 34 kB/s \n",
      "\u001b[?25hCollecting py4j==0.10.9.3\n",
      "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
      "\u001b[K     |████████████████████████████████| 198 kB 57.1 MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=a968fd21d73a443b9a92d2bf66e7d3501bfbd5c25a5fb0b1f6bb730e50fb2b06\n",
      "  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ouhv2wNrUvNV"
   },
   "source": [
    "### After successfuly installing PySpark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cAsqH_0TUyDg",
    "outputId": "421f3dbe-7bac-40a6-d82f-ec08d8cda619"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.2.1)\n",
      "Requirement already satisfied: py4j==0.10.9.3 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJXtl2epU3GW"
   },
   "source": [
    "## Importing PySpark library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "j7PZTjA1U5lg"
   },
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1kQqaCPkU_-l"
   },
   "source": [
    "## Importing dataset on which we will be working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "PzqPhFQyU9V9"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('sample_data/california_housing_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "7VdWVw5vVKm8",
    "outputId": "cb3ccf6e-33e4-45a9-ce2e-25f44a32348e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-5494f931-62ae-4b21-b2ec-7f2a4d89286a\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.05</td>\n",
       "      <td>37.37</td>\n",
       "      <td>27.0</td>\n",
       "      <td>3885.0</td>\n",
       "      <td>661.0</td>\n",
       "      <td>1537.0</td>\n",
       "      <td>606.0</td>\n",
       "      <td>6.6085</td>\n",
       "      <td>344700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-118.30</td>\n",
       "      <td>34.26</td>\n",
       "      <td>43.0</td>\n",
       "      <td>1510.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>809.0</td>\n",
       "      <td>277.0</td>\n",
       "      <td>3.5990</td>\n",
       "      <td>176500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-117.81</td>\n",
       "      <td>33.78</td>\n",
       "      <td>27.0</td>\n",
       "      <td>3589.0</td>\n",
       "      <td>507.0</td>\n",
       "      <td>1484.0</td>\n",
       "      <td>495.0</td>\n",
       "      <td>5.7934</td>\n",
       "      <td>270500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-118.36</td>\n",
       "      <td>33.82</td>\n",
       "      <td>28.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6.1359</td>\n",
       "      <td>330000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-119.67</td>\n",
       "      <td>36.33</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1241.0</td>\n",
       "      <td>244.0</td>\n",
       "      <td>850.0</td>\n",
       "      <td>237.0</td>\n",
       "      <td>2.9375</td>\n",
       "      <td>81700.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5494f931-62ae-4b21-b2ec-7f2a4d89286a')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-5494f931-62ae-4b21-b2ec-7f2a4d89286a button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-5494f931-62ae-4b21-b2ec-7f2a4d89286a');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -122.05     37.37                27.0       3885.0           661.0   \n",
       "1    -118.30     34.26                43.0       1510.0           310.0   \n",
       "2    -117.81     33.78                27.0       3589.0           507.0   \n",
       "3    -118.36     33.82                28.0         67.0            15.0   \n",
       "4    -119.67     36.33                19.0       1241.0           244.0   \n",
       "\n",
       "   population  households  median_income  median_house_value  \n",
       "0      1537.0       606.0         6.6085            344700.0  \n",
       "1       809.0       277.0         3.5990            176500.0  \n",
       "2      1484.0       495.0         5.7934            270500.0  \n",
       "3        49.0        11.0         6.1359            330000.0  \n",
       "4       850.0       237.0         2.9375             81700.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ppe14AcKVRVy"
   },
   "source": [
    "Now as we have imported the dataset and also have a look of it so now let's start working with PySpark. But before getting real work with PySpark we have to start the Spark's Session and for that we need to follow some steps which are mentioned below.\n",
    "\n",
    "1. Importing the Spark Session from the Pyspark's SQL object\n",
    "2. After importing the Spark session we will build the Spark Session using the builder function of SparkSession object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9PQyIBZqVLuA"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ouBTJDT9VXlx"
   },
   "outputs": [],
   "source": [
    "spark_session = SparkSession.builder.appName('PySpark_article').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Gcv210hVxkg"
   },
   "source": [
    "Now as we can see that with the help of **`builder`** function we have first called the **`appName`** class to name our session (here I have given *\"PySpark_article\"* as the session name) and at the last for creating the session we have called **`getOrCreate()`** function and store it in the variable named as **spark_session**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "id": "BFpDhLHnVZER",
    "outputId": "20519b71-9e42-4314-af7e-ce581cb06a13"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://0ef7a73396f8:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySpark_article</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fa567419e90>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ndbwy3DHWt-m"
   },
   "source": [
    "Now when we see that what our spark session will hold it will return the above output which have the following components:\n",
    "\n",
    "1. About the spark session : In memory\n",
    "2. Spark context:\n",
    "  - **Version:** It will return the current version of the spark which we are using - v3.2.1\n",
    "  - **Master:** Interesting thing to notice here is when we will be working in the cloud then we might have different **`clusters`** as well like first there will be master and then a tree like structure (cluster_1, cluster_2... cluster_n) but here as we are working on **local **system and not the **distributed** one so it is returning **`local`**.\n",
    "  - **AppName:** And finally the name of the app (spark session) which we gave while declaring it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MCNgn2xaYx3J"
   },
   "source": [
    "## Reading the data using spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "rNFRCz6rVbt7"
   },
   "outputs": [],
   "source": [
    "df_spark = spark_session.read.csv('sample_data/california_housing_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6JZn9KRrmbe6"
   },
   "source": [
    "In the above code we can see that spark uses spark session variable to call the **`read.csv()`** function to read the data when it is in CSV format now if you remember when we need to read the csv file in padas we used to call **`read_csv()`**. \n",
    "\n",
    "According to me when we are learning something new which has stuffs related to previous learning then it is good to compare both the stuff so in this article we will also compare the **pandas data processing** with **spark's data processing**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TLVKpr1YZF2x",
    "outputId": "44d0b4a9-cdb6-4bb4-ca29-d30d924ff4bc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string, _c8: string]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qiGaCQ-bnqVR"
   },
   "source": [
    "Now if we look at the output so it shows that it has returned the **DataFrame** in which we can see the dictionary like setup where (c0, c1, c2.....cn) are the number of columns and corresponding to each column index we can see the type of that column i.e. **String**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n1OjqJIEZMwy",
    "outputId": "3a9544ca-6bdb-40e3-f287-2e5ba2a84adf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+------------------+-----------+--------------+-----------+-----------+-------------+------------------+\n",
      "|        _c0|      _c1|               _c2|        _c3|           _c4|        _c5|        _c6|          _c7|               _c8|\n",
      "+-----------+---------+------------------+-----------+--------------+-----------+-----------+-------------+------------------+\n",
      "|  longitude| latitude|housing_median_age|total_rooms|total_bedrooms| population| households|median_income|median_house_value|\n",
      "|-114.310000|34.190000|         15.000000|5612.000000|   1283.000000|1015.000000| 472.000000|     1.493600|      66900.000000|\n",
      "|-114.470000|34.400000|         19.000000|7650.000000|   1901.000000|1129.000000| 463.000000|     1.820000|      80100.000000|\n",
      "|-114.560000|33.690000|         17.000000| 720.000000|    174.000000| 333.000000| 117.000000|     1.650900|      85700.000000|\n",
      "|-114.570000|33.640000|         14.000000|1501.000000|    337.000000| 515.000000| 226.000000|     3.191700|      73400.000000|\n",
      "|-114.570000|33.570000|         20.000000|1454.000000|    326.000000| 624.000000| 262.000000|     1.925000|      65500.000000|\n",
      "|-114.580000|33.630000|         29.000000|1387.000000|    236.000000| 671.000000| 239.000000|     3.343800|      74000.000000|\n",
      "|-114.580000|33.610000|         25.000000|2907.000000|    680.000000|1841.000000| 633.000000|     2.676800|      82400.000000|\n",
      "|-114.590000|34.830000|         41.000000| 812.000000|    168.000000| 375.000000| 158.000000|     1.708300|      48500.000000|\n",
      "|-114.590000|33.610000|         34.000000|4789.000000|   1175.000000|3134.000000|1056.000000|     2.178200|      58400.000000|\n",
      "|-114.600000|34.830000|         46.000000|1497.000000|    309.000000| 787.000000| 271.000000|     2.190800|      48100.000000|\n",
      "|-114.600000|33.620000|         16.000000|3741.000000|    801.000000|2434.000000| 824.000000|     2.679700|      86500.000000|\n",
      "|-114.600000|33.600000|         21.000000|1988.000000|    483.000000|1182.000000| 437.000000|     1.625000|      62000.000000|\n",
      "|-114.610000|34.840000|         48.000000|1291.000000|    248.000000| 580.000000| 211.000000|     2.157100|      48600.000000|\n",
      "|-114.610000|34.830000|         31.000000|2478.000000|    464.000000|1346.000000| 479.000000|     3.212000|      70400.000000|\n",
      "|-114.630000|32.760000|         15.000000|1448.000000|    378.000000| 949.000000| 300.000000|     0.858500|      45000.000000|\n",
      "|-114.650000|34.890000|         17.000000|2556.000000|    587.000000|1005.000000| 401.000000|     1.699100|      69100.000000|\n",
      "|-114.650000|33.600000|         28.000000|1678.000000|    322.000000| 666.000000| 256.000000|     2.965300|      94900.000000|\n",
      "|-114.650000|32.790000|         21.000000|  44.000000|     33.000000|  64.000000|  27.000000|     0.857100|      25000.000000|\n",
      "|-114.660000|32.740000|         17.000000|1388.000000|    386.000000| 775.000000| 320.000000|     1.204900|      44000.000000|\n",
      "+-----------+---------+------------------+-----------+--------------+-----------+-----------+-------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zx1-YiyrqKJe"
   },
   "source": [
    "From the above output we can compare the PySpark's **`show()`** function with the pandas's **`head()`** function. \n",
    "\n",
    "* In head() function we can see the **top 5 records** (unless we don't specify it in the arguments) wheras in show() function it returns top 20 records which is mentioned too at the last.\n",
    "\n",
    "* Another difference that we can notice is the **appearance of the tabular data** that we can see using both the function one can compare it as in the start of the article I've used head() function.\n",
    "\n",
    "* One more major difference that one can point out which is also a drawback of PySpark's **show()** function i.e. when we are looking at the column names it is showing (c0, c1, c2.....cn) and the exact column names are shown as the **first tuple** of records but we can fix this issue as well.\n",
    "\n",
    "\n",
    "So let's fix it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RaEg1IbLkDxo",
    "outputId": "28dcd09f-ea13-4312-dfc8-3c3d3013b36f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[longitude: string, latitude: string, housing_median_age: string, total_rooms: string, total_bedrooms: string, population: string, households: string, median_income: string, median_house_value: string]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark_col  = spark_session.read.option('header', 'true').csv('sample_data/california_housing_train.csv')\n",
    "df_spark_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7XHDrNRatfPd"
   },
   "source": [
    "Okay! so now just after looking at the output we can say that we have fixed that problem (we will still confirm that later) as in the output instead of (c0,c1,c2....cn) in the place of column name now we can see the actual name of the columns.\n",
    "\n",
    "Let's confirm it by looking at the complete data with records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zHYhRiTmtEek",
    "outputId": "91c9098a-ebf2-4013-f257-bef62fcbd2a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+------------------+-----------+--------------+-----------+-----------+-------------+------------------+\n",
      "|  longitude| latitude|housing_median_age|total_rooms|total_bedrooms| population| households|median_income|median_house_value|\n",
      "+-----------+---------+------------------+-----------+--------------+-----------+-----------+-------------+------------------+\n",
      "|-114.310000|34.190000|         15.000000|5612.000000|   1283.000000|1015.000000| 472.000000|     1.493600|      66900.000000|\n",
      "|-114.470000|34.400000|         19.000000|7650.000000|   1901.000000|1129.000000| 463.000000|     1.820000|      80100.000000|\n",
      "|-114.560000|33.690000|         17.000000| 720.000000|    174.000000| 333.000000| 117.000000|     1.650900|      85700.000000|\n",
      "|-114.570000|33.640000|         14.000000|1501.000000|    337.000000| 515.000000| 226.000000|     3.191700|      73400.000000|\n",
      "|-114.570000|33.570000|         20.000000|1454.000000|    326.000000| 624.000000| 262.000000|     1.925000|      65500.000000|\n",
      "|-114.580000|33.630000|         29.000000|1387.000000|    236.000000| 671.000000| 239.000000|     3.343800|      74000.000000|\n",
      "|-114.580000|33.610000|         25.000000|2907.000000|    680.000000|1841.000000| 633.000000|     2.676800|      82400.000000|\n",
      "|-114.590000|34.830000|         41.000000| 812.000000|    168.000000| 375.000000| 158.000000|     1.708300|      48500.000000|\n",
      "|-114.590000|33.610000|         34.000000|4789.000000|   1175.000000|3134.000000|1056.000000|     2.178200|      58400.000000|\n",
      "|-114.600000|34.830000|         46.000000|1497.000000|    309.000000| 787.000000| 271.000000|     2.190800|      48100.000000|\n",
      "|-114.600000|33.620000|         16.000000|3741.000000|    801.000000|2434.000000| 824.000000|     2.679700|      86500.000000|\n",
      "|-114.600000|33.600000|         21.000000|1988.000000|    483.000000|1182.000000| 437.000000|     1.625000|      62000.000000|\n",
      "|-114.610000|34.840000|         48.000000|1291.000000|    248.000000| 580.000000| 211.000000|     2.157100|      48600.000000|\n",
      "|-114.610000|34.830000|         31.000000|2478.000000|    464.000000|1346.000000| 479.000000|     3.212000|      70400.000000|\n",
      "|-114.630000|32.760000|         15.000000|1448.000000|    378.000000| 949.000000| 300.000000|     0.858500|      45000.000000|\n",
      "|-114.650000|34.890000|         17.000000|2556.000000|    587.000000|1005.000000| 401.000000|     1.699100|      69100.000000|\n",
      "|-114.650000|33.600000|         28.000000|1678.000000|    322.000000| 666.000000| 256.000000|     2.965300|      94900.000000|\n",
      "|-114.650000|32.790000|         21.000000|  44.000000|     33.000000|  64.000000|  27.000000|     0.857100|      25000.000000|\n",
      "|-114.660000|32.740000|         17.000000|1388.000000|    386.000000| 775.000000| 320.000000|     1.204900|      44000.000000|\n",
      "|-114.670000|33.920000|         17.000000|  97.000000|     24.000000|  29.000000|  15.000000|     1.265600|      27500.000000|\n",
      "+-----------+---------+------------------+-----------+--------------+-----------+-----------+-------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark = spark_session.read.option('header', 'true').csv('sample_data/california_housing_train.csv').show()\n",
    "df_spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezRVkbIUucOM"
   },
   "source": [
    "Yes! Now we can see that in the place of the columns we can now see the actual name of the column instead that indexing formatting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ayR5nS2FuvKe"
   },
   "source": [
    "### Difference between PySparks's type and Pandas's type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W9hhNE4ruOqc",
    "outputId": "4fea433a-1ccc-4c95-c68f-7562bb401ebc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_spark_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IbO-C3kbvpL-"
   },
   "source": [
    "Here you can see that PySpark's dataframe belongs to pyspark's main class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IBWr5BElu4Vb",
    "outputId": "c050cc1a-7762-43a7-e45b-9934b31401a0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_VGICfnv27L"
   },
   "source": [
    "And you guessed that right, similarly, Pandas's dataframe is of pandas type\n",
    "\n",
    "**Note:** One interesting fact about PySpark's data frame is that it can work on both head and show function while pandas don't work on show function instead only for head funtion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark's head() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9MTWuuGZvKMY",
    "outputId": "8c017226-52d5-4e71-e5b2-8e8d15aeacf1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(longitude='-114.310000', latitude='34.190000', housing_median_age='15.000000', total_rooms='5612.000000', total_bedrooms='1283.000000', population='1015.000000', households='472.000000', median_income='1.493600', median_house_value='66900.000000'),\n",
       " Row(longitude='-114.470000', latitude='34.400000', housing_median_age='19.000000', total_rooms='7650.000000', total_bedrooms='1901.000000', population='1129.000000', households='463.000000', median_income='1.820000', median_house_value='80100.000000'),\n",
       " Row(longitude='-114.560000', latitude='33.690000', housing_median_age='17.000000', total_rooms='720.000000', total_bedrooms='174.000000', population='333.000000', households='117.000000', median_income='1.650900', median_house_value='85700.000000'),\n",
       " Row(longitude='-114.570000', latitude='33.640000', housing_median_age='14.000000', total_rooms='1501.000000', total_bedrooms='337.000000', population='515.000000', households='226.000000', median_income='3.191700', median_house_value='73400.000000'),\n",
       " Row(longitude='-114.570000', latitude='33.570000', housing_median_age='20.000000', total_rooms='1454.000000', total_bedrooms='326.000000', population='624.000000', households='262.000000', median_income='1.925000', median_house_value='65500.000000'),\n",
       " Row(longitude='-114.580000', latitude='33.630000', housing_median_age='29.000000', total_rooms='1387.000000', total_bedrooms='236.000000', population='671.000000', households='239.000000', median_income='3.343800', median_house_value='74000.000000'),\n",
       " Row(longitude='-114.580000', latitude='33.610000', housing_median_age='25.000000', total_rooms='2907.000000', total_bedrooms='680.000000', population='1841.000000', households='633.000000', median_income='2.676800', median_house_value='82400.000000'),\n",
       " Row(longitude='-114.590000', latitude='34.830000', housing_median_age='41.000000', total_rooms='812.000000', total_bedrooms='168.000000', population='375.000000', households='158.000000', median_income='1.708300', median_house_value='48500.000000'),\n",
       " Row(longitude='-114.590000', latitude='33.610000', housing_median_age='34.000000', total_rooms='4789.000000', total_bedrooms='1175.000000', population='3134.000000', households='1056.000000', median_income='2.178200', median_house_value='58400.000000'),\n",
       " Row(longitude='-114.600000', latitude='34.830000', housing_median_age='46.000000', total_rooms='1497.000000', total_bedrooms='309.000000', population='787.000000', households='271.000000', median_income='2.190800', median_house_value='48100.000000')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark_col.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W3d1PDipwgiB"
   },
   "source": [
    "As we can see that we get the output but it is not in the Tabular format which we can see in the pandas's head function instead it return the output in the classical CSV format that's why though it can work for both show and head function still we should use the show function for better readibility of the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hmYdmKKNxKMy"
   },
   "source": [
    "Do Pyspark has the alternative to Pandas's info function ?\n",
    "Answer is Yes! \n",
    "Let' have a look at **`printSchema()`** function of PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark's printScehma() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "52b9zsWxwa0C",
    "outputId": "c08acb6b-a4e6-4622-c4ab-b663da69c956"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- housing_median_age: string (nullable = true)\n",
      " |-- total_rooms: string (nullable = true)\n",
      " |-- total_bedrooms: string (nullable = true)\n",
      " |-- population: string (nullable = true)\n",
      " |-- households: string (nullable = true)\n",
      " |-- median_income: string (nullable = true)\n",
      " |-- median_house_value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark_col.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "agvlTM9ZxkdM"
   },
   "source": [
    "If we are able to distinguish well then we can easily figure out from the above output that it is returning the **name of the column**, **type of the data** each column is holding and are there any **null values** in each column (nullable = True - Yes it have null values) just like our old friend - **info()**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Finally we have come to the last section of this article where we will sum up all the things that we have covered so far in a short description.\n",
    "\n",
    "1. In the very first segment we learned about the Apache Spark and its benefits over other alternatives.\n",
    "2. Then we saw how to install the PySpark library which is the Python distribution of Apache Spark.\n",
    "3. Then down to 3rd segment of the article we got our hands dirty when we implement those basic functions and operations by which we get started to do data preprocessing using PySpark library"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Getting started with PySpark using Python",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
