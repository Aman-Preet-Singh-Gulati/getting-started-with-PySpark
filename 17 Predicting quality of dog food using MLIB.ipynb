{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cVv-uQNY5EiA"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "We are keeping forward with the PySpark series where by far we covered **Data preprocessing techniques** and various **ML agorithm** along with real-world **consulting projects**. In this article as well we will work upon another consulting project, let's take a scenario, Suppose we have been **hired by a Dog Food company** and our task is to predict that why their manufactured food is being **spoiling rapdily comparing to their shelves life** and we will solve this particular problem statement using **PySpark's MLIB**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H1V0cgAgKyxY"
   },
   "source": [
    "## About the Problem statement\n",
    "\n",
    "From the introduction part we are well aware that **\"what\"** needs to be done but in this section we will dig more to understand the **\"how\"** and **\"why\"** part of this project.\n",
    "\n",
    "**Why Dog Food company needs us?**\n",
    "- From last few suppy chain tenure they are regularly facing the **pre-spoiling of the dog food** and they have figured out the reason as well, as they have **not upgraded to the latest machineries** so the **four secret ingredients** are not mixing up well. But they are not able to figure out that among those **4 chemicals which one is responsible or have the strongest effect.** \n",
    "\n",
    "\n",
    "**How we are gonna approach this problem statement?**\n",
    "- Our main task is to predict that **1 chemical/preservatives has the strongest effect** among those **4 ingredients** and to achieve this we are not gonna follow **train test split** methodology instead **feature importance** method because in the end that only will let us know which of those ingredients is most responsible for **spoiling the dog food** before its shelf life.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BpjRSzHMfbPM"
   },
   "source": [
    "## About the dataset\n",
    "\n",
    "This dataset holds **4 feature columns** labelled as : **A,B,C and D** and the one is **Target** column labelled as **\"Spoiled\"**. So total of **5 columns** are there in the dataset. Let's look at the short description of each column.\n",
    "\n",
    "1. **Preservative_A:** Percentage of A ingredient in the mixture.\n",
    "2. **Preservative_B:** Percentage of B ingredient in the mixture.\n",
    "3. **Preservative_C:** Percentage of C ingredient in the mixture.\n",
    "4. **Preservative_D:** Percentage of D ingredient in the mixture.\n",
    "\n",
    "\n",
    "Here you can find the source of the [dataset](https://https://github.com/SkalskiP/pySpark_Tutorial/blob/master/Sekcja_13_Decision_Trees_and_Random_Forests/dog_food.csv).\n",
    "\n",
    "**Note:** In this particular project **we will not follow that generalised method of machine learning pipeline (train-test-split)** instead we will go with other method which you will find out while moving on with this article and that will help you to **draw another template for such problems.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vwr_hyXp4kC-"
   },
   "source": [
    "**Installing PySpark:** To do the predictive analysis on the spoiling chemical we just need to install one library which is heart and soul of this project i.e. **PySpark** that will eventually set up an environment for the **MLIB library** and **established a connection with Apache Spark**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4-Az8sG0CM8D",
    "outputId": "91fed8d8-7427-4226-e087-8a93b060fcdf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting pyspark\n",
      "  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 281.3 MB 38 kB/s \n",
      "\u001b[?25hCollecting py4j==0.10.9.5\n",
      "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
      "\u001b[K     |████████████████████████████████| 199 kB 56.9 MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764026 sha256=3793c0cc5561bff60a517f42489e4bbcdc50d777a91475c885ceb2d13b97726e\n",
      "  Stored in directory: /root/.cache/pip/wheels/7a/8e/1b/f73a52650d2e5f337708d9f6a1750d451a7349a867f928b885\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.5 pyspark-3.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUTYKLDs5QMs"
   },
   "source": [
    "## Spark Session\n",
    "\n",
    "In this part of the article we will start the **Spark Session** because this is one of those mandatory process where we setup the environment with the **Apache Spark** by creating and new session via **PySpark**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "id": "FQf7TMY05EiN",
    "outputId": "e2a75301-28e4-4022-9433-400d1d460115"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://4b0490ca603b:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>dog_food_project</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f519b4854d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('dog_food_project').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JBd0c2L6pUb"
   },
   "source": [
    "**Inference:** First and foremost the **Spark Session** library is imported from the **pyspark.sql** library. \n",
    "\n",
    "Then comes the role of builder function that will **build the session** (providing that naming functionality too - **dog_food_project**) after building it we created the SparkSession using **getOrCreate()** function. \n",
    "\n",
    "At the last **calling the spark object** we can see the **UI** of the **SparkMemory** that summarizes the whole process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZBV2LwB-8ENQ"
   },
   "source": [
    "## Reading the dog food dataset\n",
    "\n",
    "Here is yet another compulsory step to be followed because any data science project is impossible to carry on without the relevant dataset it's like \"***trying to build the house without considering bricks***\". Hence one can refer the below code to read the dataset which is in the CSV format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "18NJ7-Ir5EiQ",
    "outputId": "f391962d-0fe5-41e5-eadd-870f58bf3244"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+---+-------+\n",
      "|  A|  B|   C|  D|Spoiled|\n",
      "+---+---+----+---+-------+\n",
      "|  4|  2|12.0|  3|    1.0|\n",
      "|  5|  6|12.0|  7|    1.0|\n",
      "|  6|  2|13.0|  6|    1.0|\n",
      "|  4|  2|12.0|  1|    1.0|\n",
      "|  4|  2|12.0|  3|    1.0|\n",
      "| 10|  3|13.0|  9|    1.0|\n",
      "|  8|  5|14.0|  5|    1.0|\n",
      "|  5|  8|12.0|  8|    1.0|\n",
      "|  6|  5|12.0|  9|    1.0|\n",
      "|  3|  3|12.0|  1|    1.0|\n",
      "|  9|  8|11.0|  3|    1.0|\n",
      "|  1| 10|12.0|  3|    1.0|\n",
      "|  1|  5|13.0| 10|    1.0|\n",
      "|  2| 10|12.0|  6|    1.0|\n",
      "|  1| 10|11.0|  4|    1.0|\n",
      "|  5|  3|12.0|  2|    1.0|\n",
      "|  4|  9|11.0|  8|    1.0|\n",
      "|  5|  1|11.0|  1|    1.0|\n",
      "|  4|  9|12.0| 10|    1.0|\n",
      "|  5|  8|10.0|  9|    1.0|\n",
      "+---+---+----+---+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_food = spark.read.csv('dog_food.csv',inferSchema=True,header=True)\n",
    "data_food.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eNjVhd4G9PPh"
   },
   "source": [
    "**Inference:** From the above output we have confirmed what we stated regarding the dataset in About section i.e. it have **4 ingredients/chemicals** (**A,B,C,D**) and one target variable i.e. **Spoiled**.\n",
    "\n",
    "For doing so, **read.csv function** was used, keeping the **inferSchema** and **header** parameter as **True** so that it can return the relevant type of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S7KC82gm5EiR",
    "outputId": "80a0c4ef-4c07-43f3-997d-e93e0da5d5d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- A: integer (nullable = true)\n",
      " |-- B: integer (nullable = true)\n",
      " |-- C: double (nullable = true)\n",
      " |-- D: integer (nullable = true)\n",
      " |-- Spoiled: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_food.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8m31g4OOAjE5"
   },
   "source": [
    "**Inference:** Just before this step while reading the dataset we put the **inferSchema** parameter value to **True** so that while performing the **printSchema** function we can get the right data type of each features. Hence all **4 feature** has the **integer** type and **Spoiled** (**target**) holding the **double** type of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mL4CLzbR5EiU",
    "outputId": "99ca2c0d-761a-4e04-9581-80e7c3cd9dc8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(A=4, B=2, C=12.0, D=3, Spoiled=1.0),\n",
       " Row(A=5, B=6, C=12.0, D=7, Spoiled=1.0),\n",
       " Row(A=6, B=2, C=13.0, D=6, Spoiled=1.0),\n",
       " Row(A=4, B=2, C=12.0, D=1, Spoiled=1.0),\n",
       " Row(A=4, B=2, C=12.0, D=3, Spoiled=1.0),\n",
       " Row(A=10, B=3, C=13.0, D=9, Spoiled=1.0),\n",
       " Row(A=8, B=5, C=14.0, D=5, Spoiled=1.0),\n",
       " Row(A=5, B=8, C=12.0, D=8, Spoiled=1.0),\n",
       " Row(A=6, B=5, C=12.0, D=9, Spoiled=1.0),\n",
       " Row(A=3, B=3, C=12.0, D=1, Spoiled=1.0)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_food.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N19lLD2jMPdK"
   },
   "source": [
    "**Inference:** There is one more method from which we can look at the dataset i.e. **traditional head function** which will not only return the **name of all the columns** but also the values associated with it (**row wise**) and the tuple is in the format of **Row object**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VGokJkBT5EiV",
    "outputId": "fa31b9bd-b0db-4de4-c0bc-982fc13011f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+-------------------+\n",
      "|summary|                 A|                 B|                 C|                 D|            Spoiled|\n",
      "+-------+------------------+------------------+------------------+------------------+-------------------+\n",
      "|  count|               490|               490|               490|               490|                490|\n",
      "|   mean|  5.53469387755102| 5.504081632653061| 9.126530612244897| 5.579591836734694| 0.2857142857142857|\n",
      "| stddev|2.9515204234399057|2.8537966089662063|2.0555451971054275|2.8548369309982857|0.45221563164613465|\n",
      "|    min|                 1|                 1|               5.0|                 1|                0.0|\n",
      "|    max|                10|                10|              14.0|                10|                1.0|\n",
      "+-------+------------------+------------------+------------------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_food.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MPmO1UIoMuiS"
   },
   "source": [
    "**Inference:** What if we want to access the **statistical information** about the dataset? For that **PySpark** have the **describe() function** that is used against the chosen dataset. One can see the output where it returned the **count**, **mean**, **standard deviation**, **minimum** and **maximum** values of each features as well as for the independent column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wAXiJi-7OKGu"
   },
   "source": [
    "## Vector Assembler and Vectors in PySpark\n",
    "\n",
    "Whenever we are working with **MLIB library** we need to make sure that all the **features are stacked up together in one seperate column** keeping the target column in other one. So to attain this PySpark comes with **VectorAssembler** library that will sort things up for us without handling much manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "id": "uJNpGUBa5EiY"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHT-f361RcNA"
   },
   "source": [
    "In the above cell we imported **Vectors** and **VectorAssembler** modules from the **ml.linalg** and **ml.feature** library simultaneously. Moving forward we will see the implementation of the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e1PWEuXA5Eid",
    "outputId": "a5290521-2621-424b-f59a-fb44ef1f83a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+---+-------+-------------------+\n",
      "|  A|  B|   C|  D|Spoiled|           features|\n",
      "+---+---+----+---+-------+-------------------+\n",
      "|  4|  2|12.0|  3|    1.0| [4.0,2.0,12.0,3.0]|\n",
      "|  5|  6|12.0|  7|    1.0| [5.0,6.0,12.0,7.0]|\n",
      "|  6|  2|13.0|  6|    1.0| [6.0,2.0,13.0,6.0]|\n",
      "|  4|  2|12.0|  1|    1.0| [4.0,2.0,12.0,1.0]|\n",
      "|  4|  2|12.0|  3|    1.0| [4.0,2.0,12.0,3.0]|\n",
      "| 10|  3|13.0|  9|    1.0|[10.0,3.0,13.0,9.0]|\n",
      "|  8|  5|14.0|  5|    1.0| [8.0,5.0,14.0,5.0]|\n",
      "|  5|  8|12.0|  8|    1.0| [5.0,8.0,12.0,8.0]|\n",
      "|  6|  5|12.0|  9|    1.0| [6.0,5.0,12.0,9.0]|\n",
      "|  3|  3|12.0|  1|    1.0| [3.0,3.0,12.0,1.0]|\n",
      "|  9|  8|11.0|  3|    1.0| [9.0,8.0,11.0,3.0]|\n",
      "|  1| 10|12.0|  3|    1.0|[1.0,10.0,12.0,3.0]|\n",
      "|  1|  5|13.0| 10|    1.0|[1.0,5.0,13.0,10.0]|\n",
      "|  2| 10|12.0|  6|    1.0|[2.0,10.0,12.0,6.0]|\n",
      "|  1| 10|11.0|  4|    1.0|[1.0,10.0,11.0,4.0]|\n",
      "|  5|  3|12.0|  2|    1.0| [5.0,3.0,12.0,2.0]|\n",
      "|  4|  9|11.0|  8|    1.0| [4.0,9.0,11.0,8.0]|\n",
      "|  5|  1|11.0|  1|    1.0| [5.0,1.0,11.0,1.0]|\n",
      "|  4|  9|12.0| 10|    1.0|[4.0,9.0,12.0,10.0]|\n",
      "|  5|  8|10.0|  9|    1.0| [5.0,8.0,10.0,9.0]|\n",
      "+---+---+----+---+-------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assembler_data = VectorAssembler(inputCols=['A', 'B', 'C', 'D'],outputCol=\"features\")\n",
    "output = assembler_data.transform(data_food)\n",
    "output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RYyE1BlySGdT"
   },
   "source": [
    "**Code breakdown:** \n",
    "\n",
    "1. Firstly, before using the **VectorAssembler** we first need to create the object for the same i.e. **initializing it** and passing the **input columns** (features) and **output columns** (the one which will piled up)\n",
    "\n",
    "2. After initializing the object we are transforming it, note that in the parameter **whole dataset is passed**.\n",
    "\n",
    "3. At the last, to show the **transformed** data **show function** is used and in the output the last column turned out to be features column (all in one)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aqsdLSCsUhYQ"
   },
   "source": [
    "## Model building\n",
    "\n",
    "Here comes the model building phase where specifically we will use the **Tree method** to achieve the motto of this article. Note that this **model building phase will not be same as the traditional way** because we **don't need the train test split** instead we just want to grab which **feature has more importance**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "id": "Dp_thoyU5Eig"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier,DecisionTreeClassifier\n",
    "\n",
    "rfc = DecisionTreeClassifier(labelCol='Spoiled',featuresCol='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDEE9GB-V3HW"
   },
   "source": [
    "**Inference:** So before using the tree classifiers we need to import the **Random forest classifier** and **Decision Tree classifier** from classification module. \n",
    "\n",
    "Then, **initialising the Decision Tree object** and passing in the **label** column (**target**) and features columns (**feature**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KGmiM0mM5Eik",
    "outputId": "51e706d2-54c6-4df0-a719-d2678592d6d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+\n",
      "|           features|Spoiled|\n",
      "+-------------------+-------+\n",
      "| [4.0,2.0,12.0,3.0]|    1.0|\n",
      "| [5.0,6.0,12.0,7.0]|    1.0|\n",
      "| [6.0,2.0,13.0,6.0]|    1.0|\n",
      "| [4.0,2.0,12.0,1.0]|    1.0|\n",
      "| [4.0,2.0,12.0,3.0]|    1.0|\n",
      "|[10.0,3.0,13.0,9.0]|    1.0|\n",
      "| [8.0,5.0,14.0,5.0]|    1.0|\n",
      "| [5.0,8.0,12.0,8.0]|    1.0|\n",
      "| [6.0,5.0,12.0,9.0]|    1.0|\n",
      "| [3.0,3.0,12.0,1.0]|    1.0|\n",
      "| [9.0,8.0,11.0,3.0]|    1.0|\n",
      "|[1.0,10.0,12.0,3.0]|    1.0|\n",
      "|[1.0,5.0,13.0,10.0]|    1.0|\n",
      "|[2.0,10.0,12.0,6.0]|    1.0|\n",
      "|[1.0,10.0,11.0,4.0]|    1.0|\n",
      "| [5.0,3.0,12.0,2.0]|    1.0|\n",
      "| [4.0,9.0,11.0,8.0]|    1.0|\n",
      "| [5.0,1.0,11.0,1.0]|    1.0|\n",
      "|[4.0,9.0,12.0,10.0]|    1.0|\n",
      "| [5.0,8.0,10.0,9.0]|    1.0|\n",
      "+-------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_data = output.select('features','Spoiled')\n",
    "final_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KKslsU9XW0eH"
   },
   "source": [
    "The above process of accessing only the **features** and **target** column was performed so that we can get the **final data** that needs to be passed in the **training phase**. In the output also one can confirm the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "S5hAgJEK5Eik"
   },
   "outputs": [],
   "source": [
    "rfc_model = rfc.fit(final_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4dCtFRBXZMd"
   },
   "source": [
    "**Inference:** Finally there is the **training phase** and for that **fit** method is used also have a note that here we are passing the **final data** that we grabbed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q6dMTqxC5Eil",
    "outputId": "e33967d1-ce13-4a53-84b5-8f964bdd8dd0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(4, {1: 0.0019, 2: 0.9832, 3: 0.0149})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_model.featureImportances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3L_aVrQX7VT"
   },
   "source": [
    "**Inference:** Have a close look at the output where 3 indexes are there and 2nd index has the highest value (0.9832) i.e. \n",
    "\n",
    "**Chemical C is the most important feature that stimulates Chemical C is the main cause for the early spoilage of dog food.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GkR3leBHY6qV"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "We are in the endgame now 😧, The last part of the article will let you **summarize everything** we did so far to achieve the results for which we were hypothetically **hired by the dog food company to predict the chemical which is causing for the early spoiling of the dog food.**\n",
    "\n",
    "1. As usual for any Spark project here also we first **setup the Spark Session and read the dataset** - mandatory steps.\n",
    "\n",
    "2. Then we moved forward to the **feature analysis** phase and also **feature engineering** to make the dataset ready to fed machine learning algorithm.\n",
    "\n",
    "3. At the last we **build the model** (tree method) and after the training of the same we grabbed the **feature importance** and concluded that **Chemical C was mainly responsible for early spoiling.**"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "Predicting quality of dog food using MLIB",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
