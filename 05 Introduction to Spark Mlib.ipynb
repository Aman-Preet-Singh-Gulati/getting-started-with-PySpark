{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yjNdT1bPh1dX"
   },
   "source": [
    "## PySpark ML - Mlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12uT4B14P2fh"
   },
   "source": [
    "## Overview\n",
    "\n",
    "In this article we will be working with PySpark's Mlib library it is commonly known as the Machine learning libary of the PySpark where we can use any ML algorithm that was previously available in sklearn (scikit-learn) along with that we can perform all the operation which were required in the complete ML pipeline.\n",
    "\n",
    "Read my previous blogs on Pyspark before going on with this one.\n",
    "1. Getting started with PySpark using Python\n",
    "2. Data Preprocessing using PySpark - PySpark's DataFrame\n",
    "3. Data preprocessing using PySpark - Handling missing values\n",
    "4. Data Preprocessing using PySpark - Aggregate and GroupBy functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezr7OV7DP5Fi"
   },
   "source": [
    "## What we will cover in this article?\n",
    "\n",
    "1. Setting up the Spark Session and reading the dataset\n",
    "2. PySpark's Vector Assembler\n",
    "3. Transforming the dataset\n",
    "4. Train Test Split\n",
    "5. Model Building\n",
    "6. Coffecients and Intercepts of linear regression\n",
    "7. Predicting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BKGbkVu3iQ7k",
    "outputId": "34bb5c15-d010-4652-f2f5-088426ad86b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 281.4 MB 34 kB/s \n",
      "\u001b[?25hCollecting py4j==0.10.9.3\n",
      "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
      "\u001b[K     |████████████████████████████████| 198 kB 46.4 MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=3e25f4640886aeed47aa4092cff854e692dba6bb38b3747f99aed6c55a97bb09\n",
      "  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XsswP6lx4BUg"
   },
   "source": [
    "## Starting the spark session\n",
    "\n",
    "The very first step before playing with PySpark is to setup and start the Spark session and for that we will be first importing the **`SparkSession`** function from **`pyspark.sql`** package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "id": "-vAY6W3fhWLg",
    "outputId": "1f4ee758-202a-4c22-f742-033f3f929789"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://7af66238d792:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Machine learning example</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f45ef466b10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "df_ml = SparkSession.builder.appName('Machine learning example').getOrCreate()\n",
    "df_ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rK7klwB44dPX"
   },
   "source": [
    "**Inference:** After importing the SparkSession function we have used the **`builder function`** to build our session and gave the name to the session using **`appName function`** which is under builder function only at the last we simply created the session using **`getOrCreate()`** function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CXqvL0tgimhk"
   },
   "source": [
    "## Reading the dataset\n",
    "\n",
    "Before heading towards reading the data let's understand what our dataset actually is!\n",
    "\n",
    "So this dataset is basically bank note authentication dataset from kaggle and it holds the statistical details of both real notes and fake notes. IF you wanna know more about this dataset follow this link.\n",
    "`https://www.kaggle.com/code/dsabhis04/bank-note-detection-data-set/data`\n",
    "\n",
    "Feature columns:\n",
    "1. **Variance**\n",
    "2. **Skewness**\n",
    "3. **Curtosis**\n",
    "4. **Entropy**\n",
    "\n",
    "Target column:\n",
    "1. **Target**\n",
    "\n",
    "Now, after creating and setting up the SparkSession its time to **read the dataset** on which we will be applying **machine learning** operations and before that the **data preprocessing** techniques using **PySpark**.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JZSAdOfciOVt",
    "outputId": "c72c2c19-1db4-44f7-94fc-18d551c8372a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[variance: double, skewness: double, curtosis: double, entropy: double, Target: int]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset  = df_ml.read.csv('/content/bank_notes.csv', header=True, inferSchema=True)\n",
    "training_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DrXBO9Vp6qzm"
   },
   "source": [
    "**Inference:** Here with the help of **`read.csv()`** function we have read the CSV formatted dataset and provide **header as True** so that we can get the column name as header and **inferScehma as True** so that we can get real data type of each column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VpjDFxz38fFx"
   },
   "source": [
    "**Let's see our dataset now**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bS0JrLeeiz5X",
    "outputId": "740ca525-a88d-4555-f4d7-c7d0230be0e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+--------+------+\n",
      "|variance|skewness|curtosis| entropy|Target|\n",
      "+--------+--------+--------+--------+------+\n",
      "|  3.6216|  8.6661| -2.8073|-0.44699|     0|\n",
      "|  4.5459|  8.1674| -2.4586| -1.4621|     0|\n",
      "|   3.866| -2.6383|  1.9242| 0.10645|     0|\n",
      "|  3.4566|  9.5228| -4.0112| -3.5944|     0|\n",
      "| 0.32924| -4.4552|  4.5718| -0.9888|     0|\n",
      "|  4.3684|  9.6718| -3.9606| -3.1625|     0|\n",
      "|  3.5912|  3.0129| 0.72888| 0.56421|     0|\n",
      "|  2.0922|   -6.81|  8.4636|-0.60216|     0|\n",
      "|  3.2032|  5.7588|-0.75345|-0.61251|     0|\n",
      "|  1.5356|  9.1772| -2.2718|-0.73535|     0|\n",
      "|  1.2247|  8.7779| -2.2135|-0.80647|     0|\n",
      "|  3.9899| -2.7066|  2.3946| 0.86291|     0|\n",
      "|  1.8993|  7.6625| 0.15394| -3.1108|     0|\n",
      "| -1.5768|  10.843|  2.5462| -2.9362|     0|\n",
      "|   3.404|  8.7261| -2.9915|-0.57242|     0|\n",
      "|  4.6765| -3.3895|  3.4896|  1.4771|     0|\n",
      "|  2.6719|  3.0646| 0.37158| 0.58619|     0|\n",
      "| 0.80355|  2.8473|  4.3439|  0.6017|     0|\n",
      "|  1.4479| -4.8794|  8.3428| -2.1086|     0|\n",
      "|  5.2423| 11.0272|  -4.353| -4.1013|     0|\n",
      "+--------+--------+--------+--------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_dataset.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iWuVFtUs8ldT"
   },
   "source": [
    "**Inference:** Here we can see the **top 20 rows of dataset** with the help of **show() function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G0T5YFPd9D6z"
   },
   "source": [
    "Now we will look at the Scehma of our bank note detection dataset i.e. we will see what data type each column hold and do it have null values or not? So let's answer this question with **`printSchema()`** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i0SG4kGzi5LX",
    "outputId": "441319cc-9c08-4719-94b8-f6bc38ed5291"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- variance: double (nullable = true)\n",
      " |-- skewness: double (nullable = true)\n",
      " |-- curtosis: double (nullable = true)\n",
      " |-- entropy: double (nullable = true)\n",
      " |-- Target: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_dataset.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zWvsl9lt908w"
   },
   "source": [
    "**Inference:** After calling the printSchema method we can see that it returned the type of the data of each column where: \n",
    "\n",
    "* **Variance, Skewness, Curtosis and Entropy** column holds the double type value which is our dependent columns i.e. **features** and\n",
    "* **Target** column holds the integer type value which is our independent column i.e. **Target** column.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tPN_mbIL_R5B"
   },
   "source": [
    "Though by far we saw the complete schema of our dataset but this is not something which we wanna see all the time instead to see just **how many columns are there** so let's figure that out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CXvFglkKjIb3",
    "outputId": "e1a2eba7-8a9d-48c5-b2c1-80b7cb7d44fd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['variance', 'skewness', 'curtosis', 'entropy', 'Target']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gOnRAzjqAPhj"
   },
   "source": [
    "**Inference:** By using columns object we can see **how many columns are there** in the data and it will be returned in the **list** format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "scPiEVr2BnbX"
   },
   "source": [
    "## Vector Assembler\n",
    "\n",
    "Vector assembler is the package which helps us to bring all the dependent columns i.e. **features in one column** in short it **stacked the feature columns together** in the form of **`vector type`** so now instead of dealing with multiple columns we only need to care about that one column because it holds all the data which we need to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uSTNUTp6jNT8",
    "outputId": "23e1b24b-69c3-49f3-8e29-55e31adf6d34"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorAssembler_835df549891f"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## [\"variance\", \"skewness\",\"curtosis\", \"entropy\"] -------> new feature -------> independent feature\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "featassembler = VectorAssembler(inputCols=[\"variance\", \"skewness\",\"curtosis\", \"entropy\"], outputCol = \"Independent Features\" )\n",
    "featassembler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DB8SJY5fMNPW"
   },
   "source": [
    "Code breakdown:\n",
    "\n",
    "1. Firstly we imported the **`VectorAssembler`** from **`pyspark.ml.feature`** library.\n",
    "2. Then we have used the same VectorAssembler to stack our dependent features together with the help of following parameters.\n",
    "  * **inputCols:** This parameter will hold all the features in the form of list on which we are aiming to perform ML operations.\n",
    "  * **outputCol:** Here we will give the name to the column to which we are grouping all the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GOc6tdwMNqnX"
   },
   "source": [
    "## Transforming the dataset\n",
    "\n",
    "In this section we will transform our dataset i.e. we will add our Independent feature columns in the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zl8f0hGf-qqp",
    "outputId": "ebcdca3b-ffe5-40ba-e35b-3a9bc5915c8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+--------+------+--------------------+\n",
      "|variance|skewness|curtosis| entropy|Target|Independent Features|\n",
      "+--------+--------+--------+--------+------+--------------------+\n",
      "|  3.6216|  8.6661| -2.8073|-0.44699|     0|[3.6216,8.6661,-2...|\n",
      "|  4.5459|  8.1674| -2.4586| -1.4621|     0|[4.5459,8.1674,-2...|\n",
      "|   3.866| -2.6383|  1.9242| 0.10645|     0|[3.866,-2.6383,1....|\n",
      "|  3.4566|  9.5228| -4.0112| -3.5944|     0|[3.4566,9.5228,-4...|\n",
      "| 0.32924| -4.4552|  4.5718| -0.9888|     0|[0.32924,-4.4552,...|\n",
      "|  4.3684|  9.6718| -3.9606| -3.1625|     0|[4.3684,9.6718,-3...|\n",
      "|  3.5912|  3.0129| 0.72888| 0.56421|     0|[3.5912,3.0129,0....|\n",
      "|  2.0922|   -6.81|  8.4636|-0.60216|     0|[2.0922,-6.81,8.4...|\n",
      "|  3.2032|  5.7588|-0.75345|-0.61251|     0|[3.2032,5.7588,-0...|\n",
      "|  1.5356|  9.1772| -2.2718|-0.73535|     0|[1.5356,9.1772,-2...|\n",
      "|  1.2247|  8.7779| -2.2135|-0.80647|     0|[1.2247,8.7779,-2...|\n",
      "|  3.9899| -2.7066|  2.3946| 0.86291|     0|[3.9899,-2.7066,2...|\n",
      "|  1.8993|  7.6625| 0.15394| -3.1108|     0|[1.8993,7.6625,0....|\n",
      "| -1.5768|  10.843|  2.5462| -2.9362|     0|[-1.5768,10.843,2...|\n",
      "|   3.404|  8.7261| -2.9915|-0.57242|     0|[3.404,8.7261,-2....|\n",
      "|  4.6765| -3.3895|  3.4896|  1.4771|     0|[4.6765,-3.3895,3...|\n",
      "|  2.6719|  3.0646| 0.37158| 0.58619|     0|[2.6719,3.0646,0....|\n",
      "| 0.80355|  2.8473|  4.3439|  0.6017|     0|[0.80355,2.8473,4...|\n",
      "|  1.4479| -4.8794|  8.3428| -2.1086|     0|[1.4479,-4.8794,8...|\n",
      "|  5.2423| 11.0272|  -4.353| -4.1013|     0|[5.2423,11.0272,-...|\n",
      "+--------+--------+--------+--------+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = featassembler.transform(training_dataset)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JNr0GgV7Ogyf"
   },
   "source": [
    "**Inference:** So by using the **`transform()`** over assembler object we have sucessfully added the independent feature(S) column at the last column(from left) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3dl4-tebPbGd"
   },
   "source": [
    "Technically thinking so now our dataset should hold one more column i.e. The independent feature column, let's check that using our **columns** object on the variable that holds resultant dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gaONzCRIBp5t",
    "outputId": "cdf408d0-1245-45ef-9699-dc9760a1a19a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['variance',\n",
       " 'skewness',\n",
       " 'curtosis',\n",
       " 'entropy',\n",
       " 'Target',\n",
       " 'Independent Features']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "se4T8NxxP3s-"
   },
   "source": [
    "Yes it does! we have our last column in the dataset but do we need the other columns like *curtosis, variance, skewness and entropy ?*\n",
    "\n",
    "No right! because these columns we already have in our last column that we created using **Vector Assembler** method. So at the end we should only have 2 columns from the dataset and they are:\n",
    "\n",
    "1. **Independent features:** That holds all the features which we need to apply machine learning alorithm\n",
    "2. **Target:** That holds the result and from which we will be checking our prediction.\n",
    "\n",
    "\n",
    "Here we are doing it, we are simply making a final dataset that will consist of only 2 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CqCN_M57B6uz",
    "outputId": "1404f754-71ea-40a6-b44d-e2f72e143571"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|Independent features|Target|\n",
      "+--------------------+------+\n",
      "|[3.6216,8.6661,-2...|     0|\n",
      "|[4.5459,8.1674,-2...|     0|\n",
      "|[3.866,-2.6383,1....|     0|\n",
      "|[3.4566,9.5228,-4...|     0|\n",
      "|[0.32924,-4.4552,...|     0|\n",
      "|[4.3684,9.6718,-3...|     0|\n",
      "|[3.5912,3.0129,0....|     0|\n",
      "|[2.0922,-6.81,8.4...|     0|\n",
      "|[3.2032,5.7588,-0...|     0|\n",
      "|[1.5356,9.1772,-2...|     0|\n",
      "|[1.2247,8.7779,-2...|     0|\n",
      "|[3.9899,-2.7066,2...|     0|\n",
      "|[1.8993,7.6625,0....|     0|\n",
      "|[-1.5768,10.843,2...|     0|\n",
      "|[3.404,8.7261,-2....|     0|\n",
      "|[4.6765,-3.3895,3...|     0|\n",
      "|[2.6719,3.0646,0....|     0|\n",
      "|[0.80355,2.8473,4...|     0|\n",
      "|[1.4479,-4.8794,8...|     0|\n",
      "|[5.2423,11.0272,-...|     0|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_data = result.select(\"Independent features\", \"Target\")\n",
    "final_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZRnCIgpnRvcN"
   },
   "source": [
    "**Inference:** Now here with the help of **`select()`** we filtered out the grouped feature column as well as the resultant column and now are dataset only have 2 column and these are the only one which we care for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "afTLTdShXQa-"
   },
   "source": [
    "## Train Test Split\n",
    "\n",
    "Now as we know that **`Train Test split`** is one of the known step in machine learning pipeline where we divide our training dataset and testing dataset to **remove the problem of `overfitting of the model`** as if we will train the model on the whole dataset then it will surely lead to problem of overfitting of model hence we should always divide the data into training and testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vEjik1vck3bI"
   },
   "source": [
    "In PySpark we will be using the **`randomSplit()`** function to divide the data into training and testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4e_1JAcfXSEp"
   },
   "outputs": [],
   "source": [
    "train_data, test_data = final_data.randomSplit([0.75, 0.25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QpSJK7oClKYN"
   },
   "source": [
    "**Inference:** Now as we can see that we are breaking up the data into **75% of training** and **25% of testing** data using the **randomSplit()** function and it is getting stored in **train_data** and **test_data** variable simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36TAszALmC-u"
   },
   "source": [
    "## Model building\n",
    "\n",
    "Now as if we have **splitted our dataset** and we already have our training set so it's time to **`build our model`** based on the training dataset and then test the same model corresponds to testing data. As we know that it is the **regression problem** so we will be using the **`Linear Regression algorithm`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fo-OmN5GCULg"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "model = LinearRegression(featuresCol = 'Independent features', labelCol='Target')\n",
    "model = model.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "142vp26snT2C"
   },
   "source": [
    "**Code breakdown:**\n",
    "\n",
    "1. Firstly we have imported the **`LinearRegression`** algorithm from the **`pyspark.ml.regression`** package.\n",
    "2. Then we will define our **independent features** and **target** column after specifying the **`featureCol`** and **`labelCol`** simultaneously.\n",
    "3. After defining the feature columns and target we will **`fit`** our training data with the model that we have created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9z0aFYpojRI"
   },
   "source": [
    "## Coffecients and Intercepts\n",
    "\n",
    "The one who know about the **mathematical intiution of linear regression** they can easily pick up what this coefficient and intercepts demonstrate. For the one who are not aware of the same for them we will discuss it in nuthshell.\n",
    "\n",
    "Equation of linear regression: **`y = c + b*x`**\n",
    "Where, \n",
    "* \"y\" is the dependent variable i.e. target variable.\n",
    "* \"x\"is the independent variable i.e. features.\n",
    "* \"b\" is the **slope** of the line and also known as **regression coefficient**\n",
    "* \"c\" is the **intercept** which is also known as **constant**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "opofPUKC_vm1"
   },
   "source": [
    "**Coffecients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sVLY-SGKDZ9d",
    "outputId": "7f672bca-cc48-4bf8-e8d5-72a612965b1b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([-0.142, -0.0786, -0.1014, 0.0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dddzzLIE_W2w"
   },
   "source": [
    "**Inference:** In the output we can see that it has given us the **array of list of coffecients** in the form if **Dense Vector** i.e. all the regression coefficents in the Vector format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "the28ydkGwp2"
   },
   "source": [
    "**Intercepts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A_ZYDsVNDeTs",
    "outputId": "714bd75c-780e-4441-d7b8-e062c171cb5a",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7994252652531315"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2GYEpMugG2PL"
   },
   "source": [
    "**Inference:** In the output we can see the intercept that our model have and it represents the **mean of the target variable** when all the feature variables collectively have **zero** value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GqnfNgiSopWL"
   },
   "source": [
    "## Predicting the results\n",
    "\n",
    "So here we have come to the section where we will see how our model performed after all the training it went through and we call it **Prediction section**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OyPRMhMdEh6F",
    "outputId": "7c99ead4-1b7c-473d-8785-48c6f4a53957"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.ml.regression.LinearRegressionSummary at 0x7f45ec9551d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_result = model.evaluate(test_data)\n",
    "prediction_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e4f6ibV7NMv0"
   },
   "source": [
    "**Inference:** Here for predicting the results we are using the testing data and along with that involving the **evaluate()** method to predict the results on that unseen data then in the output we can see that it returned the object of **ml.regression.LinearRegressionSummary**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "csOkfX8_Eqvd",
    "outputId": "6a05a8e9-dacd-47aa-f5ef-34486e1f83f2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/pyspark/sql/context.py:127: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+------------------+\n",
      "|Independent features|Target|        prediction|\n",
      "+--------------------+------+------------------+\n",
      "|[-6.651,6.7934,0....|     1|1.1404038856208492|\n",
      "|[-6.5235,9.6014,-...|     1|0.9969175247975464|\n",
      "|[-6.3364,9.2848,0...|     1|0.9680322282277627|\n",
      "|[-6.1632,8.7096,-...|     1|1.0120857344568213|\n",
      "|[-5.9034,6.5679,0...|     1|1.0529166027113548|\n",
      "|[-5.525,6.3258,0....|     1|0.9957800288837861|\n",
      "|[-5.3857,9.1214,-...|     1| 0.889829131214498|\n",
      "|[-5.2406,6.6258,-...|     1|1.0430280860585202|\n",
      "|[-5.2049,7.259,0....|     1|  0.96080342143579|\n",
      "|[-5.119,6.6486,-0...|     1|  1.00885077845283|\n",
      "|[-4.8392,6.6755,-...|     1|0.9865451974451206|\n",
      "|[-4.7331,-6.1789,...|     1|0.8024320566742837|\n",
      "|[-4.577,3.4515,0....|     1|1.1105526948591118|\n",
      "|[-4.5046,-5.8126,...|     1| 0.792035762049575|\n",
      "|[-4.4779,7.3708,-...|     1|0.8876080853643694|\n",
      "|[-4.2932,3.3419,0...|     1|1.0681676832868743|\n",
      "|[-4.2333,4.9166,-...|     1|1.0640529769769747|\n",
      "|[-4.211,-12.4736,...|     1|0.8597345746821361|\n",
      "|[-4.1958,-8.1819,...|     1|0.8083855451829784|\n",
      "|[-4.1244,3.7909,-...|     1|1.1534289271555092|\n",
      "+--------------------+------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction_result.predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Exmzc9-O9qX"
   },
   "source": [
    "**Inference:** Now with the help of show method we can easily compare that **how close is the predicted value than the actual value** and it returned the DataFrame where we can see the predicted as well as actual values side by side based on the DataFrame we build previously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key takeaways from this article\n",
    "\n",
    "1. First we have completed our mandatory steps of starting the spark session and reading the bank note dataset.\n",
    "2. Then we used **Vector Assembler** to stacked all our features column.\n",
    "3. Then we have **transformed the dataset** so that it can lead to clear and understandable results.\n",
    "4. Later we used the **randomsplit() method** to split our dataset into training and testing data\n",
    "5. Then we build our **linear regression model** using **fit** method and found the cooeficients and intercepts of the model\n",
    "6. Finally we draw the **prediction** that our model has predicted based on the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tkqsZE1qEz3a",
    "outputId": "ffd519ce-b85d-43f2-b80d-69892bfff747"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.13659314366111264, 0.03228711994476193)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_result.meanAbsoluteError, prediction_result.meanSquaredError"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Introduction to Spark Mlib",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
