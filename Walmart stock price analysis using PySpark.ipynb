{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYN3FrWgQsw0"
   },
   "source": [
    "# Walmart's stock price analysis using PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgrCK6FVQsw6"
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "up9GXv3WQsw_"
   },
   "source": [
    "#### Use the walmart_stock.csv file to Answer and complete the  tasks below!\n",
    "https://www.kaggle.com/datasets/amandam1/walmart-stock-20122016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FAqFPjjjQsxD"
   },
   "source": [
    "## Mandatory steps to follow:\n",
    "\n",
    "1. **Starting the spark session:** In this segment we will setup the spark session so that we can use it with the python's distribution of Spark i.e. **PySpark**.\n",
    "\n",
    "\n",
    "2. **Reading the dataset:** In this section we will be reading the walmart stock price data which I have first downloaded from **Kaggle** and then extracted the stock data from **2012-2017** and we will be analyzing the data for these years only in this particular blog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4r00SRngQsxF"
   },
   "source": [
    "**Let's get started and analyze the walmart stocks using PySpark** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XhdxCWP3QsxH"
   },
   "source": [
    "## Setting up a simple Spark Session\n",
    "\n",
    "This is one of the mandatory things to do before getting started with PySpark i.e. to **setup an environment for the Spark** to use the python's PySpark library and use all of it's resources in that session.\n",
    "\n",
    "**Note:** Before importing stuffs don't forget to install PySpark library, Command to install the library is **!pip install pyspark**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nTPtixXRQwiI",
    "outputId": "780efe33-2961-445e-b9fa-28540a55a90f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 281.4 MB 37 kB/s \n",
      "\u001b[?25hCollecting py4j==0.10.9.3\n",
      "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
      "\u001b[K     |████████████████████████████████| 198 kB 33.6 MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=a3412aa65aab592354c67ba92994d77a34ce7f642617a3884b41f66ddd6bd632\n",
      "  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "id": "qngmqXjPQsxI",
    "outputId": "d783a411-dbbe-4bf6-f78f-85b5f7346fe4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://84ce00cf80ec:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>walmart</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fe57a5d11d0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark_walmart = SparkSession.builder.appName(\"Walmart_Stock_Price\").getOrCreate()\n",
    "spark_walmart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ew2KwiodTcnJ"
   },
   "source": [
    "**Code breakdown**\n",
    "\n",
    "Here we will breakdown the above code and understand each element required for creating and starting the Spark environment.\n",
    "\n",
    "1. Firstly we have imported **SparkSession** object from the pyspark.sql library and this object will hold all the curated functions that are required.\n",
    "\n",
    "2. Then using the SparkSession object we are calling the builder function which will **build** the session and give the specific name to the session using **AppName** at the end we will create the environment using **getOrCreate()** function.\n",
    "\n",
    "3. At the last we will be looking at **Spark UI** which curates all the neccessary information about the Spark's environment like it's **version**, **Branch (Master)** and the **AppName**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CMgKt4LvQsxK"
   },
   "source": [
    "## Reading the Walmart stock price data\n",
    "\n",
    "In this section we will be reading the Walmart's stock price data using PySpark and store it in the variable to use it for further analysis. As we know that in **pandas** we used **csv()** function similarly we use **read.csv()** function in **PySpark**. Let's further discuss about the same.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qcl8oXQgQsxK",
    "outputId": "885c37a3-0ca3-4b97-bfbb-6910455b229e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+--------+------------------+\n",
      "|      Date|              Open|              High|               Low|             Close|  Volume|         Adj Close|\n",
      "+----------+------------------+------------------+------------------+------------------+--------+------------------+\n",
      "|2012-01-03|         59.970001|         61.060001|         59.869999|         60.330002|12668800|52.619234999999996|\n",
      "|2012-01-04|60.209998999999996|         60.349998|         59.470001|59.709998999999996| 9593300|         52.078475|\n",
      "|2012-01-05|         59.349998|         59.619999|         58.369999|         59.419998|12768200|         51.825539|\n",
      "|2012-01-06|         59.419998|         59.450001|         58.869999|              59.0| 8069400|          51.45922|\n",
      "|2012-01-09|         59.029999|         59.549999|         58.919998|             59.18| 6679300|51.616215000000004|\n",
      "|2012-01-10|             59.43|59.709998999999996|             58.98|59.040001000000004| 6907300|         51.494109|\n",
      "|2012-01-11|         59.060001|         59.529999|59.040001000000004|         59.400002| 6365600|         51.808098|\n",
      "|2012-01-12|59.790001000000004|              60.0|         59.400002|              59.5| 7236400|51.895315999999994|\n",
      "|2012-01-13|             59.18|59.610001000000004|59.009997999999996|59.540001000000004| 7729300|51.930203999999996|\n",
      "|2012-01-17|         59.869999|60.110001000000004|             59.52|         59.849998| 8500000|         52.200581|\n",
      "|2012-01-18|59.790001000000004|         60.029999|         59.650002|60.009997999999996| 5911400|         52.340131|\n",
      "|2012-01-19|             59.93|             60.73|             59.75|60.610001000000004| 9234600|         52.863447|\n",
      "|2012-01-20|             60.75|             61.25|         60.669998|61.009997999999996|10378800|53.212320999999996|\n",
      "|2012-01-23|         60.810001|             60.98|60.509997999999996|             60.91| 7134100|         53.125104|\n",
      "|2012-01-24|             60.75|              62.0|             60.75|61.389998999999996| 7362800| 53.54375400000001|\n",
      "|2012-01-25|             61.18|61.610001000000004|61.040001000000004|         61.470001| 5915800| 53.61353100000001|\n",
      "|2012-01-26|         61.799999|             61.84|             60.77|         60.970001| 7436200|         53.177436|\n",
      "|2012-01-27|60.860001000000004|         61.119999|60.540001000000004|60.709998999999996| 6287300|         52.950665|\n",
      "|2012-01-30|         60.470001|             61.32|         60.349998|         61.299999| 7636900|53.465256999999994|\n",
      "|2012-01-31|         61.529999|             61.57|         60.580002|61.360001000000004| 9761500|53.517590000000006|\n",
      "+----------+------------------+------------------+------------------+------------------+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark_walmart.read.csv('walmart_stock.csv',header=True,inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BVeMIhiqqYDl"
   },
   "source": [
    "**Inference:** So here we can see that **show()** function have returned the **top 20 rows** of the dataset. Note that we have keep the **header type as True** so that spark will treat first row as header and **inferSchema** is also set to **True** so that it return the values with real data type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qtAJ6umCwrjc"
   },
   "source": [
    "## Understanding about dataset\n",
    "\n",
    "In this section we will be using the relevant functions of PySpark to **analyze the dataset** from analyzing here I mean that we will see **what our dataset looks like**, **what is the structure of the same** and **what formatting needs to be done** as a cleaning part.\n",
    "\n",
    "Here are the following things that we will be covering here:\n",
    "\n",
    "1. Looking at the **name of the columns** that walmart data holds.\n",
    "2. Understanding about the **Schema of the dataset**.\n",
    "3. Looping through the **specific number of rows** in the data.\n",
    "4. Understanding about the **statistical information** that our data signifies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56_5jnw5QsxP"
   },
   "source": [
    "**Let's have a look on the columns names.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KuktRzmgQsxQ",
    "outputId": "9dafe3ae-37c8-4f36-a508-f76cfa574c02"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7voP-1exy7KR"
   },
   "source": [
    "**Inference:** From the above output we can see that it returned the list of values where **'values'** denotes the **name of the columns** which are present and for that we used the **columns** object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-duLg7-QsxS"
   },
   "source": [
    "**Now we will see how the schema of dataset looks like!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ke7fKvbaQsxT",
    "outputId": "f87a252c-b0f0-455f-c102-73adfc3c5cf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yF1WvjlV3P03"
   },
   "source": [
    "**Inference:** Here by using the **printScehma()** function we are actually looking at the data type of each column of the dataset and here we can note one thing as well which is **\"nullable = True\"** this signifies that the column can hold the **NULL** values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SIYy9nc7QsxW"
   },
   "source": [
    "**Looping through the data and fetching top 5 rows.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "btOyQr0UQsxY",
    "outputId": "6c0fa1bb-8996-4ff6-dbdb-e36035329dce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(Date='2012-01-03', Open=59.970001, High=61.060001, Low=59.869999, Close=60.330002, Volume=12668800, Adj Close=52.619234999999996)\n",
      "\n",
      "\n",
      "Row(Date='2012-01-04', Open=60.209998999999996, High=60.349998, Low=59.470001, Close=59.709998999999996, Volume=9593300, Adj Close=52.078475)\n",
      "\n",
      "\n",
      "Row(Date='2012-01-05', Open=59.349998, High=59.619999, Low=58.369999, Close=59.419998, Volume=12768200, Adj Close=51.825539)\n",
      "\n",
      "\n",
      "Row(Date='2012-01-06', Open=59.419998, High=59.450001, Low=58.869999, Close=59.0, Volume=8069400, Adj Close=51.45922)\n",
      "\n",
      "\n",
      "Row(Date='2012-01-09', Open=59.029999, High=59.549999, Low=58.919998, Close=59.18, Volume=6679300, Adj Close=51.616215000000004)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for row in df.head(5):\n",
    "    print(row)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jq3GQDpyLy32"
   },
   "source": [
    "**Inference:** In the above output we can see that it returned the \"**ROW**\" object and in this row object it hold the real top 5 data ( because we iterated through **top 5 data using the head() function** ) and this is one of the ways where we can extract the one or multiple tuples of record seperately.\n",
    "\n",
    "**Note:** This is completely optional thing to involve in this analysis as we will be using this concept if we want to hold each row in a different variable to play with each data point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4zWzBBb0Qsxa"
   },
   "source": [
    "**Using describe() function to see the statistical information of dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RF3ItfLhQsxb",
    "outputId": "ac970e49-41ff-4ae5-b23c-e969b6c2522c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|summary|      Date|              Open|             High|              Low|            Close|           Volume|        Adj Close|\n",
      "+-------+----------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|  count|      1258|              1258|             1258|             1258|             1258|             1258|             1258|\n",
      "|   mean|      null| 72.35785375357709|72.83938807631165| 71.9186009594594|72.38844998012726|8222093.481717011|67.23883848728146|\n",
      "| stddev|      null|  6.76809024470826|6.768186808159218|6.744075756255496|6.756859163732991|  4519780.8431556|6.722609449996857|\n",
      "|    min|2012-01-03|56.389998999999996|        57.060001|        56.299999|        56.419998|          2094900|        50.363689|\n",
      "|    max|2016-12-30|         90.800003|        90.970001|            89.25|        90.470001|         80898100|84.91421600000001|\n",
      "+-------+----------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fyyr1Y75Y9ro"
   },
   "source": [
    "## Formatting the dataset\n",
    "\n",
    "In this section we will **format our dataset** to make it more clearer and precise so that we will have the clean data which will be easier to analyze than to the current state of the data as the inferences which we will draw now will not **give clearer picture of the results.**\n",
    "\n",
    "**Hence here we will first format some of the columns to their nearest integer value and along with that adding one column too for avoid further calculations**\n",
    "\n",
    "So let's format our dataset and perform each step one by one:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1EgjyqF0gCD3"
   },
   "source": [
    "Before moving further and changing the formatting of the data points let us first see on what columns we have to apply those changes and for that we will use the **combination of printSchema and describe function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y0F47NogQsxg",
    "outputId": "3095f662-674e-4a5e-d7b8-f3bfe6f3ea19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- summary: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Open: string (nullable = true)\n",
      " |-- High: string (nullable = true)\n",
      " |-- Low: string (nullable = true)\n",
      " |-- Close: string (nullable = true)\n",
      " |-- Volume: string (nullable = true)\n",
      " |-- Adj Close: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tXvsd0jFhviW"
   },
   "source": [
    "**Inference:** In the above output we can see the data type of each method that was returned by describe function and from the output we can see that all the columns hold the string values which is not good if we want to format them and further analysing them \n",
    "\n",
    "Hence now it's our task to first change the data type of these columns (specifically for **mean** and **stddev**) and later format it for better understanding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3nA0fodujnkt"
   },
   "source": [
    "As discussed earlier, now as we know what to do its time to know how to do and to answer this question we will be using the **format_number** function from the \"**functions**\" package and this will help us to,\n",
    "\n",
    "* Firstly change the data type of the column to the relevant type\n",
    "* Secondly it will also converts the values to the nearest integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "y0LK_-25Qsxi"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import format_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gSXJyhU8Qsxj",
    "outputId": "a1af38c0-2b94-4aeb-821c-9104c51bea39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+--------+--------+--------+--------+\n",
      "|summary|    Open|    High|     Low|   Close|  Volume|\n",
      "+-------+--------+--------+--------+--------+--------+\n",
      "|  count|1,258.00|1,258.00|1,258.00|1,258.00|    1258|\n",
      "|   mean|   72.36|   72.84|   71.92|   72.39| 8222093|\n",
      "| stddev|    6.77|    6.77|    6.74|    6.76| 4519780|\n",
      "|    min|   56.39|   57.06|   56.30|   56.42| 2094900|\n",
      "|    max|   90.80|   90.97|   89.25|   90.47|80898100|\n",
      "+-------+--------+--------+--------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = df.describe()\n",
    "result.select(result['summary'],\n",
    "              format_number(result['Open'].cast('float'),2).alias('Open'),\n",
    "              format_number(result['High'].cast('float'),2).alias('High'),\n",
    "              format_number(result['Low'].cast('float'),2).alias('Low'),\n",
    "              format_number(result['Close'].cast('float'),2).alias('Close'),\n",
    "              result['Volume'].cast('int').alias('Volume')\n",
    "             ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code breakdown:**\n",
    "\n",
    "1. In this first step of formatting the data we have imported the **format_number** class from the **functions** package of pyspark.\n",
    "2. Then, as we already knew that we have to change the data of **mean and stddev** so based on that **describe** method has been used and stored in the seperate variable.\n",
    "3. Then comes the main thing where firstly select statement is used (equally to SQL's select statement) and here,\n",
    "    * **cast** function is used to change the data type (type casting) and setting the decimal values after point.\n",
    "    * **alias** function is used to change the column name (not permanently just for first time view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create an altogether new DataFrame which will have one column named as **HV ratio** which will stimulates the ratio of **High Price** and **Total Volume** of stock which were traded for a day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uigkY3LMQsxm",
    "outputId": "3161f892-4b67-492a-dab3-087b7a0bf503"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            HV Ratio|\n",
      "+--------------------+\n",
      "|4.819714653321546E-6|\n",
      "|6.290848613094555E-6|\n",
      "|4.669412994783916E-6|\n",
      "|7.367338463826307E-6|\n",
      "|8.915604778943901E-6|\n",
      "|8.644477436914568E-6|\n",
      "|9.351828421515645E-6|\n",
      "| 8.29141562102703E-6|\n",
      "|7.712212102001476E-6|\n",
      "|7.071764823529412E-6|\n",
      "|1.015495466386981E-5|\n",
      "|6.576354146362592...|\n",
      "| 5.90145296180676E-6|\n",
      "|8.547679455011844E-6|\n",
      "|8.420709512685392E-6|\n",
      "|1.041448341728929...|\n",
      "|8.316075414862431E-6|\n",
      "|9.721183814992126E-6|\n",
      "|8.029436027707578E-6|\n",
      "|6.307432259386365E-6|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.withColumn(\"HV Ratio\",df[\"High\"]/df[\"Volume\"])#.show()\n",
    "# df2.show()\n",
    "df2.select('HV Ratio').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inference**\n",
    "\n",
    "Here in the output one can see that this new DataFrame holds the ratio of discussed fields, we introduced the column in this DataFrame with the help of **\"withColumn()\"** function and then simply performed the **ratio operation** and showed it with combination of **select and show** statement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enough of preparing the dataset! We are suppose to do the analysis of **Walmart Stock Price** and now it's time for it!\n",
    "\n",
    "## Questions to analyze using PySpark\n",
    "\n",
    "In this section we will give answer to some questions proposed to us by a firm to give us a feel of the data science consulting projects here we will draw the insights using the PySpark's data preprocessing technique\n",
    "\n",
    "1. On what day stock price was the highest?\n",
    "2. What is the average of Closing price?\n",
    "3. What is the maximum and minimum Volume of stock traded\n",
    "4. For how many days the closing value was less than 60 dollars?\n",
    "5. What could be the maximum high value for each year?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qoCxVtc_Qsxn"
   },
   "source": [
    "**On what day stock price was the highest?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "yqeCWBKpQsxo",
    "outputId": "793b4ca4-59c2-4606-aa58-d92829781c99"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'2015-01-13'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.orderBy(df[\"High\"].desc()).head(1)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inference:** In the above output we can see that we got the date on which stock price was highest by usinh the orderBy function and selecting it as **descending** order then we have simply used **head** function with a little bit of **indexing** to fetch the **date object** from the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p7vDmvSzQsxp"
   },
   "source": [
    "**What is the average of Closing price?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fdyY3clKQsxp",
    "outputId": "f387f4fc-06aa-41b4-ee7f-58ad1275fb80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|       avg(Close)|\n",
      "+-----------------+\n",
      "|72.38844998012726|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "df.select(mean(\"Close\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inference:** From the above output we can say that the average closing price is **72.38844998012726** and we have fethed this value by using the **select** statement and then **mean** function to show the **mean closing stock price**\n",
    "\n",
    "**Note:** Here we could have also used the describe method but I wanted you to know the operations that are possible with PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hwfvr6GCQsxq"
   },
   "source": [
    "**What is the maximum and minimum Volume of stock traded**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "xKL2Z4SYQsxq"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max,min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v7lMCyzFQsxr",
    "outputId": "102d1be8-ff3c-46ad-eb37-8f40948cc32a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|max(Volume)|min(Volume)|\n",
      "+-----------+-----------+\n",
      "|   80898100|    2094900|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(max(\"Volume\"),min(\"Volume\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inference:** To get the maximum and minumum volume of the stocks we have first imported the \"max\" and \"min\" function using **\"functions\"** package and then it's like walking on the cake, we have simply used these function to get the desired results using select statement. Here also we can use the **describe** function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ehJn-411Qsxr"
   },
   "source": [
    "**For how many days the closing value was less than 60 dollars?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KhTeLAGgQsxs",
    "outputId": "a216a528-6693-4ac7-faad-13037904060b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(\"Close < 60\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L7jHbi9gQsxu",
    "outputId": "07107feb-aae1-4234-e39c-429a44f53781"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(df['Close'] < 60).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eKyeAskgQsxv",
    "outputId": "44f29281-8291-4739-bdd7-e3b0b12660ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|count(Close)|\n",
      "+------------+\n",
      "|          81|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "result = df.filter(df['Close'] < 60)\n",
    "result.select(count('Close')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inference:** Now to get the total number of days to get the closing value which is less than 60 dollars we have to follow **two steps** to get the desired results:\n",
    "\n",
    "1. Using the **filter operations** we are filtering the **closing value less than 60** so we could get the required data only and then with the count function counting the total number of filtered records.\n",
    "2. Then we are simply using the **select** statement and putting everything in this select statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0qLI5_OaQsxz"
   },
   "source": [
    "**What could be the maximum high value for each year?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "00G_iNiSQsx0"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year\n",
    "yeardf = df.withColumn(\"Year\",year(df[\"Date\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AGvaz55GQsx2"
   },
   "outputs": [],
   "source": [
    "max_df = yeardf.groupBy('Year').max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GGk_etnuQsx2",
    "outputId": "ebc2f743-cacc-4693-fc59-a8bd0dcd8bac",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "|Year|max(High)|\n",
      "+----+---------+\n",
      "|2015|90.970001|\n",
      "|2013|81.370003|\n",
      "|2014|88.089996|\n",
      "|2012|77.599998|\n",
      "|2016|75.190002|\n",
      "+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_df.select('Year','max(High)').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inference:** To get the maximum value of stock price for each year (as it will be more informative in terms of collective information to analyze) we need to follow **three steps** and they are as follows:\n",
    "1. Firstly we have imported the **\"year\"** function from functions package then we created a new dataframe and inserted a new column named as **Year** along with that extracted it from the date column.\n",
    "\n",
    "2. Then by using the GroupBy function we simply grouped the Yearly column with max aggregate function.\n",
    "\n",
    "3. At the last we have shown the maximum value per year using the show function, note that we use the max function for the output parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this section we will discuss about whatever we have learned so far in this blog from discussing about the setup up the Spark session ....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2WWl9j-JQsxw"
   },
   "source": [
    "#### What percentage of the time was the High greater than 80 dollars ?\n",
    "#### In other words, (Number of Days High>80)/(Total Days in the dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UHD_Pl0EQsxw",
    "outputId": "11c3280a-f9b6-4120-e1d0-22bd1dab075b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.141494435612083"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 9.14 percent of the time it was over 80\n",
    "# Many ways to do this\n",
    "(df.filter(df[\"High\"]>80).count()*1.0/df.count())*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmk_5ylvQsxx"
   },
   "source": [
    "#### What is the Pearson correlation between High and Volume?\n",
    "#### [Hint](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameStatFunctions.corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qcoEwc5AQsxy",
    "outputId": "f850d4a0-fff9-49c3-fa59-e50f75c69a36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "| corr(High, Volume)|\n",
      "+-------------------+\n",
      "|-0.3384326061737161|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import corr\n",
    "df.select(corr(\"High\",\"Volume\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yu6SHtkzQsx3"
   },
   "source": [
    "#### What is the average Close for each Calendar Month?\n",
    "#### In other words, across all the years, what is the average Close price for Jan,Feb, Mar, etc... Your result will have a value for each of these months. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9YnRttg8Qsx4",
    "outputId": "b6c07241-ba5c-4623-8710-7cbdff81c01a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|Month|       avg(Close)|\n",
      "+-----+-----------------+\n",
      "|    1|71.44801958415842|\n",
      "|    2|  71.306804443299|\n",
      "|    3|71.77794377570092|\n",
      "|    4|72.97361900952382|\n",
      "|    5|72.30971688679247|\n",
      "|    6| 72.4953774245283|\n",
      "|    7|74.43971943925233|\n",
      "|    8|73.02981855454546|\n",
      "|    9|72.18411785294116|\n",
      "|   10|71.57854545454543|\n",
      "|   11| 72.1110893069307|\n",
      "|   12|72.84792478301885|\n",
      "+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import month\n",
    "monthdf = df.withColumn(\"Month\",month(\"Date\"))\n",
    "monthavgs = monthdf.select(\"Month\",\"Close\").groupBy(\"Month\").mean()\n",
    "monthavgs.select(\"Month\",\"avg(Close)\").orderBy('Month').show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "Spark DataFrames Project Exercise - SOLUTIONS.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
