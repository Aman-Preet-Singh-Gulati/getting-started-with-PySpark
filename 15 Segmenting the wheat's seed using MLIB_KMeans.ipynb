{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qqea0GX8yD3H"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Most of the machine learning task usually revolves around either the **supervised** learning approach i.e. the one which gives the label (**the column to be predicted**) or the **unsupervised** learning that don't have any label column in the dataset we have to make relevant groups out of it under certain criteria (**choosing the best K value and centroid for each data point**).\n",
    "\n",
    "Similarly in this article we are going to involve the concept of unsupervised method more specifically KMeans to **divide the seeds of wheat into clusters** i.e. **we have the features of all the wheat seed data though we don't know to which category they belong to hence clustering technique can help us to seggregate that**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wnb-A_6Pd2pt"
   },
   "source": [
    "## About the dataset\n",
    "\n",
    "Before going forward with any problem statement it is very much essential that **we should get the background and source of the dataset** so that the authenticity should sustain. This dataset includes thre different categories of wheat they are, **Canadian, Kama and Rosa**. For experiment purpose **70 features** were selected from each of the category. \n",
    "\n",
    "\n",
    "If we talk about the image resolution because that is one key area which is highly responsible for accuracy of the experiment then there were **high quality of visualization** using the **soft X-ray technique** and those images were captured by **X-ray KODAK plates**.\n",
    "\n",
    "**If one needs to know more about this dataset then please visit this [link](https://archive.ics.uci.edu/ml/datasets/seeds)**.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Y9FgGbVyD3T"
   },
   "source": [
    "This dataset is one of the great example as it can be used as **clustering** task as well as for the **classification** i.e. we can either group different wheat seed or we can classify which type of wheat seed is this?  \n",
    "\n",
    "\n",
    "**Features Information:**\n",
    "\n",
    "To maintain the authentic dataset it is being evaluated from **7 different geometric values**. They are as follows\n",
    "\n",
    "1. **Area:** Denoted by A, have the total area of wheat kernels. \n",
    "2. **Perimeter:** Denoted by P, consisting the perimeter. \n",
    "3. **Compactness:** Denoted by C and following calculation is done to calculate this aspect = 4*pi*A/P^2. \n",
    "4. **Length:** Length of the kernel.\n",
    "5. **Width:** Width of the kernel.\n",
    "6. **Asymmetry coefficient:** The coefficient value of symmetrical kernels\n",
    "7. **Length Kernel:** Length of the kernel groove. \n",
    "\n",
    "\n",
    "Now our main goal is to cluster the wheat seeds into 3 groups using K-means clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t-t13YMTf8TI",
    "outputId": "f741d27c-113b-4a7d-eec8-ee1780b3c8fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting pyspark\n",
      "  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 281.3 MB 46 kB/s \n",
      "\u001b[?25hCollecting py4j==0.10.9.5\n",
      "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
      "\u001b[K     |████████████████████████████████| 199 kB 50.8 MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764026 sha256=2dccc2528000832f72221457130cea869f1c9ebd3d5f5ada885b457e3ac9cb5b\n",
      "  Stored in directory: /root/.cache/pip/wheels/7a/8e/1b/f73a52650d2e5f337708d9f6a1750d451a7349a867f928b885\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.5 pyspark-3.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "collapsed": true,
    "id": "bCher1F5yD3h",
    "outputId": "3124a847-1dee-4b4a-f808-9d8c91915361"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://e895b468312f:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>wheat_seed</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f067c28d050>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('wheat_seed').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZMiBctFk2aEB"
   },
   "source": [
    "**Inference:** If any one is following my PySpark series then by far they are aware of this mandatory steps by which we setup the Spark environment by its PySpark distribution.\n",
    "\n",
    "Here we gave the name to the session as **wheat_seed** and created the same using **builder** and **getOrCreate()** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Zzl9U8bDyD3o"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QbkUKvsHCza2"
   },
   "source": [
    "**Inference:** Importing the libraries beforehand is usually recommended so that we don't fell short of resources that we need. \n",
    "\n",
    "Here we are importing specifically the KMeans algorithm from the clustering module of PySpark's MLIB which take in **input** columns and **return** the **predictions** as cluster tag.\n",
    "\n",
    "Though clustering module don't only have the KMeans as the options but also **LDA, Bisecting KMeans, Guassian Mixture Model, and Power Iteration Clustering**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d_YewDjvFXiI"
   },
   "source": [
    "## Reading the dataset\n",
    "\n",
    "Let's read the Wheat seed dataset which is there with us in the **CSV format** before actually reading it let's recall few major points of this dataset.\n",
    "\n",
    "1. It has total of **7 features** or we can say **7 measurements of wheat kernels**.\n",
    "2. We already know that in this whole dataset there are **3 types of seeds** hence through clustering we just need to give them tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q1zKgNX4f24j",
    "outputId": "6a7aca51-088f-4fed-f14c-2293e6fe5b0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+-----------+------------------+------------------+---------------------+------------------+\n",
      "| area|perimeter|compactness|  length_of_kernel|   width_of_kernel|asymmetry_coefficient|  length_of_groove|\n",
      "+-----+---------+-----------+------------------+------------------+---------------------+------------------+\n",
      "|15.26|    14.84|      0.871|             5.763|             3.312|                2.221|              5.22|\n",
      "|14.88|    14.57|     0.8811| 5.553999999999999|             3.333|                1.018|             4.956|\n",
      "|14.29|    14.09|      0.905|             5.291|3.3369999999999997|                2.699|             4.825|\n",
      "|13.84|    13.94|     0.8955|             5.324|3.3789999999999996|                2.259|             4.805|\n",
      "|16.14|    14.99|     0.9034|5.6579999999999995|             3.562|                1.355|             5.175|\n",
      "|14.38|    14.21|     0.8951|             5.386|             3.312|   2.4619999999999997|             4.956|\n",
      "|14.69|    14.49|     0.8799|             5.563|             3.259|   3.5860000000000003| 5.218999999999999|\n",
      "|14.11|     14.1|     0.8911|              5.42|             3.302|                  2.7|               5.0|\n",
      "|16.63|    15.46|     0.8747|             6.053|             3.465|                 2.04| 5.877000000000001|\n",
      "|16.44|    15.25|      0.888|5.8839999999999995|             3.505|                1.969|5.5329999999999995|\n",
      "|15.26|    14.85|     0.8696|5.7139999999999995|             3.242|                4.543|             5.314|\n",
      "|14.03|    14.16|     0.8796|             5.438|             3.201|   1.7169999999999999|             5.001|\n",
      "|13.89|    14.02|      0.888|             5.439|             3.199|                3.986|             4.738|\n",
      "|13.78|    14.06|     0.8759|             5.479|             3.156|                3.136|             4.872|\n",
      "|13.74|    14.05|     0.8744|             5.482|             3.114|                2.932|             4.825|\n",
      "|14.59|    14.28|     0.8993|             5.351|             3.333|                4.185| 4.781000000000001|\n",
      "|13.99|    13.83|     0.9183|             5.119|             3.383|                5.234| 4.781000000000001|\n",
      "|15.69|    14.75|     0.9058|             5.527|             3.514|                1.599|             5.046|\n",
      "| 14.7|    14.21|     0.9153|             5.205|             3.466|                1.767|             4.649|\n",
      "|12.72|    13.57|     0.8686|             5.226|             3.049|                4.102|             4.914|\n",
      "+-----+---------+-----------+------------------+------------------+---------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = spark.read.csv(\"seeds_dataset.csv\",header=True,inferSchema=True)\n",
    "dataset.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G1vy0tKpH8jK"
   },
   "source": [
    "**Inference:** As usual we used the **read.csv** function of the PySpark to **read the data which was in the CSV format** and kept the **header** parameter as **True** so that the first column of the dataset should be treated as column heading. Similarly **inferSchema** is also set to **True** because we want to see the original type of each column.\n",
    "\n",
    "\n",
    "**Note:** If we closely look the above output then we can find out that **this dataset requires the \"standard scaling\" of columns** that will be done in the later section of this article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HoXZahjUyD3r",
    "outputId": "2251431e-cd73-466d-ef0f-9313edfbfc12"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(area=15.26, perimeter=14.84, compactness=0.871, length_of_kernel=5.763, width_of_kernel=3.312, asymmetry_coefficient=2.221, length_of_groove=5.22)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eVA-PWkUV9pd"
   },
   "source": [
    "**Inference:** If one wants to see the column name with their corresponding values i.e. **tuple of one or more records** then the best way is to go with **head**() function which will return the **Row** object which have the records and its values as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DDu4JE44yD3v",
    "outputId": "6e6e9560-4f54-4b0b-8215-23c24bf55410"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+--------------------+-------------------+------------------+---------------------+-------------------+\n",
      "|summary|              area|         perimeter|         compactness|   length_of_kernel|   width_of_kernel|asymmetry_coefficient|   length_of_groove|\n",
      "+-------+------------------+------------------+--------------------+-------------------+------------------+---------------------+-------------------+\n",
      "|  count|               210|               210|                 210|                210|               210|                  210|                210|\n",
      "|   mean|14.847523809523816|14.559285714285718|  0.8709985714285714|  5.628533333333335| 3.258604761904762|   3.7001999999999997|  5.408071428571429|\n",
      "| stddev|2.9096994306873647|1.3059587265640225|0.023629416583846364|0.44306347772644983|0.3777144449065867|   1.5035589702547392|0.49148049910240543|\n",
      "|    min|             10.59|             12.41|              0.8081|              4.899|              2.63|                0.765|              4.519|\n",
      "|    max|             21.18|             17.25|              0.9183|              6.675|             4.033|                8.456|               6.55|\n",
      "+-------+------------------+------------------+--------------------+-------------------+------------------+---------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r2yb5OutXeDg"
   },
   "source": [
    "**Inference:** The **describe()** method is the go to function of PySpark when we want to see the **statistical information** of the dataset. In the above output as well we can see that total number of instances are **210** and its same for each column that means there are **no null values**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aLxSr-dAyD3z"
   },
   "source": [
    "## Formatting the data for MLIB\n",
    "\n",
    "In MLIB we can't really fed all the features to the model in this case we have to first **combine all the columns together in the vectorised format** so that model in the backend can traverse through each numerical value. This clubbing features task is done by **VectorAssembler**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "9C9bbrj4yD31",
    "outputId": "603359e5-8b46-497e-a821-889cd46231dd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['area',\n",
       " 'perimeter',\n",
       " 'compactness',\n",
       " 'length_of_kernel',\n",
       " 'width_of_kernel',\n",
       " 'asymmetry_coefficient',\n",
       " 'length_of_groove']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "dataset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TUIMo7nVdiex"
   },
   "source": [
    "**Inference:** As we need to format our data using **Vectors** and **VectorAssembler** so we are importing them from PySpark's **feature** module. Also later looking at all the available columns which will help us in the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "PpGtKuFpyD3-",
    "outputId": "e0978039-553c-4f31-b490-fb4a04a2f90e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+-----------+------------------+------------------+---------------------+------------------+--------------------+\n",
      "| area|perimeter|compactness|  length_of_kernel|   width_of_kernel|asymmetry_coefficient|  length_of_groove|            features|\n",
      "+-----+---------+-----------+------------------+------------------+---------------------+------------------+--------------------+\n",
      "|15.26|    14.84|      0.871|             5.763|             3.312|                2.221|              5.22|[15.26,14.84,0.87...|\n",
      "|14.88|    14.57|     0.8811| 5.553999999999999|             3.333|                1.018|             4.956|[14.88,14.57,0.88...|\n",
      "|14.29|    14.09|      0.905|             5.291|3.3369999999999997|                2.699|             4.825|[14.29,14.09,0.90...|\n",
      "|13.84|    13.94|     0.8955|             5.324|3.3789999999999996|                2.259|             4.805|[13.84,13.94,0.89...|\n",
      "|16.14|    14.99|     0.9034|5.6579999999999995|             3.562|                1.355|             5.175|[16.14,14.99,0.90...|\n",
      "|14.38|    14.21|     0.8951|             5.386|             3.312|   2.4619999999999997|             4.956|[14.38,14.21,0.89...|\n",
      "|14.69|    14.49|     0.8799|             5.563|             3.259|   3.5860000000000003| 5.218999999999999|[14.69,14.49,0.87...|\n",
      "|14.11|     14.1|     0.8911|              5.42|             3.302|                  2.7|               5.0|[14.11,14.1,0.891...|\n",
      "|16.63|    15.46|     0.8747|             6.053|             3.465|                 2.04| 5.877000000000001|[16.63,15.46,0.87...|\n",
      "|16.44|    15.25|      0.888|5.8839999999999995|             3.505|                1.969|5.5329999999999995|[16.44,15.25,0.88...|\n",
      "|15.26|    14.85|     0.8696|5.7139999999999995|             3.242|                4.543|             5.314|[15.26,14.85,0.86...|\n",
      "|14.03|    14.16|     0.8796|             5.438|             3.201|   1.7169999999999999|             5.001|[14.03,14.16,0.87...|\n",
      "|13.89|    14.02|      0.888|             5.439|             3.199|                3.986|             4.738|[13.89,14.02,0.88...|\n",
      "|13.78|    14.06|     0.8759|             5.479|             3.156|                3.136|             4.872|[13.78,14.06,0.87...|\n",
      "|13.74|    14.05|     0.8744|             5.482|             3.114|                2.932|             4.825|[13.74,14.05,0.87...|\n",
      "|14.59|    14.28|     0.8993|             5.351|             3.333|                4.185| 4.781000000000001|[14.59,14.28,0.89...|\n",
      "|13.99|    13.83|     0.9183|             5.119|             3.383|                5.234| 4.781000000000001|[13.99,13.83,0.91...|\n",
      "|15.69|    14.75|     0.9058|             5.527|             3.514|                1.599|             5.046|[15.69,14.75,0.90...|\n",
      "| 14.7|    14.21|     0.9153|             5.205|             3.466|                1.767|             4.649|[14.7,14.21,0.915...|\n",
      "|12.72|    13.57|     0.8686|             5.226|             3.049|                4.102|             4.914|[12.72,13.57,0.86...|\n",
      "+-----+---------+-----------+------------------+------------------+---------------------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vec_assembler = VectorAssembler(inputCols = dataset.columns, outputCol='features')\n",
    "final_data = vec_assembler.transform(dataset)\n",
    "final_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "anC1-lr4gRXz"
   },
   "source": [
    "**Code breakdown:**\n",
    "\n",
    "1. Creating a **VectorAssembler** object and passing all the columns present in the dataset in the **input** **columns** parameter and naming it as the **features** column.\n",
    "2. **Transforming the changes** so that it will reflect in the real dataset.\n",
    "3. Then at the last if we will look in the **dataset**, the last column is the **collection of all the features in vector**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S5990cjAyD4E"
   },
   "source": [
    "## Scaling the data\n",
    "\n",
    "Scaling the data is completely optional step in the data preprocessing stage but sometimes equally necessary as well **depending on the nature of the dataset** also scaling down the dataset at same scale helps to ncrease the accuracy and deals with the **curse of dimensionality**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "id": "U9o9kR56yD4H"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=False)\n",
    "\n",
    "scalerModel = scaler.fit(final_data)\n",
    "\n",
    "final_data = scalerModel.transform(final_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "39Gm_pL7jXQ_"
   },
   "source": [
    "**Code breakdown:**\n",
    "\n",
    "1. Importing the **StandardScaler** object from **ml.feature** library of the PySpark.\n",
    "\n",
    "2. Then passing the features as the **input** column value and **scaled** features as **output** column features. Main thing to note here is that we are scaling the data in terms of **standard deviation (True)** but not with **mean (False)**.\n",
    "\n",
    "3. At the third step we are gonna compute the **summary statistics** by using the **fit function**.\n",
    "\n",
    "4. At the last step **scaled model** will normalize every feature to have the same unit of standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SBqZsuO8yD4Q"
   },
   "source": [
    "## Training and evaluating the model\n",
    "\n",
    "Now we are actually in the **model development phase** where first we are gonna build the **KMeans clustering model** and then for the **testing phase** we will **evaluate the model** using relevant metrics which will let us know how our model performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "id": "MxE88q6dyD4T"
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(featuresCol='scaledFeatures',k=3)\n",
    "model = kmeans.fit(final_data)\n",
    "model = model.transform(final_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wkaa-IsbypcF"
   },
   "source": [
    "**Inference:** First thing to note that in the training phase we are passing the k value i.e. **number of clusters as 3** because we already know that there are 3 types of seeds available.\n",
    "\n",
    "Then its necesssary to **transform** the changes i.e. **training of the model on the whole dataset (as there are no labels)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5O45HAAsyD4U",
    "outputId": "ddf5ae4e-c499-4f14-b8ee-a8f449a25b71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette evaluation results for wheat seed segmentation= 0.630000103338996\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "silhouette_3_groups = evaluator.evaluate(model)\n",
    "print(\"Silhouette evaluation results for wheat seed segmentation= \" + str(silhouette_3_groups))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dH6YP4hsz7_R"
   },
   "source": [
    "**Inference:** Here comes the model evaluation phase where first and foremost we import the **ClusteringEvaluator** module so that we could statistically check how well the model performed using the **Silhouette** evaluation measures. **The results are neither too good nor too bad.** For that one could tune the model and see if it is resulting to better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RR-E_LuxyD4W",
    "outputId": "4b33b794-075f-42b9-92ca-a5b852bb7d78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|prediction|\n",
      "+----------+\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         2|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         0|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.select('prediction').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jE4LhYxI019O"
   },
   "source": [
    "**Inference:** If one wants to see the tag, like **which sets of records belongs to what cluster** then navigate through the **\"prediction\"** column and you can see the results as the above output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bGy4FPpE1cCp"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "The final part of the article where we will go through each step in a brief explanation that helped us to solve the problem of seggregating the three types of wheat seeds through KMeans clustering.\n",
    "\n",
    "1. Firstly we went through the theory part and learnt about the dataset then followed few compulsory steps like **starting the spark session and reading the dataset using PySpark.**\n",
    "\n",
    "2. Then after some analysis of data we format it to make it ready for the machine learning algorithm ~ **KMeans clustering**.\n",
    "\n",
    "3. When we closely looked at the data we found that it requires standard scaling as well, so after **scaling** the data we trained it and get through the **evaluation** part too later reached to the conclusion that model **moderately** **performed**."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "Segmenting the wheat's seed using MLIB - KMeans",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
