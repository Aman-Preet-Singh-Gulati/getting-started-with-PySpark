{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uTK2hH8D-iMQ"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This article will help to solve the real world problem for students to classify the **university as the Private or the Public university** based on the features we fed in the model that will be trained by **various trees methods** which we will discuss later on. In a nutshell, PySpark library is involved as we will be working with it's **MLIB** library (**The machine learning library of PySpark**).\n",
    "\n",
    "\n",
    "## About the dataset\n",
    "\n",
    "We are using the famous **Private VS Public Universities** dataset which have **17 features** that will work as **the dependent columns** and a target column named as **Private** (the categorical column which have \"**Yes**\" for **Private** and \"**No**\" for **Public**) \n",
    "\n",
    "**Here is the brief description of all the columns:**\n",
    "\n",
    "1. **Private** is our target column which have two values, **Yes/No** to classify the university as **private/public** respectively.\n",
    "2. **Apps** is the number of applications received.\n",
    "3. **Accept** is the total number of the application received.\n",
    "4. **Enroll number** is the total number of students who enrolled.\n",
    "5. **Top10per Pct.** has all the students from the top 10% of High School.\n",
    "6. **Top25perc Pct.** has the students from the top 20% of High School.\n",
    "7. **F.Undergrad** holds the total number of full-time undergraduates.\n",
    "8. **P.Undergrad** have the number of part-time undergrduates.\n",
    "9. **Outstate** column holds the number of out of station students.\n",
    "10. **Room_board** is the room costs.\n",
    "11. **Books_estimated** is the costs of the books.\n",
    "12. **Personal_estimated** column stores the personal spending of students.\n",
    "13. **PhD Pct.** holds the total number of Phd holder faculty.\n",
    "14. **Terminal Pct.** column have the number of terminal holder faculty.\n",
    "15. **S.F ratio** stimulates the Student/Faculty ratio.\n",
    "16. **perc.alumni Pct.** have the number of alumini who donate.\n",
    "17. **Expend** has the instructional expenditure of each student.\n",
    "18. **Grad rate** have the graduation rate values.\n",
    "    \n",
    "**To achieve the aim of developing the good model for our problem statement we will be using various trees methods that are as follows:**\n",
    "\n",
    "* A single decision tree\n",
    "* A random forest\n",
    "* A gradient boosted tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8TC9Qqc4KdBv",
    "outputId": "d81a0a80-2d43-41ec-acca-a9c498f65f03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting pyspark\n",
      "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 281.4 MB 32 kB/s \n",
      "\u001b[?25hCollecting py4j==0.10.9.3\n",
      "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
      "\u001b[K     |████████████████████████████████| 198 kB 50.0 MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=a5d8dec75c12ccdfd2693d15c0399bb42ae7d0c0d8548e5d69187d95e150ccd6\n",
      "  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mG1uQtEZRlZZ"
   },
   "source": [
    "## Intializing the Spark Session\n",
    "\n",
    "As we are well aware of the **mandatory steps** that we need to follow in order to start the **spark session** because for working with **PySpark** there should be all the resources available with us for that **setting up the environment** is the key thing to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "id": "H_GjfT1LDSQN"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('treecode').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mr-gfCIsY6jC"
   },
   "source": [
    "**Inference:** For initializing the session we imported the Spark object from **pyspark** library. Then for creating the enivironment of Apache Spark we used the **builder** and the **create** function of the **SparkSession** object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SOu8bapzq1Dd"
   },
   "source": [
    "## Reading the dataset\n",
    "\n",
    "Let's read our dataset now from PySpark's **read.csv** function so that we could then predict that according to the given features it is the **Private** college of the **Public** one.\n",
    "\n",
    "**Note:** While we are passing the name of the dataset in the first parameter though note that in second and third param i.e. **inferSchema and header is set to True** so that original types in the dataset should be shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "JX6cZ6GBDSQR",
    "outputId": "30897766-aab2-4b06-f0a7-67e73c5d39ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+----+------+------+---------+---------+-----------+-----------+--------+----------+-----+--------+---+--------+---------+-----------+------+---------+\n",
      "|              School|Private|Apps|Accept|Enroll|Top10perc|Top25perc|F_Undergrad|P_Undergrad|Outstate|Room_Board|Books|Personal|PhD|Terminal|S_F_Ratio|perc_alumni|Expend|Grad_Rate|\n",
      "+--------------------+-------+----+------+------+---------+---------+-----------+-----------+--------+----------+-----+--------+---+--------+---------+-----------+------+---------+\n",
      "|Abilene Christian...|    Yes|1660|  1232|   721|       23|       52|       2885|        537|    7440|      3300|  450|    2200| 70|      78|     18.1|         12|  7041|       60|\n",
      "|  Adelphi University|    Yes|2186|  1924|   512|       16|       29|       2683|       1227|   12280|      6450|  750|    1500| 29|      30|     12.2|         16| 10527|       56|\n",
      "|      Adrian College|    Yes|1428|  1097|   336|       22|       50|       1036|         99|   11250|      3750|  400|    1165| 53|      66|     12.9|         30|  8735|       54|\n",
      "| Agnes Scott College|    Yes| 417|   349|   137|       60|       89|        510|         63|   12960|      5450|  450|     875| 92|      97|      7.7|         37| 19016|       59|\n",
      "|Alaska Pacific Un...|    Yes| 193|   146|    55|       16|       44|        249|        869|    7560|      4120|  800|    1500| 76|      72|     11.9|          2| 10922|       15|\n",
      "|   Albertson College|    Yes| 587|   479|   158|       38|       62|        678|         41|   13500|      3335|  500|     675| 67|      73|      9.4|         11|  9727|       55|\n",
      "|Albertus Magnus C...|    Yes| 353|   340|   103|       17|       45|        416|        230|   13290|      5720|  500|    1500| 90|      93|     11.5|         26|  8861|       63|\n",
      "|      Albion College|    Yes|1899|  1720|   489|       37|       68|       1594|         32|   13868|      4826|  450|     850| 89|     100|     13.7|         37| 11487|       73|\n",
      "|    Albright College|    Yes|1038|   839|   227|       30|       63|        973|        306|   15595|      4400|  300|     500| 79|      84|     11.3|         23| 11644|       80|\n",
      "|Alderson-Broaddus...|    Yes| 582|   498|   172|       21|       44|        799|         78|   10468|      3380|  660|    1800| 40|      41|     11.5|         15|  8991|       52|\n",
      "|   Alfred University|    Yes|1732|  1425|   472|       37|       75|       1830|        110|   16548|      5406|  500|     600| 82|      88|     11.3|         31| 10932|       73|\n",
      "|   Allegheny College|    Yes|2652|  1900|   484|       44|       77|       1707|         44|   17080|      4440|  400|     600| 73|      91|      9.9|         41| 11711|       76|\n",
      "|Allentown Coll. o...|    Yes|1179|   780|   290|       38|       64|       1130|        638|    9690|      4785|  600|    1000| 60|      84|     13.3|         21|  7940|       74|\n",
      "|        Alma College|    Yes|1267|  1080|   385|       44|       73|       1306|         28|   12572|      4552|  400|     400| 79|      87|     15.3|         32|  9305|       68|\n",
      "|     Alverno College|    Yes| 494|   313|   157|       23|       46|       1317|       1235|    8352|      3640|  650|    2449| 36|      69|     11.1|         26|  8127|       55|\n",
      "|American Internat...|    Yes|1420|  1093|   220|        9|       22|       1018|        287|    8700|      4780|  450|    1400| 78|      84|     14.7|         19|  7355|       69|\n",
      "|     Amherst College|    Yes|4302|   992|   418|       83|       96|       1593|          5|   19760|      5300|  660|    1598| 93|      98|      8.4|         63| 21424|      100|\n",
      "| Anderson University|    Yes|1216|   908|   423|       19|       40|       1819|        281|   10100|      3520|  550|    1100| 48|      61|     12.1|         14|  7994|       59|\n",
      "|  Andrews University|    Yes|1130|   704|   322|       14|       23|       1586|        326|    9996|      3090|  900|    1320| 62|      66|     11.5|         18| 10908|       46|\n",
      "|Angelo State Univ...|     No|3540|  2001|  1016|       24|       54|       4190|       1512|    5130|      3592|  500|    2000| 60|      62|     23.1|          5|  4010|       34|\n",
      "+--------------------+-------+----+------+------+---------+---------+-----------+-----------+--------+----------+-----+--------+---+--------+---------+-----------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.read.csv('College.csv',inferSchema=True,header=True)\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e2Jt0fFSlXs_"
   },
   "source": [
    "**Inference:** So in the output it returned the complete data in the form of dataset and showing up the **top 20 rows**. Please have a note to our target column i.e. **Private**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HEL0LJR5jRnv",
    "outputId": "46985572-ec22-465f-ccd1-d881dfdcec7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-------+------------------+------------------+----------------+------------------+------------------+-----------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+----------------+------------------+\n",
      "|summary|              School|Private|              Apps|            Accept|          Enroll|         Top10perc|         Top25perc|      F_Undergrad|      P_Undergrad|          Outstate|        Room_Board|             Books|          Personal|               PhD|          Terminal|         S_F_Ratio|       perc_alumni|          Expend|         Grad_Rate|\n",
      "+-------+--------------------+-------+------------------+------------------+----------------+------------------+------------------+-----------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+----------------+------------------+\n",
      "|  count|                 777|    777|               777|               777|             777|               777|               777|              777|              777|               777|               777|               777|               777|               777|               777|               777|               777|             777|               777|\n",
      "|   mean|                null|   null|3001.6383526383524|2018.8043758043757|779.972972972973| 27.55855855855856|  55.7966537966538|3699.907335907336|855.2985842985843| 10440.66924066924| 4357.526383526383| 549.3809523809524|1340.6422136422136| 72.66023166023166| 79.70270270270271|14.089703989703986|22.743886743886744|9660.17117117117| 65.46332046332046|\n",
      "| stddev|                null|   null|3870.2014844352884|  2451.11397099263| 929.17619013287|17.640364385452134|19.804777595131373|4850.420530887386|1522.431887295513|4023.0164841119727|1096.6964155935289|165.10536013709253|  677.071453590578|16.328154687939314|14.722358527903374|3.9583491352055478| 12.39180148937615|5221.76843985609|17.177709897155403|\n",
      "|    min|Abilene Christian...|     No|                81|                72|              35|                 1|                 9|              139|                1|              2340|              1780|                96|               250|                 8|                24|               2.5|                 0|            3186|                10|\n",
      "|    max|York College of P...|    Yes|             48094|             26330|            6392|                96|               100|            31643|            21836|             21700|              8124|              2340|              6800|               103|               100|              39.8|                64|           56233|               118|\n",
      "+-------+--------------------+-------+------------------+------------------+----------------+------------------+------------------+-----------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ywVVykKoNuE"
   },
   "source": [
    "**Inference:** The **describe** function of **PySpark** provides us the **brief statistical information** about the dataset which is quite informative. We can also see that the count of each column is same i.e. **777** which stimulates **there are no null values**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xr0tAV5kDSQV",
    "outputId": "40439b26-0009-4e2e-9196-aa4578ed72da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- School: string (nullable = true)\n",
      " |-- Private: string (nullable = true)\n",
      " |-- Apps: integer (nullable = true)\n",
      " |-- Accept: integer (nullable = true)\n",
      " |-- Enroll: integer (nullable = true)\n",
      " |-- Top10perc: integer (nullable = true)\n",
      " |-- Top25perc: integer (nullable = true)\n",
      " |-- F_Undergrad: integer (nullable = true)\n",
      " |-- P_Undergrad: integer (nullable = true)\n",
      " |-- Outstate: integer (nullable = true)\n",
      " |-- Room_Board: integer (nullable = true)\n",
      " |-- Books: integer (nullable = true)\n",
      " |-- Personal: integer (nullable = true)\n",
      " |-- PhD: integer (nullable = true)\n",
      " |-- Terminal: integer (nullable = true)\n",
      " |-- S_F_Ratio: double (nullable = true)\n",
      " |-- perc_alumni: integer (nullable = true)\n",
      " |-- Expend: integer (nullable = true)\n",
      " |-- Grad_Rate: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJm_sYkAqj0o"
   },
   "source": [
    "**Inference:** **printSchema()** is yet another function of **PySpark** where it gives us the complete information about the origin**al Schema of the dataset** along with that it returns the type of the data as well as **nullable value (true/false)** corresponding to each features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iT6vggHsDSQa",
    "outputId": "9edab33e-7f18-4fd4-de0b-15d561b3a122"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(School='Abilene Christian University', Private='Yes', Apps=1660, Accept=1232, Enroll=721, Top10perc=23, Top25perc=52, F_Undergrad=2885, P_Undergrad=537, Outstate=7440, Room_Board=3300, Books=450, Personal=2200, PhD=70, Terminal=78, S_F_Ratio=18.1, perc_alumni=12, Expend=7041, Grad_Rate=60)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-zNLnZ_sWOF"
   },
   "source": [
    "**Inference:** Head method is yet another method to look into data more closely as along with **showing the column name it also return the value associated with it** hence, becomes quite handy when we want to get more infernce of **what data** it is constituting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQlqDKGzDSQd"
   },
   "source": [
    "## Formatting the dataset \n",
    "\n",
    "By far we are investigating the data like **what type of data each feature is holding, is there any null values or not and stuff like that** but now it's time to format the dataset in such a way that it becomes capable enough to be **feeded to machine learning algorithm.**\n",
    "\n",
    "There are few mandatory things that we need to **perform so that Spark MLIB could accept our data**. It should have only two columns i.e. the label column which is the **target** one and the features column that holds the **set of all features**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "id": "rshGY15HDSQg"
   },
   "outputs": [],
   "source": [
    "# Import VectorAssembler and Vectors\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LjsreO6zu-i6"
   },
   "source": [
    "**Inference:** Here we are importing two of the main libraries that will help us to format the dataset that PySpark could accept that are **Vectors** and **VectorAssembler**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zSsToHvkDSQk",
    "outputId": "afb94c3b-f3e9-44ce-b029-be7c0bfa515d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['School',\n",
       " 'Private',\n",
       " 'Apps',\n",
       " 'Accept',\n",
       " 'Enroll',\n",
       " 'Top10perc',\n",
       " 'Top25perc',\n",
       " 'F_Undergrad',\n",
       " 'P_Undergrad',\n",
       " 'Outstate',\n",
       " 'Room_Board',\n",
       " 'Books',\n",
       " 'Personal',\n",
       " 'PhD',\n",
       " 'Terminal',\n",
       " 'S_F_Ratio',\n",
       " 'perc_alumni',\n",
       " 'Expend',\n",
       " 'Grad_Rate']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ssUeZ8L2wE6i"
   },
   "source": [
    "**Inference:** Whomsoever is following my PySpark series they will notice that just before creating the assembler object I always used to see my data columns **this actually helps me in saving time** while typing the names hence **one can either see the name of the features + target or can take it as a tip for effecient coding.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "6Lqc0ZiIDSQm"
   },
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "  inputCols=['Apps',\n",
    "             'Accept',\n",
    "             'Enroll',\n",
    "             'Top10perc',\n",
    "             'Top25perc',\n",
    "             'F_Undergrad',\n",
    "             'P_Undergrad',\n",
    "             'Outstate',\n",
    "             'Room_Board',\n",
    "             'Books',\n",
    "             'Personal',\n",
    "             'PhD',\n",
    "             'Terminal',\n",
    "             'S_F_Ratio',\n",
    "             'perc_alumni',\n",
    "             'Expend',\n",
    "             'Grad_Rate'],\n",
    "              outputCol=\"features\")\n",
    "\n",
    "output = assembler.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTxMFedZw-HF"
   },
   "source": [
    "**Inference:** So now as we were aware of the right columns to be chosen hence we can make our **assembler object** that will help to combine all the features together and make it right fit for MLIB's format.\n",
    "\n",
    "Don't forget to **transform** it so that we can see the permanent changes in the newly created DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-PnihO88DSQr"
   },
   "source": [
    "## Converting type of Target column\n",
    "\n",
    "Previously we saw that the Private column i.e. **independent column** has the values as **Yes/No** but we can't feed this type of **String** data to model hence we need to convert this string data to **binary categorical values (0/1)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "id": "NRkLsVO4DSQw"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"Private\", outputCol=\"PrivateIndex\")\n",
    "output_fixed = indexer.fit(output).transform(output)\n",
    "\n",
    "final_data = output_fixed.select(\"features\",'PrivateIndex')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BI1b9nv6DcXd"
   },
   "source": [
    "**Code breakdown:**\n",
    "\n",
    "1. When we want to convert any **string data type to the integer categorical values** then **StringIndexer** comes into action.\n",
    "\n",
    "2. StringIndexer object is created where **input column** is there as the parameter that **needs name of the column to be converted**. Again we also need to **transform** it to view the permanent changes in the data.\n",
    "\n",
    "3. At the last we created a new **DataFrame (final_data)** that have **features** column and **Private Index (converted target column)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-xvnvGvaF21f"
   },
   "source": [
    "## Splitting the dataset (Training/Testing)\n",
    "\n",
    "This is one of the quite important step in any machine learning pipeline as after **splitting the whole dataset we have enough data for our training purpose as well enough data for the testing purpose** as the validation of the model is equally important than training it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "id": "sl-uWrVpDSQ5"
   },
   "outputs": [],
   "source": [
    "train_data,test_data = final_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8o13NWQtHTl3"
   },
   "source": [
    "**Inference:** After the execution of above cell we are left with **70%** of the training data and **30%** of the testing dataset which will eventually we used for validating the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bf9JtpsADSQ6"
   },
   "source": [
    "## Model development phase\n",
    "\n",
    "So as we have seperated our dataset and left with independent training set of data. Now, with this we will train our model with all the available tree methods like **Decision tree, Gradient Boosting classifier and Random forest classifier**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "id": "u5wT_cDXDSQ7"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier,GBTClassifier,RandomForestClassifier\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GSyTFSzjKF_H"
   },
   "source": [
    "**Inference:** Importing all the mentioned **tree classifiers** from the **ml.classification** library. Along with that **Pipeline module** is imported and this one is completely optional as I'll personally suggest that use the pipeline way only when you are **transforming the data multiple time and it needs a specific order of execution.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "irHtJFzIDSQ8"
   },
   "source": [
    "Now we have to create the object of each model i.e. all the three **tree models** and store it in a certain variables so that later one can easily **fit (train)** it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "id": "dgqMWEAQDSQ-"
   },
   "outputs": [],
   "source": [
    "dtc = DecisionTreeClassifier(labelCol='PrivateIndex',featuresCol='features')\n",
    "rfc = RandomForestClassifier(labelCol='PrivateIndex',featuresCol='features')\n",
    "gbt = GBTClassifier(labelCol='PrivateIndex',featuresCol='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nmZFOfe0MrJy"
   },
   "source": [
    "**Inference:** From the above **three line of codes** we can assure that each **tree model is created** one common thing is that each model has the same parameter i.e. **label column** and the **features column**\n",
    "\n",
    "**Note:** Here we are using the **default parameters** to maintain the simplicity of the model though one can change that to **fine tune** the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ApvaME2dDSQ_"
   },
   "source": [
    "Now let's **train the model** i.e. **fit the models using the fit function** of MLIB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "GZ3NoJ_ZDSRA"
   },
   "outputs": [],
   "source": [
    "dtc_model = dtc.fit(train_data)\n",
    "rfc_model = rfc.fit(train_data)\n",
    "gbt_model = gbt.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UvRTANMKONCj"
   },
   "source": [
    "**Note:** When one will train all the three models together (in one cell) then one should patient enough as it will take some time **(depending on one's system configuration)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lLlim5ntDSRC"
   },
   "source": [
    "## Evaluating and comparing the model\n",
    "\n",
    "In this section we will **compare and evaluate each model** simultaneously so that we could come to the conclusion that which model has **performed the best** hence that particular model will be taken to the **deployment phase**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "KPhloIdJDSRE"
   },
   "outputs": [],
   "source": [
    "dtc_predictions = dtc_model.transform(test_data)\n",
    "rfc_predictions = rfc_model.transform(test_data)\n",
    "gbt_predictions = gbt_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "69-FG0VSQZLW"
   },
   "source": [
    "**Inference:** For evaluaing the model we need to **transform each tree algorithm** via testing data as the evaluation are only done on the basis of the results that we get from the **testing data**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DUaSBwTbDSRF"
   },
   "source": [
    "There are various evaluation metrics available in PySpark we just have to figure out what we need at what point of time i.e. either **Binary Classification Evaluator** or **Multi class Evaluator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "id": "tak6crN1DSRI"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BwOs4ypIavlx"
   },
   "source": [
    "**Inference:** Here we acquired the **Multi class classification Evaluator** as we wanna see the Accuracy of the model and one needs to know that in classification problem if we are going with **Binary classification evaluator then Accuracy, Precision and such other metrics we can't access but with Multi class we can.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "id": "HZ2cvOdXDSRK"
   },
   "outputs": [],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "acc_evaluator = MulticlassClassificationEvaluator(labelCol=\"PrivateIndex\", predictionCol=\"prediction\", metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lUlUtPB6bx6_"
   },
   "source": [
    "**Inference:** So we build the **MulticlassClassificationEvaluator** object where we passed the **label column, prediction column** as well as the **name of the metrics** that we wanna see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "JniImITmDSRL"
   },
   "outputs": [],
   "source": [
    "dtc_acc = acc_evaluator.evaluate(dtc_predictions)\n",
    "rfc_acc = acc_evaluator.evaluate(rfc_predictions)\n",
    "gbt_acc = acc_evaluator.evaluate(gbt_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RqihgeBocSNf"
   },
   "source": [
    "**Inference:** To see the results **evaluate** method is used which needs the **evaluated data (tree_model_predictions)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Rpr6K3lDSRL",
    "outputId": "a53c9828-ef70-4038-c9ff-616eea4ec749"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the results!\n",
      "--------------------------------------------------\n",
      "A single decision tree had an accuracy of: 91.67%\n",
      "--------------------------------------------------\n",
      "A random forest ensemble had an accuracy of: 92.92%\n",
      "--------------------------------------------------\n",
      "A ensemble using GBT had an accuracy of: 91.67%\n"
     ]
    }
   ],
   "source": [
    "print(\"Here are the results!\")\n",
    "print('-'*50)\n",
    "print('A single decision tree had an accuracy of: {0:2.2f}%'.format(dtc_acc*100))\n",
    "print('-'*50)\n",
    "print('A random forest ensemble had an accuracy of: {0:2.2f}%'.format(rfc_acc*100))\n",
    "print('-'*50)\n",
    "print('A ensemble using GBT had an accuracy of: {0:2.2f}%'.format(gbt_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E86eiOEcdYfC"
   },
   "source": [
    "**Inference:** At the last we printed the **accuracy of each model** and found out that **random forest ensemble is the best** when it comes to this particular problem statement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4woXixQQDSRN"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "This is the last section of the article where we will have a look on everything we did so far to **classify the private and public universities** i.e. from starting the **SparkSession** to evaluating the model and choosing the **best model** the brief conlusion will help you to understand the **flow of the machine learning pipeline via PySpark's MLIB**.\n",
    "\n",
    "1. Firstly, as usual we setup an **hastle free environment** and read the **college dataset** to do the data analysis.\n",
    "2. Then we look into the data closely to undertand what changes needs to be done in **data preprocessing** step and later updated it too according to the requirements.\n",
    "3. Then comes the turn of model development phase where we build various **tree models** so that later we could compare and choose the best one.\n",
    "4. In the last section we evaluated the model and came to the final conclusion that **random forest ensemble model performed best on the testing data.**\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "Classify University as Private or Public using MLIB",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
