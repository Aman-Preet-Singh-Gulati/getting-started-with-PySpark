{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EDoGwC104s0o"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Tree methods are one of the most efficient ways of handling both the classification and the regression problems and there are ample of methods available to choose from like **Decision Tree, Random forest and Gradient Boosting**. In this article we will use the **official [dataset](https://github.com/apache/spark/blob/master/data/mllib/sample_libsvm_data.txt) provided by Spark** (perfectly cleaned and ready to use) so that we won't be focussing much on data preprocessing and more on the model development and evaluating process.\n",
    "\n",
    "By this way we can go through each tree algorithm in a detailed and descriptive way. So without wasting more time in story building let's get our hands on building **tree model using PySpark's MLIB**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NrMjrlHWg0vf"
   },
   "source": [
    "## Installing PySpark\n",
    "\n",
    "Before using the PySpark's methods, libraries and utilities one will have to install the pyspark which is quite a straightforward step, we just have to use a simple command i.e. **pip install pyspark**. \n",
    "\n",
    "**Note:** In the below cell note that I used **\"!\"** before command which denotes that it is the **jupyter notebook** cell if one is using the command line then exclamation sign is not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P7ml40pb5z3R",
    "outputId": "0b198b7d-0d71-4669-de1c-6ddc0d205be0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting pyspark\n",
      "  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 281.3 MB 44 kB/s \n",
      "\u001b[?25hCollecting py4j==0.10.9.5\n",
      "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
      "\u001b[K     |████████████████████████████████| 199 kB 31.4 MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764026 sha256=11a067bbe7ee644552220a7b079f167c8fdf7bc197025e04547080e0f9b491ba\n",
      "  Stored in directory: /root/.cache/pip/wheels/7a/8e/1b/f73a52650d2e5f337708d9f6a1750d451a7349a867f928b885\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.5 pyspark-3.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q2Uf8xjAjHFS"
   },
   "source": [
    "## Importing required PySpark libraries\n",
    "\n",
    "Let's start by **importing the libraries as per the initial requirements** we might end up import more libraries as we move on with more functionalities meanwhile let's get started with **first round of modules**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "id": "Juvwyymo4s0v"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YO8C2sZNkZI5"
   },
   "source": [
    "So we have imported mainly three libraries for now and they are Pipeline, RandomForestClassifier and MulticlassClassificationEvaluator let's get a brief introduction for all three of them.\n",
    "\n",
    "1. **RandomForestClassifier:** As name denotes it is the **Random forest algorithm for the classification problems**.\n",
    "\n",
    "2. **Pipeline:** This module helps us to maintain the proper workflow of the machine learning process and **staging each step** for hassle free load balancing.\n",
    "\n",
    "3. **MulticlassClassificationEvaluator:** This is the model evaulation metric more specifically for the **multi class classification**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "id": "WRJNJ9WD4s00",
    "outputId": "73de1c48-725e-4b57-9270-2374b8e0d82d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ae96b3e1e77c:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>random_forest_intro</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f19cbe965d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('random_forest_intro').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fmqatzaSpu1T"
   },
   "source": [
    "**Inference:** In the above set of codes we imported the **SparkSession module** and then we have created the **Spark object** which is kind of the \"**must-todo**\" step for accessing and utilising all the **PySpark's methods, libraries and modules.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "WYDueB7V4s03"
   },
   "outputs": [],
   "source": [
    "data_tree = spark.read.format(\"libsvm\").load(\"sample_libsvm_data.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RPbKtSTdqyko"
   },
   "source": [
    "**Inference:** So now we are loading that official dataset from the spark github repository. Note that this one is **not** in the traditional **CSV** format instead in **libsvm** format hence we will load it in that way only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OQ2jmj6q4s06",
    "outputId": "2a91fbc6-81e6-4af0-e4e5-e7a6c05d5d64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(692,[127,128,129...|\n",
      "|  1.0|(692,[158,159,160...|\n",
      "|  1.0|(692,[124,125,126...|\n",
      "|  1.0|(692,[152,153,154...|\n",
      "|  1.0|(692,[151,152,153...|\n",
      "|  0.0|(692,[129,130,131...|\n",
      "|  1.0|(692,[158,159,160...|\n",
      "|  1.0|(692,[99,100,101,...|\n",
      "|  0.0|(692,[154,155,156...|\n",
      "|  0.0|(692,[127,128,129...|\n",
      "|  1.0|(692,[154,155,156...|\n",
      "|  0.0|(692,[153,154,155...|\n",
      "|  0.0|(692,[151,152,153...|\n",
      "|  1.0|(692,[129,130,131...|\n",
      "|  0.0|(692,[154,155,156...|\n",
      "|  1.0|(692,[150,151,152...|\n",
      "|  0.0|(692,[124,125,126...|\n",
      "|  0.0|(692,[152,153,154...|\n",
      "|  1.0|(692,[97,98,99,12...|\n",
      "|  1.0|(692,[124,125,126...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_tree.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G6HdSbiXt2-3"
   },
   "source": [
    "**Inference:** Here is the sneek peak of the dataset on which we will be working to **apply tree algorithms** one need to notice that it is already preprocessed and cleaned just the way PySpark want it to be and infact we don't even have to use **VectorAssembler** object because that task is also done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pNdmoVyyxHkS",
    "outputId": "2e577f89-4ff2-4097-ff08-527edb963f87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_tree.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OGv3qyZcxOZz"
   },
   "source": [
    "**Inference:** As discussed earlier that we have preprocessed data hence one is the features column **(Vector type)** and the label column i.e. **target column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m0Vvyb8L4s08",
    "outputId": "4f3d955a-0500-40ac-94f6-c01143916719"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=0.0, features=SparseVector(692, {127: 51.0, 128: 159.0, 129: 253.0, 130: 159.0, 131: 50.0, 154: 48.0, 155: 238.0, 156: 252.0, 157: 252.0, 158: 252.0, 159: 237.0, 181: 54.0, 182: 227.0, 183: 253.0, 184: 252.0, 185: 239.0, 186: 233.0, 187: 252.0, 188: 57.0, 189: 6.0, 207: 10.0, 208: 60.0, 209: 224.0, 210: 252.0, 211: 253.0, 212: 252.0, 213: 202.0, 214: 84.0, 215: 252.0, 216: 253.0, 217: 122.0, 235: 163.0, 236: 252.0, 237: 252.0, 238: 252.0, 239: 253.0, 240: 252.0, 241: 252.0, 242: 96.0, 243: 189.0, 244: 253.0, 245: 167.0, 262: 51.0, 263: 238.0, 264: 253.0, 265: 253.0, 266: 190.0, 267: 114.0, 268: 253.0, 269: 228.0, 270: 47.0, 271: 79.0, 272: 255.0, 273: 168.0, 289: 48.0, 290: 238.0, 291: 252.0, 292: 252.0, 293: 179.0, 294: 12.0, 295: 75.0, 296: 121.0, 297: 21.0, 300: 253.0, 301: 243.0, 302: 50.0, 316: 38.0, 317: 165.0, 318: 253.0, 319: 233.0, 320: 208.0, 321: 84.0, 328: 253.0, 329: 252.0, 330: 165.0, 343: 7.0, 344: 178.0, 345: 252.0, 346: 240.0, 347: 71.0, 348: 19.0, 349: 28.0, 356: 253.0, 357: 252.0, 358: 195.0, 371: 57.0, 372: 252.0, 373: 252.0, 374: 63.0, 384: 253.0, 385: 252.0, 386: 195.0, 399: 198.0, 400: 253.0, 401: 190.0, 412: 255.0, 413: 253.0, 414: 196.0, 426: 76.0, 427: 246.0, 428: 252.0, 429: 112.0, 440: 253.0, 441: 252.0, 442: 148.0, 454: 85.0, 455: 252.0, 456: 230.0, 457: 25.0, 466: 7.0, 467: 135.0, 468: 253.0, 469: 186.0, 470: 12.0, 482: 85.0, 483: 252.0, 484: 223.0, 493: 7.0, 494: 131.0, 495: 252.0, 496: 225.0, 497: 71.0, 510: 85.0, 511: 252.0, 512: 145.0, 520: 48.0, 521: 165.0, 522: 252.0, 523: 173.0, 538: 86.0, 539: 253.0, 540: 225.0, 547: 114.0, 548: 238.0, 549: 253.0, 550: 162.0, 566: 85.0, 567: 252.0, 568: 249.0, 569: 146.0, 570: 48.0, 571: 29.0, 572: 85.0, 573: 178.0, 574: 225.0, 575: 253.0, 576: 223.0, 577: 167.0, 578: 56.0, 594: 85.0, 595: 252.0, 596: 252.0, 597: 252.0, 598: 229.0, 599: 215.0, 600: 252.0, 601: 252.0, 602: 252.0, 603: 196.0, 604: 130.0, 622: 28.0, 623: 199.0, 624: 252.0, 625: 252.0, 626: 253.0, 627: 252.0, 628: 252.0, 629: 233.0, 630: 145.0, 651: 25.0, 652: 128.0, 653: 252.0, 654: 253.0, 655: 252.0, 656: 141.0, 657: 37.0})),\n",
       " Row(label=1.0, features=SparseVector(692, {158: 124.0, 159: 253.0, 160: 255.0, 161: 63.0, 185: 96.0, 186: 244.0, 187: 251.0, 188: 253.0, 189: 62.0, 213: 127.0, 214: 251.0, 215: 251.0, 216: 253.0, 217: 62.0, 240: 68.0, 241: 236.0, 242: 251.0, 243: 211.0, 244: 31.0, 245: 8.0, 267: 60.0, 268: 228.0, 269: 251.0, 270: 251.0, 271: 94.0, 295: 155.0, 296: 253.0, 297: 253.0, 298: 189.0, 322: 20.0, 323: 253.0, 324: 251.0, 325: 235.0, 326: 66.0, 349: 32.0, 350: 205.0, 351: 253.0, 352: 251.0, 353: 126.0, 377: 104.0, 378: 251.0, 379: 253.0, 380: 184.0, 381: 15.0, 404: 80.0, 405: 240.0, 406: 251.0, 407: 193.0, 408: 23.0, 431: 32.0, 432: 253.0, 433: 253.0, 434: 253.0, 435: 159.0, 459: 151.0, 460: 251.0, 461: 251.0, 462: 251.0, 463: 39.0, 486: 48.0, 487: 221.0, 488: 251.0, 489: 251.0, 490: 172.0, 514: 234.0, 515: 251.0, 516: 251.0, 517: 196.0, 518: 12.0, 542: 253.0, 543: 251.0, 544: 251.0, 545: 89.0, 569: 159.0, 570: 255.0, 571: 253.0, 572: 253.0, 573: 31.0, 596: 48.0, 597: 228.0, 598: 253.0, 599: 247.0, 600: 140.0, 601: 8.0, 624: 64.0, 625: 251.0, 626: 253.0, 627: 220.0, 652: 64.0, 653: 251.0, 654: 253.0, 655: 220.0, 680: 24.0, 681: 193.0, 682: 253.0, 683: 220.0})),\n",
       " Row(label=1.0, features=SparseVector(692, {124: 145.0, 125: 255.0, 126: 211.0, 127: 31.0, 151: 32.0, 152: 237.0, 153: 253.0, 154: 252.0, 155: 71.0, 179: 11.0, 180: 175.0, 181: 253.0, 182: 252.0, 183: 71.0, 208: 144.0, 209: 253.0, 210: 252.0, 211: 71.0, 235: 16.0, 236: 191.0, 237: 253.0, 238: 252.0, 239: 71.0, 263: 26.0, 264: 221.0, 265: 253.0, 266: 252.0, 267: 124.0, 268: 31.0, 292: 125.0, 293: 253.0, 294: 252.0, 295: 252.0, 296: 108.0, 321: 253.0, 322: 252.0, 323: 252.0, 324: 108.0, 349: 255.0, 350: 253.0, 351: 253.0, 352: 108.0, 377: 253.0, 378: 252.0, 379: 252.0, 380: 108.0, 405: 253.0, 406: 252.0, 407: 252.0, 408: 108.0, 433: 253.0, 434: 252.0, 435: 252.0, 436: 108.0, 461: 255.0, 462: 253.0, 463: 253.0, 464: 170.0, 489: 253.0, 490: 252.0, 491: 252.0, 492: 252.0, 493: 42.0, 517: 149.0, 518: 252.0, 519: 252.0, 520: 252.0, 521: 144.0, 545: 109.0, 546: 252.0, 547: 252.0, 548: 252.0, 549: 144.0, 574: 218.0, 575: 253.0, 576: 253.0, 577: 255.0, 578: 35.0, 602: 175.0, 603: 252.0, 604: 252.0, 605: 253.0, 606: 35.0, 630: 73.0, 631: 252.0, 632: 252.0, 633: 253.0, 634: 35.0, 658: 31.0, 659: 211.0, 660: 252.0, 661: 253.0, 662: 35.0})),\n",
       " Row(label=1.0, features=SparseVector(692, {152: 5.0, 153: 63.0, 154: 197.0, 180: 20.0, 181: 254.0, 182: 230.0, 183: 24.0, 208: 20.0, 209: 254.0, 210: 254.0, 211: 48.0, 236: 20.0, 237: 254.0, 238: 255.0, 239: 48.0, 264: 20.0, 265: 254.0, 266: 254.0, 267: 57.0, 292: 20.0, 293: 254.0, 294: 254.0, 295: 108.0, 320: 16.0, 321: 239.0, 322: 254.0, 323: 143.0, 349: 178.0, 350: 254.0, 351: 143.0, 377: 178.0, 378: 254.0, 379: 143.0, 405: 178.0, 406: 254.0, 407: 162.0, 433: 178.0, 434: 254.0, 435: 240.0, 461: 113.0, 462: 254.0, 463: 240.0, 489: 83.0, 490: 254.0, 491: 245.0, 492: 31.0, 517: 79.0, 518: 254.0, 519: 246.0, 520: 38.0, 546: 214.0, 547: 254.0, 548: 150.0, 574: 144.0, 575: 241.0, 576: 8.0, 602: 144.0, 603: 240.0, 604: 2.0, 630: 144.0, 631: 254.0, 632: 82.0, 658: 230.0, 659: 247.0, 660: 40.0, 686: 168.0, 687: 209.0, 688: 31.0})),\n",
       " Row(label=1.0, features=SparseVector(692, {151: 1.0, 152: 168.0, 153: 242.0, 154: 28.0, 179: 10.0, 180: 228.0, 181: 254.0, 182: 100.0, 208: 190.0, 209: 254.0, 210: 122.0, 236: 83.0, 237: 254.0, 238: 162.0, 264: 29.0, 265: 254.0, 266: 248.0, 267: 25.0, 292: 29.0, 293: 255.0, 294: 254.0, 295: 103.0, 320: 29.0, 321: 254.0, 322: 254.0, 323: 109.0, 348: 29.0, 349: 254.0, 350: 254.0, 351: 109.0, 376: 29.0, 377: 254.0, 378: 254.0, 379: 109.0, 404: 29.0, 405: 255.0, 406: 254.0, 407: 109.0, 432: 29.0, 433: 254.0, 434: 254.0, 435: 109.0, 460: 29.0, 461: 254.0, 462: 254.0, 463: 63.0, 488: 29.0, 489: 254.0, 490: 254.0, 491: 28.0, 516: 29.0, 517: 254.0, 518: 254.0, 519: 28.0, 544: 29.0, 545: 254.0, 546: 254.0, 547: 35.0, 572: 29.0, 573: 254.0, 574: 254.0, 575: 109.0, 600: 6.0, 601: 212.0, 602: 254.0, 603: 109.0, 629: 203.0, 630: 254.0, 631: 178.0, 657: 155.0, 658: 254.0, 659: 190.0, 685: 32.0, 686: 199.0, 687: 104.0}))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tree.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Co66OKPivCTy"
   },
   "source": [
    "**Inference:** There is one more way to have a look into the dataset and this one is quite similar to pandas i.e. the **head()** method which not only will return the **name and type of the columns** but also the **values** each are holding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucmufkCI1z6r"
   },
   "source": [
    "## Train-Test split\n",
    "\n",
    "This phase of the machine learning cycle is also known as **splitting the dataset** phase where we will breakdown the dataset into training and testing set so that we can **train the model** on **training set** and **test** the same on **testing set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "id": "NLepmBxv4s0_"
   },
   "outputs": [],
   "source": [
    "(trainingSet, testSet) = data_tree.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jTO92Xvl794j"
   },
   "source": [
    "**Inference:** **RandomSplit()** is the method which is responsible to divide the dataset into training and testing set and from the parameter values we can stimulate that there is **70% of training data and 30% of testing data**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xQTWPTWd9FJf"
   },
   "source": [
    "Now we will **train** the **random forest model (here classifier)** for that firstly the random forest object needs to be created by **passing in the relevant parameter**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "id": "DjGbarYi4s1D"
   },
   "outputs": [],
   "source": [
    "random_forest = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pzgh0qeU9Z7H"
   },
   "source": [
    "**Inference:** While creating the Random forest classifer we are passing the **label** column that is our target and the **features** column (collectively all the features) and as it is **random forest classifier** so we have to specify the **total number of trees** as **20**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "id": "wdsY1LyO4s1G"
   },
   "outputs": [],
   "source": [
    "model_rf = random_forest.fit(trainingSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nf2PZqQFEHzP"
   },
   "source": [
    "**Inference:** Now to actually train the model we use the **fit()** method by passing the set of training data as the parameter to that function and don't forget to use the random forest object for calling it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "id": "EcJClX8T4s1I"
   },
   "outputs": [],
   "source": [
    "predictions = model_rf.transform(testSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N0ObcFV1b0fp"
   },
   "source": [
    "**Inference:** Here comes the stage where we will make **predictions** using the **evaluate** method of **MLIB** library and make sure to evaluate the model on testing data as then only the purpose of using it will be fullfilled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rkPKK0XB4s1K",
    "outputId": "679bda6e-b2db-4559-961c-5b5d110d7412"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8c__Nevwdrex"
   },
   "source": [
    "**Inference:** Let's look what the prediction DataFrame holds. So from the above output we can see that there are **5** columns:\n",
    "1. **Label:** This is the target column.\n",
    "2. **features:** All the **features/dependent** columns in the form of **vector**\n",
    "3. **rawPrediction:** This column will be very handy in the case of **GBT classifier** \n",
    "4. **Probability:** It holds the **probability** of how much is the chnces that the predcitions is correct.\n",
    "5. **Predictions:** **Predicted** values by model during evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3GwO-l_k4s1N",
    "outputId": "1c3bf952-e64a-4b10-db93-adf62adb5179"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|label|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       0.0|  0.0|(692,[95,96,97,12...|\n",
      "|       0.0|  0.0|(692,[98,99,100,1...|\n",
      "|       0.0|  0.0|(692,[121,122,123...|\n",
      "|       0.0|  0.0|(692,[122,123,124...|\n",
      "|       0.0|  0.0|(692,[123,124,125...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G0n2cidr6FLl"
   },
   "source": [
    "**Inference:** In the above output we have filtered the main DataFrame to extract the important columns only i.e. **prediction, label and features.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "id": "FcrdkXrH4s1Q"
   },
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ribkkXx6sRj"
   },
   "source": [
    "**Inference:** Here we are in the stage of model evaluation and for that we are using the Multi class classification Evaluator there is one key difference between **Binary** and **Multi class evaluators**, binary on one side can only return the **AUC** curve while other one can return the **accuracy, precision and recall metrics** as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HWtbOfoq4s1R",
    "outputId": "773d06ec-dfd5-4dbb-af76-b8c043a62dbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BpXCkM747YWM"
   },
   "source": [
    "**Inference:** Well!! to get the **0 test error** seems to be bit unacceptable especially in the real setup, having said that keep one thing in mind that this dataset is **highly seperable and cleaned** so one might expect such tremendeous results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFL5Hk2N4s1V"
   },
   "source": [
    "## Gradient Boosted Trees\n",
    "\n",
    "**Gradient boosting Trees (GBT's)** is another tree method which can be used for both **classification** and **regression** problems though GBT's are built on the basis of **ensemble methods** using number of **decision trees**. Though one don't have to worry about the mathematics behind this algorithm as Spark handles it in a better way.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e34K8ViT4s1W",
    "outputId": "bc9ecf7d-725b-4454-c8c0-82723ff8473f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|label|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       0.0|  0.0|(692,[95,96,97,12...|\n",
      "|       0.0|  0.0|(692,[123,124,125...|\n",
      "|       0.0|  0.0|(692,[123,124,125...|\n",
      "|       0.0|  0.0|(692,[124,125,126...|\n",
      "|       0.0|  0.0|(692,[124,125,126...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "\n",
    "data_gbt = spark.read.format(\"libsvm\").load(\"sample_libsvm_data.txt\")\n",
    "\n",
    "\n",
    "(trainingSet, testSet) = data_gbt.randomSplit([0.7, 0.3])\n",
    "\n",
    "gbt_mdl = GBTClassifier(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n",
    "\n",
    "model = gbt_mdl.fit(trainingSet)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testSet)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XC7be0H49lKY"
   },
   "source": [
    "**Code breakdown:** This is just for the walkthrough purpose otherwise if you have learnt the random forest part then this one will be easy to pick up in terms of implementation.\n",
    "\n",
    "\n",
    "1. Firstly importing the **GBT model** and reading the dataset by exactly the same method.\n",
    "2. Splitting the dataset into **training** and **testing** set using **randomSplit** method.\n",
    "3. Training of the GBT model using object creation and **fit** method.\n",
    "4. Making **predictions** using the **transform** method on testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ptndf4H94s1Y",
    "outputId": "5c9b2a9a-1470-40ea-f325-c94d36e07959"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.0625\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZBOxFcpR_Ema"
   },
   "source": [
    "**Inference:** As we did in the case of random forest similarly following the same approach for **GBT** where we got the test result as around **0.088** which seems bit realistic.\n",
    "\n",
    "\n",
    "**Note:** The same pipeline can be applied in the case of decision tree as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hbA2P3Qf4s1d"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this article we have discussed about all the important **tree methods** that can be implemented using **PySpark's MLIB** and went through the hands on practice by using the official documented dataset provided by **Spark**. Now, let's discuss everything we did in a nutshell.\n",
    "\n",
    "1. First we did an **environment setup thing** and reading the dataset and then look at the dataset which was preprocessed and ready to use for model development.\n",
    "\n",
    "2. Then the stage of **splitting the dataset** comes into existence following by the **random forest** model development, prediction and in the end evaluation.\n",
    "\n",
    "3. Similarly we perform the same process in the case of **GBT model** (practically) and conclude that using any tree method need to follow the same process."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "Introduction to tree methods in MLIB",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
