{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lmkiw9Q4OaZT"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this article we will be learning how to perform filtering operations and particularly here it will be based on two types of filter operations.\n",
    "\n",
    "1. **Relational filters:** These operations will be working on the basis of relational operators that we are aware of like equal to, less than equal to or greater than equal to **(=, <=, >=)**.\n",
    "\n",
    "2. **Logical filters:** These are the operations which needs to be performed when we are dealing with multiple conditions specifically and here we will be discuss all three main types of logical operators and they are, **AND(&), OR(|), NOT(~)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are new to PySpark then I'll suggest you to follow my Pyspark series\n",
    "\n",
    "1. Getting started with PySpark using Python\n",
    "2. Data Preprocessing using PySpark - PySpark's DataFrame\n",
    "3. Data preprocessing using PySpark - Handling missing values\n",
    "4. Data Preprocessing using PySpark - Aggregate and GroupBy functions\n",
    "5. Introduction to PySpark MLIB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fELNm51trEb7",
    "outputId": "63e56028-c581-422f-cd08-83cf60ec4578"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 281.4 MB 35 kB/s \n",
      "\u001b[?25hCollecting py4j==0.10.9.3\n",
      "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
      "\u001b[K     |████████████████████████████████| 198 kB 42.2 MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=7232292e1bc9b0af9951eed9866e3ee9466238ff02ef659794df85b09463ecfa\n",
      "  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1qj8WUoTgt6w"
   },
   "source": [
    "## Mandatory Steps\n",
    " If you are already following my PySpark series then you would easily know what steps I'm gonna perform now. Let's first discuss them in the nutshell:\n",
    "\n",
    "1. First we have **imported the SparkSession** to start the PySpark Session.\n",
    "2. Then with the help of **getOrCreate()** function we have created our session of Apache Spark.\n",
    "3. At the last we saw what our **spark object** holds in a graphical format.\n",
    "\n",
    "**Note:** I have discussed these steps in detail in my first article, [Getting Started with PySpark Using Python](https://www.analyticsvidhya.com/blog/2022/04/getting-started-with-pyspark-using-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "q6a9XTAir3R-"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "b3jv8tUTr7H6"
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('filter_operations').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "id": "_VEzU6T9sE4P",
    "outputId": "79fd1c8d-8466-42fd-8d68-e0eaf5da58cb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://82110fa62838:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>filter_operations</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f35de0ced50>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r3VjW3U3SQj_"
   },
   "source": [
    "## Reading the dataset\n",
    "\n",
    "In this section we will be reading and **storing the intance of our dummy dataset** with header and Scehma as True which will give us the exact information about the table and its column types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V-O39QwWsIk7",
    "outputId": "e7830021-ed7a-4581-fb41-64ca5d51095e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------------+---------+\n",
      "|EmpName|EmpAge|EmpExperience|EmpSalary|\n",
      "+-------+------+-------------+---------+\n",
      "| Oliver|    31|           10|    30000|\n",
      "|  Harry|    30|            8|    25000|\n",
      "| George|    29|            4|    20000|\n",
      "|   Jack|    24|            3|    20000|\n",
      "|  Jacob|    21|            1|    15000|\n",
      "|    Leo|    23|            2|    18000|\n",
      "|  Oscar|  null|         null|    40000|\n",
      "|   null|    34|           10|    38000|\n",
      "|   null|    36|         null|     null|\n",
      "+-------+------+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filter_pyspark = spark.read.csv('/content/part2.2.csv', header = True, inferSchema=True)\n",
    "df_filter_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "69JxY4gVsxFr"
   },
   "source": [
    "## Relational filtering\n",
    "\n",
    "Here comes the section where we will be doing hands on filtering techniques and in relational filteration we can use different operators like **less than, less than equal to, greater than, greater than equal to and equal to.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o2naFyyKsyox",
    "outputId": "f3d3e3d9-4dd9-44f9-816d-fc875ec698a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------------+---------+\n",
      "|EmpName|EmpAge|EmpExperience|EmpSalary|\n",
      "+-------+------+-------------+---------+\n",
      "|  Harry|    30|            8|    25000|\n",
      "| George|    29|            4|    20000|\n",
      "|   Jack|    24|            3|    20000|\n",
      "|  Jacob|    21|            1|    15000|\n",
      "|    Leo|    23|            2|    18000|\n",
      "+-------+------+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filter_pyspark.filter(\"EmpSalary<=25000\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inference:** Here we can see that the records are filtered out where employees has the salary less than or equal to 25000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ntCHkBNstCln"
   },
   "source": [
    "**Selecting the relevant columns instead of showing al the columns** \n",
    "\n",
    "This is one of the best cost effective technique in terms of execution time as when working with the large dataset if we will retrieve all the records (all columns) then it will take more execution time but if we know what records we want to see then we can easily choose selected columns as mentioned below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZELuM6UotIaY",
    "outputId": "47f743aa-ecc0-42a2-865c-0a824f44b90d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|EmpName|EmpAge|\n",
      "+-------+------+\n",
      "|  Harry|    30|\n",
      "| George|    29|\n",
      "|   Jack|    24|\n",
      "|  Jacob|    21|\n",
      "|    Leo|    23|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filter_pyspark.filter(\"EmpSalary<=25000\").select(['EmpName','EmpAge']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UoIS6dkLxACP"
   },
   "source": [
    "**Code breakdown:**\n",
    "\n",
    "So the above code can be broken down into three simple steps for acchieving the goal:\n",
    "\n",
    "1. This particular filter operation can also come into the category of multiple filering as in first condtion we are filtering out the employees based on the salary i.e. when employee's salary is less than 25000.\n",
    "2. Then comes the main condition where we are selecting the two columns \"EmpName\" and \"EmpAge\" using the select function.\n",
    "3. At the last showing the filtered DataFrame using show function.\n",
    "\n",
    "**Note:** Similarly we can use other operators of relational type according to the problem statement we just need to replace the operator and we are good to go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3jTUwLSJNDNM"
   },
   "source": [
    "**Another approach of selecting the columns**\n",
    "\n",
    "Here we will be looking at one more way where we can select our desired columns and get the exact same result like in the previous output\n",
    "\n",
    "**Tip:** By looking at this line of code one will get reminded about how **Pandas** used to filter the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hpVwhGrsw7V_",
    "outputId": "dd8520cd-8e8e-42fe-bced-99018a8a810f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|EmpName|EmpAge|\n",
      "+-------+------+\n",
      "|  Harry|    30|\n",
      "| George|    29|\n",
      "|   Jack|    24|\n",
      "|  Jacob|    21|\n",
      "|    Leo|    23|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filter_pyspark.filter(df_filter_pyspark['EmpSalary']<=25000).select(['EmpName','EmpAge']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ukDyu4QGOz_E"
   },
   "source": [
    "**Inference:** In the output we can clearly see that we got the exact same result as we got in previous filter operation. The only change we can see here is the way how we selected the records based on the salary - **df_filter_pyspark['EmpSalary']<=25000** here we have first took the object and entered the name of the column then at the last simply we added the filter condition just like we used to do in **Pandas**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CkRKnKhTPk_3"
   },
   "source": [
    "## Logical filtering \n",
    "\n",
    "In this section we will be using different cases to filter out the records based on multiple conditions and for that we will be having three different cases\n",
    "\n",
    "1. AND condition \n",
    "2. OR condtion \n",
    "3. NOT condition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcaRG_iJTSch"
   },
   "source": [
    "**\"AND\" condtion:** The one familiar with SQL or any programming language in which they have to deal with manipulation of data they are well aware of the fact that when we will be using AND operation then it means all the conditions needs to be **TRUE** i.e. if any of the condition will be **false** then there would not be any output shown.\n",
    "\n",
    "**Note:** In PySpark we use **\"&\"** symbol to denote the **AND** operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pmUE_LzKOS92",
    "outputId": "97b91b32-9987-4c40-d9a9-4676249384f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------------+---------+\n",
      "|EmpName|EmpAge|EmpExperience|EmpSalary|\n",
      "+-------+------+-------------+---------+\n",
      "| Oliver|    31|           10|    30000|\n",
      "|  Harry|    30|            8|    25000|\n",
      "| George|    29|            4|    20000|\n",
      "|   Jack|    24|            3|    20000|\n",
      "|    Leo|    23|            2|    18000|\n",
      "+-------+------+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filter_pyspark.filter((df_filter_pyspark['EmpSalary']<=30000)\n",
    "                          & (df_filter_pyspark['EmpSalary']>=18000)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C7jASrNSUfuu"
   },
   "source": [
    "**Code breakdown:**\n",
    "Here we can see that we used two conditions one where salary of the employee is less than equal to 30000 & (AND) greater than equal to 18000 i.e. the records which falls into this bracket will be shown in the results other records will be skipped.\n",
    "\n",
    "1. Condtion 1: **df_filter_pyspark['EmpSalary']<=30000** where salary is greater than 30000\n",
    "2. Condtion 2: **df_filter_pyspark['EmpSalary']<=18000** where salary is less than 18000\n",
    "3. Then we used **\"&\"** operation to filter out the records and at the last **show() function** to give the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QLRVePiCWP28"
   },
   "source": [
    "**\"OR\" condition:** This condtion is basically used when we don't want to get very stiff with filteration i.e. when we want to access the records if any of the condition is True **unlike AND condition** where all the conditon needs to be True. So be careful to use this OR condtion only when you know either of the conditon can be picked.\n",
    "\n",
    "**Note:** In PySpark we use \"|\" symbol to denote the **OR operation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kkkglAOTQHM9",
    "outputId": "a1da3133-61e7-492d-da38-63dc21c0e78b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------------+---------+\n",
      "|EmpName|EmpAge|EmpExperience|EmpSalary|\n",
      "+-------+------+-------------+---------+\n",
      "| Oliver|    31|           10|    30000|\n",
      "|  Harry|    30|            8|    25000|\n",
      "| George|    29|            4|    20000|\n",
      "|   Jack|    24|            3|    20000|\n",
      "|  Jacob|    21|            1|    15000|\n",
      "|    Leo|    23|            2|    18000|\n",
      "|   null|    34|           10|    38000|\n",
      "+-------+------+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filter_pyspark.filter((df_filter_pyspark['EmpSalary']<=30000)\n",
    "                          | (df_filter_pyspark['EmpExperience']>=3)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FmAoAsLx55VF"
   },
   "source": [
    "**Code breakdown:** If one will compare the results of AND and OR then they would get the difference of using both of them and the right time to use it according to the problem statement. Let's look at how we have used OR operation:\n",
    "\n",
    "1. Condition 1: **df_filter_pyspark['EmpSalary']<=30000** here we were plucking out the person who have salary less than equal to 30000.\n",
    "2. Condition 2: **df_filter_pyspark['EmpExperience']>=3** here we were getting the records where employee's experience is greater than equal to 3 years.\n",
    "3. For combining both of the condition we used **\"|\" (OR)** operations and at the end used show function to give the result in the form of DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q0yTwJJe8BZI"
   },
   "source": [
    "**\"NOT\" condition:** This is the condition where we have to **counter the condition** i.e. we have to do everything else the condition which we have specified itself if we try to simplify more then we can say that **if the condition is False then only NOT operation will work**.\n",
    "\n",
    "Note: In PySpark we use \"~\" symbol to denote the **NOT operation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OoQ1xVuoYHM-",
    "outputId": "00403ea9-e08a-4c27-879e-0b3d557c4577"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------------+---------+\n",
      "|EmpName|EmpAge|EmpExperience|EmpSalary|\n",
      "+-------+------+-------------+---------+\n",
      "| George|    29|            4|    20000|\n",
      "|   Jack|    24|            3|    20000|\n",
      "|  Jacob|    21|            1|    15000|\n",
      "|    Leo|    23|            2|    18000|\n",
      "+-------+------+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filter_pyspark.filter(~(df_filter_pyspark['EmpAge']>=30)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UhK-sCR3APZo"
   },
   "source": [
    "**Inference:** Here we can see how the employee who has age greater than equal to **30 doesn't even appeared in the list of records** so it is clear that if the condition is False then only there is credibility of **NOT opeation**\n",
    "\n",
    "**Note:** While using NOT (\"~\") we need to keep one thing in mind that we can't use multiple condition here as long as we are not combining it with other **logical condition like \"AND\"/\"OR\" **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key takeaways from this article\n",
    "\n",
    "In this section we will summarize everything we did previously like as we started from setting up the environment for the Python's distribution of PySpark then we had head towards performing the both relational and logical filtering on our dummy dataset.\n",
    "\n",
    "1. Firstly, we completed our mandatory steps of setting up the Spark Session and reading the dataset as these are the pillars of further analysis.\n",
    "2. Then we got to know about Relation filtering techniques which includes hands on operations using PySpark DataFrame here we discussed about a single operator and learned how to implement other basis on the same approach.\n",
    "3. At the last we move to second type of filtering i.e. logical filtering where we discussed all three types of it which were AND, OR and Not condition"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Data preprocessing using PySpark - Filter Operations",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
