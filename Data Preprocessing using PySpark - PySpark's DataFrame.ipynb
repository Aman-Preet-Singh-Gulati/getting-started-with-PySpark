{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "From this article I'm starting the PySpark tutorial series and this is first arrow. In this particular article we will be closely looking at how to get started with PySpark's data preprocessing techniques and moreover introducing how the PySpark's DataFrame look like and perform some general operations on the same i.e. from starting the PySpark's session to dealing with data preprocessing technique using PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74HM34-dtnyq"
   },
   "source": [
    "## Table of content\n",
    "\n",
    "1. **Starting PySpark session:** Mandatory step to get started with PySpark.\n",
    "2. **Reading the dataset:** In this section we will read the dataset using PySpark function only.\n",
    "3. **Datatypes of the column:** In this section we will analyze the datatypes and related thing for each column.\n",
    "4. **Indexing:** Here we will came to know that how one can do the indexing through columns.\n",
    "5. **Describe:** Similar function to Pandas and we will get to know how to use it.\n",
    "6. **Adding columns:** Get to know how one can add the columns using PySpark.\n",
    "7. **Dropping columns:** Get to know how to drop the irrelavant columns. \n",
    "8. **Renaming columns:** Get to know how one can rename the existing columns in the PySpark DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aiTXOeDGvjfg"
   },
   "source": [
    "Before moving towards the main functionalities we have to **`start the spark session`**. So let's do that first!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z66GGRqjwi4U"
   },
   "source": [
    "## Starting PySpark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gDsAsSuUxJl3",
    "outputId": "1d5ba05e-b8b6-47e7-e0ed-6ca4ccfd627c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 281.4 MB 33 kB/s \n",
      "\u001b[?25hCollecting py4j==0.10.9.3\n",
      "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
      "\u001b[K     |████████████████████████████████| 198 kB 59.3 MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=fec94c17ef29546704079ced4911417cbb1f97e6df190bd03abdbef95fad7464\n",
      "  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "id": "dTv_QqVjwKAa",
    "outputId": "ba80b393-dc68-4640-dd2c-6525e7d03446"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ad3be6897bd0:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>DataFrame_article</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f38f423f350>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "data_spark = SparkSession.builder.appName('DataFrame_article').getOrCreate()\n",
    "\n",
    "data_spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6FvNaWNxxBY"
   },
   "source": [
    "**Code breakdown**\n",
    "\n",
    "1. Firstly we have imported the **`SparkSession`** from **`pyspark.sql`** object.\n",
    "2. Then by using **`getOrCreate()`** and **`builder`** function we created a SparkSession and stored it in a variable.\n",
    "3. At the last we simply saw what is there in the **`data_spark`** variable.\n",
    "\n",
    "**Note:** This is not the detailed illustration of *\"how to start spark session\" *and if you are not able to get every bit of it then I'll recommend to go through my previous article on- **`Getting started with PySpark using Python`**\n",
    "\n",
    "Who already understood can jump to the main section of the article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17QD-OcG085a"
   },
   "source": [
    "## Reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Is_0tWJmxmtA",
    "outputId": "98dc4e3b-9d85-417e-b63c-9530a5ccf83e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[longitude: string, latitude: string, housing_median_age: string, total_rooms: string, total_bedrooms: string, population: string, households: string, median_income: string, median_house_value: string]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_spark.read.option('header','true').csv('/content/sample_data/california_housing_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZ_c1MeP19TN"
   },
   "source": [
    "**read.option.csv:** This complete set of function is responsible to read the csv type of file using PySpark where **read.csv()** can also work but to make the column name as the column header we need to use **option()** as well\n",
    "\n",
    "**Inference:** Here in the output we can see that the DataFrame object is returned which shows the column name and corresponding type of columns.\n",
    "\n",
    "Now let's see the whole dataset i.e. column and records as well using show() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QQlP9xlC1UFa",
    "outputId": "96f3f903-05fa-499e-c71d-686138f9ed29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+------------------+-----------+--------------+-----------+-----------+-------------+------------------+\n",
      "|  longitude| latitude|housing_median_age|total_rooms|total_bedrooms| population| households|median_income|median_house_value|\n",
      "+-----------+---------+------------------+-----------+--------------+-----------+-----------+-------------+------------------+\n",
      "|-114.310000|34.190000|         15.000000|5612.000000|   1283.000000|1015.000000| 472.000000|     1.493600|      66900.000000|\n",
      "|-114.470000|34.400000|         19.000000|7650.000000|   1901.000000|1129.000000| 463.000000|     1.820000|      80100.000000|\n",
      "|-114.560000|33.690000|         17.000000| 720.000000|    174.000000| 333.000000| 117.000000|     1.650900|      85700.000000|\n",
      "|-114.570000|33.640000|         14.000000|1501.000000|    337.000000| 515.000000| 226.000000|     3.191700|      73400.000000|\n",
      "|-114.570000|33.570000|         20.000000|1454.000000|    326.000000| 624.000000| 262.000000|     1.925000|      65500.000000|\n",
      "|-114.580000|33.630000|         29.000000|1387.000000|    236.000000| 671.000000| 239.000000|     3.343800|      74000.000000|\n",
      "|-114.580000|33.610000|         25.000000|2907.000000|    680.000000|1841.000000| 633.000000|     2.676800|      82400.000000|\n",
      "|-114.590000|34.830000|         41.000000| 812.000000|    168.000000| 375.000000| 158.000000|     1.708300|      48500.000000|\n",
      "|-114.590000|33.610000|         34.000000|4789.000000|   1175.000000|3134.000000|1056.000000|     2.178200|      58400.000000|\n",
      "|-114.600000|34.830000|         46.000000|1497.000000|    309.000000| 787.000000| 271.000000|     2.190800|      48100.000000|\n",
      "|-114.600000|33.620000|         16.000000|3741.000000|    801.000000|2434.000000| 824.000000|     2.679700|      86500.000000|\n",
      "|-114.600000|33.600000|         21.000000|1988.000000|    483.000000|1182.000000| 437.000000|     1.625000|      62000.000000|\n",
      "|-114.610000|34.840000|         48.000000|1291.000000|    248.000000| 580.000000| 211.000000|     2.157100|      48600.000000|\n",
      "|-114.610000|34.830000|         31.000000|2478.000000|    464.000000|1346.000000| 479.000000|     3.212000|      70400.000000|\n",
      "|-114.630000|32.760000|         15.000000|1448.000000|    378.000000| 949.000000| 300.000000|     0.858500|      45000.000000|\n",
      "|-114.650000|34.890000|         17.000000|2556.000000|    587.000000|1005.000000| 401.000000|     1.699100|      69100.000000|\n",
      "|-114.650000|33.600000|         28.000000|1678.000000|    322.000000| 666.000000| 256.000000|     2.965300|      94900.000000|\n",
      "|-114.650000|32.790000|         21.000000|  44.000000|     33.000000|  64.000000|  27.000000|     0.857100|      25000.000000|\n",
      "|-114.660000|32.740000|         17.000000|1388.000000|    386.000000| 775.000000| 320.000000|     1.204900|      44000.000000|\n",
      "|-114.670000|33.920000|         17.000000|  97.000000|     24.000000|  29.000000|  15.000000|     1.265600|      27500.000000|\n",
      "+-----------+---------+------------------+-----------+--------------+-----------+-----------+-------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark = data_spark.read.option('header','true').csv('/content/sample_data/california_housing_train.csv').show()\n",
    "df_spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQ1pza5A3uoW"
   },
   "source": [
    "Here with the help of show() function we can see the whole dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VhnVMo-84DSs"
   },
   "source": [
    "## Checking DataTypes of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vvwi290p3qTI",
    "outputId": "9602a887-dc4a-439f-d2e7-68db8687c453"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- housing_median_age: string (nullable = true)\n",
      " |-- total_rooms: string (nullable = true)\n",
      " |-- total_bedrooms: string (nullable = true)\n",
      " |-- population: string (nullable = true)\n",
      " |-- households: string (nullable = true)\n",
      " |-- median_income: string (nullable = true)\n",
      " |-- median_house_value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = data_spark.read.option('header','true').csv('/content/sample_data/california_housing_train.csv')\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lQr9OUje4sxO"
   },
   "source": [
    "**Inference:** Here with the help of **printSchema** function we can notice that it returned an ample of information related to columns and its datatypes.\n",
    "\n",
    "But, Hold on! We can see that every column shows the **`string`** value but that is not True right? \n",
    "**Answer:** Reason behind this glitch is the **default** setting of **printScehma()** function as it will always return the column type as String until we fix it.\n",
    "\n",
    "So, Let's fix this issue first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4vjGMdtM4XcO",
    "outputId": "67b43d30-93a5-4fbb-db28-cd883782e996"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- housing_median_age: double (nullable = true)\n",
      " |-- total_rooms: double (nullable = true)\n",
      " |-- total_bedrooms: double (nullable = true)\n",
      " |-- population: double (nullable = true)\n",
      " |-- households: double (nullable = true)\n",
      " |-- median_income: double (nullable = true)\n",
      " |-- median_house_value: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = data_spark.read.option('header','true').csv('/content/sample_data/california_housing_train.csv', inferSchema=True)\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mHrxNp7h6Z-a"
   },
   "source": [
    "**Inference:** So now we can see the valid data type corresponding to each column with just a minor change of adding one more argument as **`inferScehma = True`** which will change the default setting of **printSchema()**. One more thing to keep a note is **nullable = True** which certainly means that column might have null values in it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExxosuTcXAIO"
   },
   "source": [
    "There is one more way of **checking the Data types of the columns** which is pretty similar to what we used to do in the case of the pandas DataFrame. Let's see that approach as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9hkfyeC2XQkA",
    "outputId": "82028b4d-fa01-462c-b3e3-0afcc45a19f7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('longitude', 'double'),\n",
       " ('latitude', 'double'),\n",
       " ('housing_median_age', 'double'),\n",
       " ('total_rooms', 'double'),\n",
       " ('total_bedrooms', 'double'),\n",
       " ('population', 'double'),\n",
       " ('households', 'double'),\n",
       " ('median_income', 'double'),\n",
       " ('median_house_value', 'double')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJwQMSG5XY28"
   },
   "source": [
    "**Inference:** Here also it returns the same output as in previous approach but this time in the different format as it returns the output in the form of **\"list of tuple\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7RzSzZnO8fs2"
   },
   "source": [
    "## Column Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kkvOlvb88pLd"
   },
   "source": [
    "First let us see how we can get name of each column so that based on that we can perform our column indexing and other operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-TXNkhub6TOM",
    "outputId": "40052bf7-5baa-44f3-a4dd-7b4f2a383139"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['longitude',\n",
       " 'latitude',\n",
       " 'housing_median_age',\n",
       " 'total_rooms',\n",
       " 'total_bedrooms',\n",
       " 'population',\n",
       " 'households',\n",
       " 'median_income',\n",
       " 'median_house_value']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7qquGQxv9R9P"
   },
   "source": [
    "**Inference:** By using the **`columns`** object we can see the name of all the columns present in the dataset in the **list object**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Ke5lk6P-Gfy"
   },
   "source": [
    "Now let's understand how we can select the columns. For an instance let's say that we want to pluck out the **total_rooms** column only from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B5Mh0FIK86j6",
    "outputId": "08b5b9b1-0b9b-48f5-da44-78b8b4e914d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|total_rooms|\n",
      "+-----------+\n",
      "|     5612.0|\n",
      "|     7650.0|\n",
      "|      720.0|\n",
      "|     1501.0|\n",
      "|     1454.0|\n",
      "|     1387.0|\n",
      "|     2907.0|\n",
      "|      812.0|\n",
      "|     4789.0|\n",
      "|     1497.0|\n",
      "|     3741.0|\n",
      "|     1988.0|\n",
      "|     1291.0|\n",
      "|     2478.0|\n",
      "|     1448.0|\n",
      "|     2556.0|\n",
      "|     1678.0|\n",
      "|       44.0|\n",
      "|     1388.0|\n",
      "|       97.0|\n",
      "+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select('total_rooms').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHfm617l_iHC"
   },
   "source": [
    "**Inference:** Here with the help of **`select`** function we have selected the **total_rooms** column only and it returned that column as **DataFrame** of PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "406afRlfAPBM"
   },
   "source": [
    "So we have by far pluck out only single column from the dataset but what if we want to grab **multiple columns**. So let's have a look at it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pno3DUs1-cm4",
    "outputId": "161d2f84-2d87-472e-d4f0-6352a502ef9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+-------------+\n",
      "|total_rooms|total_bedrooms|median_income|\n",
      "+-----------+--------------+-------------+\n",
      "|     5612.0|        1283.0|       1.4936|\n",
      "|     7650.0|        1901.0|         1.82|\n",
      "|      720.0|         174.0|       1.6509|\n",
      "|     1501.0|         337.0|       3.1917|\n",
      "|     1454.0|         326.0|        1.925|\n",
      "|     1387.0|         236.0|       3.3438|\n",
      "|     2907.0|         680.0|       2.6768|\n",
      "|      812.0|         168.0|       1.7083|\n",
      "|     4789.0|        1175.0|       2.1782|\n",
      "|     1497.0|         309.0|       2.1908|\n",
      "|     3741.0|         801.0|       2.6797|\n",
      "|     1988.0|         483.0|        1.625|\n",
      "|     1291.0|         248.0|       2.1571|\n",
      "|     2478.0|         464.0|        3.212|\n",
      "|     1448.0|         378.0|       0.8585|\n",
      "|     2556.0|         587.0|       1.6991|\n",
      "|     1678.0|         322.0|       2.9653|\n",
      "|       44.0|          33.0|       0.8571|\n",
      "|     1388.0|         386.0|       1.2049|\n",
      "|       97.0|          24.0|       1.2656|\n",
      "+-----------+--------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select(['total_rooms', 'total_bedrooms', 'median_income']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ySZmVdlrV3xf"
   },
   "source": [
    "**Inference:** Now we have simply passed the **multiple column names** in the argument of **`select`** method but in the form of **`list`**, same logic as we used to perform in **pandas DataFrame** and with just this minute change we can grab out multiple columns from our dataset based on the requirement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "htagyWBOaMy9"
   },
   "source": [
    "## Describe function in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m4qw2Mn1BBI-",
    "outputId": "7b7ae638-51af-4d6a-c139-a0df27500134"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------------------+------------------+-----------------+-----------------+------------------+-----------------+------------------+------------------+\n",
      "|summary|          longitude|          latitude|housing_median_age|      total_rooms|   total_bedrooms|        population|       households|     median_income|median_house_value|\n",
      "+-------+-------------------+------------------+------------------+-----------------+-----------------+------------------+-----------------+------------------+------------------+\n",
      "|  count|              17000|             17000|             17000|            17000|            17000|             17000|            17000|             17000|             17000|\n",
      "|   mean|-119.56210823529375|  35.6252247058827| 28.58935294117647|2643.664411764706|539.4108235294118|1429.5739411764705|501.2219411764706| 3.883578100000021|207300.91235294117|\n",
      "| stddev| 2.0051664084260357|2.1373397946570867|12.586936981660406|2179.947071452777|421.4994515798648| 1147.852959159527|384.5208408559016|1.9081565183791036|115983.76438720895|\n",
      "|    min|            -124.35|             32.54|               1.0|              2.0|              1.0|               3.0|              1.0|            0.4999|           14999.0|\n",
      "|    max|            -114.31|             41.95|              52.0|          37937.0|           6445.0|           35682.0|           6082.0|           15.0001|          500001.0|\n",
      "+-------+-------------------+------------------+------------------+-----------------+-----------------+------------------+-----------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVsyn0Decc5J"
   },
   "source": [
    "**Inference:** So here is the result from the **describe function of PySpark** and by looking at the output one who is familiar with using **pandas's describe function** they can consider it the spitting image of the pandas DataFrame because it is showing the exact same **statistics** in the same way.\n",
    "\n",
    "In this function you can find the below mentioned detail of the dataset:\n",
    "1. count: Where you find the total number of records present in each column.\n",
    "2. mean: Here one can see the mean of the column values.\n",
    "3. stddev: It will return the standard deviation of the column values.\n",
    "4. min: This will return the minimum value present in the column.\n",
    "5. max: This will return the maximum value present in the column.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bMmSgn-CegLt"
   },
   "source": [
    "## Adding columns in PySpark DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eQi1fEJNcL9M",
    "outputId": "e306a0ad-3038-4228-ab05-11d1ddcdd77c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+-------------------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|  Updated longitude|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+-------------------+\n",
      "|  -114.31|   34.19|              15.0|     5612.0|        1283.0|    1015.0|     472.0|       1.4936|           66900.0|            -113.11|\n",
      "|  -114.47|    34.4|              19.0|     7650.0|        1901.0|    1129.0|     463.0|         1.82|           80100.0|            -113.27|\n",
      "|  -114.56|   33.69|              17.0|      720.0|         174.0|     333.0|     117.0|       1.6509|           85700.0|            -113.36|\n",
      "|  -114.57|   33.64|              14.0|     1501.0|         337.0|     515.0|     226.0|       3.1917|           73400.0|-113.36999999999999|\n",
      "|  -114.57|   33.57|              20.0|     1454.0|         326.0|     624.0|     262.0|        1.925|           65500.0|-113.36999999999999|\n",
      "|  -114.58|   33.63|              29.0|     1387.0|         236.0|     671.0|     239.0|       3.3438|           74000.0|            -113.38|\n",
      "|  -114.58|   33.61|              25.0|     2907.0|         680.0|    1841.0|     633.0|       2.6768|           82400.0|            -113.38|\n",
      "|  -114.59|   34.83|              41.0|      812.0|         168.0|     375.0|     158.0|       1.7083|           48500.0|            -113.39|\n",
      "|  -114.59|   33.61|              34.0|     4789.0|        1175.0|    3134.0|    1056.0|       2.1782|           58400.0|            -113.39|\n",
      "|   -114.6|   34.83|              46.0|     1497.0|         309.0|     787.0|     271.0|       2.1908|           48100.0|-113.39999999999999|\n",
      "|   -114.6|   33.62|              16.0|     3741.0|         801.0|    2434.0|     824.0|       2.6797|           86500.0|-113.39999999999999|\n",
      "|   -114.6|    33.6|              21.0|     1988.0|         483.0|    1182.0|     437.0|        1.625|           62000.0|-113.39999999999999|\n",
      "|  -114.61|   34.84|              48.0|     1291.0|         248.0|     580.0|     211.0|       2.1571|           48600.0|            -113.41|\n",
      "|  -114.61|   34.83|              31.0|     2478.0|         464.0|    1346.0|     479.0|        3.212|           70400.0|            -113.41|\n",
      "|  -114.63|   32.76|              15.0|     1448.0|         378.0|     949.0|     300.0|       0.8585|           45000.0|-113.42999999999999|\n",
      "|  -114.65|   34.89|              17.0|     2556.0|         587.0|    1005.0|     401.0|       1.6991|           69100.0|            -113.45|\n",
      "|  -114.65|    33.6|              28.0|     1678.0|         322.0|     666.0|     256.0|       2.9653|           94900.0|            -113.45|\n",
      "|  -114.65|   32.79|              21.0|       44.0|          33.0|      64.0|      27.0|       0.8571|           25000.0|            -113.45|\n",
      "|  -114.66|   32.74|              17.0|     1388.0|         386.0|     775.0|     320.0|       1.2049|           44000.0|            -113.46|\n",
      "|  -114.67|   33.92|              17.0|       97.0|          24.0|      29.0|      15.0|       1.2656|           27500.0|            -113.47|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = df_pyspark.withColumn('Updated longitude', df_pyspark['longitude']+1.2)\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cpag_Lf3f8NA"
   },
   "source": [
    "**Inference:** Now from the above output we can clearly see that new column is updated in the DataFrame as **\"Updated longitude\"**.\n",
    "\n",
    "Let's discuss what we did to add the columns:\n",
    "1. We used the **`withcolumn()`** function to add the columns or change the existing columns in the Pyspark DataFrame.\n",
    "2. Then in that function we will be giving two parameters \n",
    "  * First one will be the **name of the new column**\n",
    "  * Second one will be **what value** that **new column will hold.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47j3w68Xh4nQ"
   },
   "source": [
    "## Dropping columns in PySpark DataFrame\n",
    "\n",
    "Dropping the column from the dataset is pretty straightforward task and for that we will be using the **`drop()`** function from PySpark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cl8jTXE8fzJb",
    "outputId": "08edb9b5-fa46-4a70-f6dc-ea63f4260f76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
      "|  -114.31|   34.19|              15.0|     5612.0|        1283.0|    1015.0|     472.0|       1.4936|           66900.0|\n",
      "|  -114.47|    34.4|              19.0|     7650.0|        1901.0|    1129.0|     463.0|         1.82|           80100.0|\n",
      "|  -114.56|   33.69|              17.0|      720.0|         174.0|     333.0|     117.0|       1.6509|           85700.0|\n",
      "|  -114.57|   33.64|              14.0|     1501.0|         337.0|     515.0|     226.0|       3.1917|           73400.0|\n",
      "|  -114.57|   33.57|              20.0|     1454.0|         326.0|     624.0|     262.0|        1.925|           65500.0|\n",
      "|  -114.58|   33.63|              29.0|     1387.0|         236.0|     671.0|     239.0|       3.3438|           74000.0|\n",
      "|  -114.58|   33.61|              25.0|     2907.0|         680.0|    1841.0|     633.0|       2.6768|           82400.0|\n",
      "|  -114.59|   34.83|              41.0|      812.0|         168.0|     375.0|     158.0|       1.7083|           48500.0|\n",
      "|  -114.59|   33.61|              34.0|     4789.0|        1175.0|    3134.0|    1056.0|       2.1782|           58400.0|\n",
      "|   -114.6|   34.83|              46.0|     1497.0|         309.0|     787.0|     271.0|       2.1908|           48100.0|\n",
      "|   -114.6|   33.62|              16.0|     3741.0|         801.0|    2434.0|     824.0|       2.6797|           86500.0|\n",
      "|   -114.6|    33.6|              21.0|     1988.0|         483.0|    1182.0|     437.0|        1.625|           62000.0|\n",
      "|  -114.61|   34.84|              48.0|     1291.0|         248.0|     580.0|     211.0|       2.1571|           48600.0|\n",
      "|  -114.61|   34.83|              31.0|     2478.0|         464.0|    1346.0|     479.0|        3.212|           70400.0|\n",
      "|  -114.63|   32.76|              15.0|     1448.0|         378.0|     949.0|     300.0|       0.8585|           45000.0|\n",
      "|  -114.65|   34.89|              17.0|     2556.0|         587.0|    1005.0|     401.0|       1.6991|           69100.0|\n",
      "|  -114.65|    33.6|              28.0|     1678.0|         322.0|     666.0|     256.0|       2.9653|           94900.0|\n",
      "|  -114.65|   32.79|              21.0|       44.0|          33.0|      64.0|      27.0|       0.8571|           25000.0|\n",
      "|  -114.66|   32.74|              17.0|     1388.0|         386.0|     775.0|     320.0|       1.2049|           44000.0|\n",
      "|  -114.67|   33.92|              17.0|       97.0|          24.0|      29.0|      15.0|       1.2656|           27500.0|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.drop('Updated longitude').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KD6iHhEFkJGj"
   },
   "source": [
    "**Inference:** In the output we can see that \"Updated longitude\" column doesn't exist anymore in the dataset and as we have noticed that we simply gave the name of the column in the paramter and got that column dropped from the dataset.\n",
    "\n",
    "Note: If we want to drop **multiple columns** from the dataset in the same instance then we can pass the **list of column name** as the paramter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LLdTbyIel3Uc"
   },
   "source": [
    "## Renaming the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qhvWt-LWl5V5",
    "outputId": "0649a261-316e-4bd7-88eb-6f28b110c0c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+---------------------+----------+-------------+------------------+-------------------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population per capita|households|median_income|median_house_value|  Updated longitude|\n",
      "+---------+--------+------------------+-----------+--------------+---------------------+----------+-------------+------------------+-------------------+\n",
      "|  -114.31|   34.19|              15.0|     5612.0|        1283.0|               1015.0|     472.0|       1.4936|           66900.0|            -113.11|\n",
      "|  -114.47|    34.4|              19.0|     7650.0|        1901.0|               1129.0|     463.0|         1.82|           80100.0|            -113.27|\n",
      "|  -114.56|   33.69|              17.0|      720.0|         174.0|                333.0|     117.0|       1.6509|           85700.0|            -113.36|\n",
      "|  -114.57|   33.64|              14.0|     1501.0|         337.0|                515.0|     226.0|       3.1917|           73400.0|-113.36999999999999|\n",
      "|  -114.57|   33.57|              20.0|     1454.0|         326.0|                624.0|     262.0|        1.925|           65500.0|-113.36999999999999|\n",
      "|  -114.58|   33.63|              29.0|     1387.0|         236.0|                671.0|     239.0|       3.3438|           74000.0|            -113.38|\n",
      "|  -114.58|   33.61|              25.0|     2907.0|         680.0|               1841.0|     633.0|       2.6768|           82400.0|            -113.38|\n",
      "|  -114.59|   34.83|              41.0|      812.0|         168.0|                375.0|     158.0|       1.7083|           48500.0|            -113.39|\n",
      "|  -114.59|   33.61|              34.0|     4789.0|        1175.0|               3134.0|    1056.0|       2.1782|           58400.0|            -113.39|\n",
      "|   -114.6|   34.83|              46.0|     1497.0|         309.0|                787.0|     271.0|       2.1908|           48100.0|-113.39999999999999|\n",
      "|   -114.6|   33.62|              16.0|     3741.0|         801.0|               2434.0|     824.0|       2.6797|           86500.0|-113.39999999999999|\n",
      "|   -114.6|    33.6|              21.0|     1988.0|         483.0|               1182.0|     437.0|        1.625|           62000.0|-113.39999999999999|\n",
      "|  -114.61|   34.84|              48.0|     1291.0|         248.0|                580.0|     211.0|       2.1571|           48600.0|            -113.41|\n",
      "|  -114.61|   34.83|              31.0|     2478.0|         464.0|               1346.0|     479.0|        3.212|           70400.0|            -113.41|\n",
      "|  -114.63|   32.76|              15.0|     1448.0|         378.0|                949.0|     300.0|       0.8585|           45000.0|-113.42999999999999|\n",
      "|  -114.65|   34.89|              17.0|     2556.0|         587.0|               1005.0|     401.0|       1.6991|           69100.0|            -113.45|\n",
      "|  -114.65|    33.6|              28.0|     1678.0|         322.0|                666.0|     256.0|       2.9653|           94900.0|            -113.45|\n",
      "|  -114.65|   32.79|              21.0|       44.0|          33.0|                 64.0|      27.0|       0.8571|           25000.0|            -113.45|\n",
      "|  -114.66|   32.74|              17.0|     1388.0|         386.0|                775.0|     320.0|       1.2049|           44000.0|            -113.46|\n",
      "|  -114.67|   33.92|              17.0|       97.0|          24.0|                 29.0|      15.0|       1.2656|           27500.0|            -113.47|\n",
      "+---------+--------+------------------+-----------+--------------+---------------------+----------+-------------+------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.withColumnRenamed('population', 'population per capita').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "axKMnzWhmNeS"
   },
   "source": [
    "**Inference:** From the above output we can see that **\"population\"** column is renamed to **\"population per capita\"** by using the with **`columnRenamed()`** function where in one parameter we need to pass the column name which is to be renamed and next parameter will be the updated name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "So finally it's time to conclude this article and let's quickly discuss everything that we have covered in this article with short description of the same.\n",
    "\n",
    "1. The very first thing that we learned is how to start the spark session as this is the mandatory step to go with PySpark.\n",
    "2. Then we learned how to get information regarding the columns of the dataset by using printSchema() function, columns object and describe function().\n",
    "3. Then at the last we also look at how to manipulate the Schema of the dataset when we saw how to add, drop and rename the columns."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Data Preprocessing using PySpark - Part 1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
